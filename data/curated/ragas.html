<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RAGAS - RAG Assessment Framework</title>
    <link rel="stylesheet" href="../../assets/css/main.css">
    <style>
        body { max-width: 900px; margin: 0 auto; padding: 2rem; background-color: #0f0f1a; }
        .reader-container { background: #1a1a2e; padding: 3rem; border-radius: 12px; box-shadow: 0 4px 20px rgba(0,0,0,0.3); }
        .meta-header { margin-bottom: 2rem; padding-bottom: 1rem; border-bottom: 1px solid #30363d; }
        .meta-header h1 { color: #e0e0e0; margin: 0 0 1rem 0; }
        .meta-header a { color: #58a6ff; }
        .content { color: #c9d1d9; line-height: 1.7; }
        .content h2 { color: #e0e0e0; margin-top: 2rem; border-bottom: 1px solid #30363d; padding-bottom: 0.5rem; }
        .content h3 { color: #e0e0e0; margin-top: 1.5rem; }
        .content ul { margin: 1rem 0; padding-left: 1.5rem; }
        .content li { margin: 0.5rem 0; }
        .content code { background: #0d1117; padding: 2px 6px; border-radius: 4px; font-family: 'Fira Code', monospace; }
        .content pre { background: #0d1117; padding: 1rem; border-radius: 6px; overflow-x: auto; }
        .content pre code { padding: 0; background: none; }
        .metric { background: #1a2a3a; padding: 1rem; border-radius: 8px; margin: 1rem 0; border-left: 4px solid #58a6ff; }
        .metric h4 { color: #58a6ff; margin: 0 0 0.5rem 0; }
    </style>
</head>
<body>
    <div class="reader-container">
        <div class="meta-header">
            <h1>ðŸ“Š RAGAS - RAG Assessment Framework</h1>
            <p>Source: <a href="https://docs.ragas.io/" target="_blank">docs.ragas.io</a></p>
        </div>
        <div class="content">
            <h2>What is RAGAS?</h2>
            <p>RAGAS (Retrieval Augmented Generation Assessment) is a framework for evaluating RAG pipelines. It provides metrics to measure the quality of both retrieval and generation components without requiring ground truth human annotations.</p>
            
            <h2>Core Metrics</h2>
            
            <div class="metric">
                <h4>1. Faithfulness</h4>
                <p>Measures if the generated answer is grounded in the retrieved context. Higher score = fewer hallucinations.</p>
                <p><strong>Formula:</strong> Number of claims supported by context / Total claims in answer</p>
            </div>
            
            <div class="metric">
                <h4>2. Answer Relevancy</h4>
                <p>Measures how relevant the generated answer is to the question asked.</p>
                <p><strong>Method:</strong> Generates questions from the answer and compares similarity to original question.</p>
            </div>
            
            <div class="metric">
                <h4>3. Context Precision</h4>
                <p>Measures if relevant context chunks are ranked higher than irrelevant ones.</p>
                <p><strong>Requires:</strong> Ground truth answers for evaluation.</p>
            </div>
            
            <div class="metric">
                <h4>4. Context Recall</h4>
                <p>Measures how much of the ground truth answer can be attributed to the retrieved context.</p>
                <p><strong>Requires:</strong> Ground truth answers for evaluation.</p>
            </div>
            
            <h2>Installation</h2>
            <pre><code>pip install ragas</code></pre>
            
            <h2>Basic Usage (Python)</h2>
            <pre><code>from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy
from datasets import Dataset

# Prepare evaluation data
eval_data = {
    "question": ["What is RAG?"],
    "answer": ["RAG combines retrieval with generation..."],
    "contexts": [["RAG stands for Retrieval Augmented Generation..."]],
    "ground_truth": ["RAG is a technique that..."]
}

dataset = Dataset.from_dict(eval_data)

# Run evaluation
results = evaluate(
    dataset,
    metrics=[faithfulness, answer_relevancy]
)

print(results)
# {'faithfulness': 0.85, 'answer_relevancy': 0.92}</code></pre>
            
            <h2>When to Use Each Metric</h2>
            <ul>
                <li><strong>No ground truth available:</strong> Use Faithfulness + Answer Relevancy</li>
                <li><strong>Have ground truth:</strong> Add Context Precision + Context Recall</li>
                <li><strong>Testing retrieval only:</strong> Focus on Context metrics</li>
                <li><strong>Testing generation only:</strong> Focus on Faithfulness + Answer Relevancy</li>
            </ul>
            
            <h2>Integration Tips</h2>
            <ul>
                <li>Run evaluations on representative test sets</li>
                <li>Track metrics over time as you iterate</li>
                <li>Set thresholds for acceptable quality</li>
                <li>Use in CI/CD to catch regressions</li>
            </ul>
        </div>
    </div>
</body>
</html>
