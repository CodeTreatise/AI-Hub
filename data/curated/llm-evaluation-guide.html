
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Evaluation Guide</title>
    <link rel="stylesheet" href="../../assets/css/main.css">
    <style>
        body {
            max-width: 900px;
            margin: 0 auto;
            padding: 2rem;
            background-color: #0f0f1a; /* Match main theme */
        }
        .reader-container {
            background: #1a1a2e;
            padding: 3rem;
            border-radius: 12px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.3);
        }
        .meta-header {
            margin-bottom: 2rem;
            padding-bottom: 1rem;
            border-bottom: 1px solid #30363d;
        }
        .meta-header a { color: #58a6ff; }
        
        /* Content Styles */
        img { max-width: 100%; height: auto; border-radius: 8px; margin: 1rem 0; }
        pre { background: #0d1117; padding: 1rem; border-radius: 6px; overflow-x: auto; }
        code { font-family: 'Fira Code', monospace; color: #e0e0e0; }
        h1, h2, h3 { color: #e0e0e0; margin-top: 1.5rem; }
        p { line-height: 1.6; color: #c9d1d9; margin-bottom: 1rem; }
        a { color: #58a6ff; text-decoration: none; }
        a:hover { text-decoration: underline; }
        blockquote { border-left: 4px solid #58a6ff; padding-left: 1rem; color: #8b949e; }
    </style>
</head>
<body>
    <div class="reader-container">
        <div class="meta-header">
            <h1>LLM Evaluation Guide</h1>
            <p>Source: <a href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation" target="_blank">https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation</a></p>
        </div>
        <div class="content">
            <div class="styles_contentWrap__oQ7Ax"><div class="styles_sideBarWrap__9MJUd styles_mobile__NXSzR"><div class="styles_sidebar__HWsOS" id="sidebar"><span class="styles_sidebarHeading__Oeazn">In this story<!-- --> <img alt="down-wards facing arrow" data-nimg="1" decoding="async" height="12" loading="lazy" src="https://www.confident-ai.com/icons/arrow-down-black.svg" style="color:transparent" width="12"/></span><ul class="styles_list__bjylu"><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#tldr" target="_blank">TL;DR</a></li><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#what-are-llm-evaluation-metrics" target="_blank">What are LLM Evaluation Metrics?</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#what-makes-great-metrics" target="_blank">What makes great metrics?</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#different-ways-to-compute-metric-scores" target="_blank">Different Ways to Compute Metric Scores</a></li><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#statistical-scorers" target="_blank">Statistical Scorers</a></li><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#model-based-scorers" target="_blank">Model-Based Scorers</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#g-eval" target="_blank">G-Eval</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#dag-deep-acyclic-graph" target="_blank">DAG (Deep Acyclic Graph)</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#prometheus" target="_blank">Prometheus</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#combining-statistical-and-model-based-scorers" target="_blank">Combining Statistical and Model-Based Scorers</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#qag-score" target="_blank">QAG Score</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#gptscore" target="_blank">GPTScore</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#selfcheckgpt" target="_blank">SelfCheckGPT</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#choosing-your-evaluation-metrics" target="_blank">Choosing Your Evaluation Metrics</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#single-or-multi-turn" target="_blank">Single or Multi-Turn?</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#the-5-metric-rule" target="_blank">The 5 Metric Rule</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#ai-agent-metrics" target="_blank">AI Agent Metrics</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#task-completion" target="_blank">Task Completion</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#argument-correctness" target="_blank">Argument Correctness</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#tool-correctness" target="_blank">Tool Correctness</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#rag-metrics" target="_blank">RAG Metrics</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#faithfulness" target="_blank">Faithfulness</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#answer-relevancy" target="_blank">Answer Relevancy</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#contextual-precision" target="_blank">Contextual Precision</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#contextual-recall" target="_blank">Contextual Recall</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#contextual-relevancy" target="_blank">Contextual Relevancy</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#fine-tuning-metrics" target="_blank">Fine-Tuning Metrics</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#hallucination" target="_blank">Hallucination</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#toxicity" target="_blank">Toxicity</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#bias" target="_blank">Bias</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#use-case-specific-metrics" target="_blank">Use Case Specific Metrics</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#helpfulness" target="_blank">Helpfulness</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#prompt-alignment" target="_blank">Prompt Alignment</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#summarization" target="_blank">Summarization</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#conclusion" target="_blank">Conclusion</a></li></ul></div></div><div class="styles_articleAuthor__D9xTk"><div class="styles_profilePicture__KlIB2 styles_evaluation__kiDv1"><img alt="Author Pfp" loading="lazy" src="https://images.ctfassets.net/otwaplf7zuwf/2swj8Oso6Wg566BRYgqiM3/185fe649bda0b37caadfde071fc849bf/image.png"/></div><div class="styles_infoWrap__1vXyt"><span class="styles_username__ncS1X">Jeffrey Ip</span><p class="styles_about__9LibZ">Cofounder @ Confident AI, creator of DeepEval &amp; DeepTeam. Working overtime to enforce responsible AI, with an unhealthy LLM evals addiction. Ex-Googler (YouTube), Microsoft AI (Office365).</p></div></div><div class="styles_wrap__KCCHD"><div class="styles_articleTitle__vQmJc"><h1>LLM Evaluation Metrics: The Ultimate LLM Evaluation Guide</h1></div><div class="styles_articleMeta__Bn2tH"><span class="styles_publishDate__sLyOk">Oct 10, 2025</span><span class="styles_seperator__1Yq79">.</span><span class="styles_readTime__yOHsK">16 min read</span></div></div><div class="styles_mainContent__H1BED"><div class="styles_ctaWrap__U7F7Z styles_deepEval__teOoC"><div class="styles_cta__6DjIP"><span class="styles_subHeading__1zcvn">Presenting...</span><div class="styles_headingImg__LWvUF"><svg class="styles_Text__xIZTS" fill="none" height="auto" viewbox="0 0 117 25" width="100%" xmlns="http://www.w3.org/2000/svg"><path d="M3.108 19q-.182 0-.338-.026a1.8 1.8 0 0 1-1.248-.546 1.8 1.8 0 0 1-.52-1.274q.026-1.17-.026-7.462-.052-6.318.052-7.514.026-.702.572-1.222A1.81 1.81 0 0 1 2.926.462q.234-.078.468-.078Q4.408.279 4.876.254 5.37.228 6.462.202 7.58.149 8.334.28q.78.13 1.742.364a6.7 6.7 0 0 1 1.846.676 7.7 7.7 0 0 1 1.56 1.118 7.7 7.7 0 0 1 2.028 2.938q.702 1.716.676 3.744-.026 2.002-.598 3.718a9.2 9.2 0 0 1-1.716 3.094 8.6 8.6 0 0 1-2.73 2.158q-1.637.78-4.316.91-.546.026-1.95 0zm3.562-3.562q1.924-.13 2.86-.572 1.56-.754 2.392-2.522t.598-3.926q-.234-2.184-1.482-3.328-1.014-.91-2.574-1.144-1.534-.234-3.848-.052A388 388 0 0 0 4.59 7.69q0 1.326.026 3.926t.026 3.796q1.274.052 2.028.026m18.312 3.796q-2.08 0-3.64-.884-1.56-.885-2.392-2.366-.807-1.482-.806-3.276 0-2.262.832-3.9a6.37 6.37 0 0 1 2.392-2.626q1.56-.962 3.536-.962 2.028 0 3.457.832 1.457.832 2.185 2.262.727 1.404.727 3.146 0 .885-.572 1.378-.57.495-1.533.572-1.95.182-3.459.182-1.43 0-3.067-.078l-.91-.026q.156 1.275.988 2.002.858.702 2.574.65.78-.026 1.247-.182.47-.182.832-.52l.442-.442q.235-.234.52-.364t.703-.13q.675 0 1.117.468.469.442.468 1.04 0 .832-.832 1.586t-2.157 1.196a8.2 8.2 0 0 1-2.652.442m-3.198-8.398q1.534.208 3.406.156 1.793-.026 2.625-.234-.052-1.117-.675-1.794-.6-.675-2.184-.65-1.326.026-2.158.598-.833.572-1.014 1.924m18.153 8.398q-2.08 0-3.64-.884-1.56-.885-2.392-2.366-.806-1.482-.806-3.276 0-2.262.832-3.9a6.37 6.37 0 0 1 2.392-2.626q1.56-.962 3.536-.962 2.027 0 3.458.832 1.455.832 2.184 2.262.728 1.404.728 3.146 0 .885-.572 1.378-.573.495-1.534.572-1.95.182-3.458.182-1.43 0-3.068-.078l-.91-.026q.156 1.275.988 2.002.858.702 2.574.65.78-.026 1.248-.182.468-.182.832-.52l.442-.442q.234-.234.52-.364.285-.13.702-.13.675 0 1.118.468.468.442.468 1.04 0 .832-.832 1.586t-2.158 1.196a8.2 8.2 0 0 1-2.652.442m-3.198-8.398q1.533.208 3.406.156 1.793-.026 2.626-.234-.052-1.117-.676-1.794-.6-.675-2.184-.65-1.326.026-2.158.598t-1.014 1.924m15.345 11.596a1.78 1.78 0 0 1-.572 1.274 1.65 1.65 0 0 1-1.274.494q-.755 0-1.274-.546a1.73 1.73 0 0 1-.52-1.274q.078-4.498.078-15.288 0-.754.52-1.274a1.78 1.78 0 0 1 1.3-.546q.987 0 1.534.858 2.808-1.56 5.356-.416 2.235 1.04 3.276 3.692a7.8 7.8 0 0 1 .494 2.288q.103 1.17-.182 2.444-.261 1.248-1.092 2.366a5.85 5.85 0 0 1-2.158 1.768q-2.237 1.118-4.004.598a5.1 5.1 0 0 1-1.43-.676q-.027 2.002-.052 4.238m2.522-6.916q.39.13 1.352-.338 1.014-.52 1.352-1.768.338-1.274-.13-2.704-.573-1.275-1.456-1.69-.91-.416-1.768-.078t-1.794 1.768v2.99q.519.962 1.17 1.404.65.416 1.274.416m21.715 3.51q-3.847 0-10.504-.078-.52 0-.962-.286a2 2 0 0 1-.624-.754 1.8 1.8 0 0 1-.312-1.014q.027-.91-.026-7.332-.025-4.68.234-7.358.027-.338.156-.624.209-.52.65-.832a1.76 1.76 0 0 1 1.014-.364q1.899-.025 5.122 0 3.225 0 5.122-.026.755 0 1.274.52.546.52.546 1.274 0 .728-.52 1.274-.493.52-1.248.52h-4.342a176 176 0 0 0-4.342 0 103 103 0 0 0-.104 3.744q1.248-.026 7.696-.026.755 0 1.274.52.546.52.546 1.274 0 .728-.546 1.274-.52.52-1.274.52-6.474 0-7.67.026.053 2.911.026 4.108 5.278.052 8.814.052.755 0 1.274.52.52.52.52 1.274t-.52 1.274-1.274.52m7.654-.13q-.806-.364-1.013-1.248a.3.3 0 0 0-.026-.13q-1.665-5.486-3.719-9.724a1.7 1.7 0 0 1-.156-1.3q.209-.702.832-1.04.624-.365 1.3-.156.677.182 1.014.832a82 82 0 0 1 2.809 6.604q1.507-3.822 2.99-6.578.363-.598.962-.806.597-.208 1.195.182.599.365.755 1.04.18.676-.209 1.326l-.546 1.17-1.846 3.848-.415.91-.442 1.04a54 54 0 0 0-.416.988q-.078.234-.416 1.092l-.416 1.014q-.286.65-.937.936-.624.26-1.3 0m13.21.13q-.936 0-2.028-.364a5.8 5.8 0 0 1-2.262-1.534q-.884-1.04-1.092-2.262a8 8 0 0 1-.156-1.56q0-1.741.676-3.276.936-2.158 2.626-3.484a6.07 6.07 0 0 1 3.848-1.326q.78 0 1.69.208.599.156 1.17.546h.13l.26-.026q.65 0 1.144.468.52.468.598 1.144l.182 1.794q.052 1.145.13 3.9l.052 1.404q.052 1.3.052 1.716.026.39-.026.754-.104.754-.572 1.3-.443.52-1.17.546a1.85 1.85 0 0 1-1.092-.286q-.468-.338-.676-1.196-.598.727-1.456 1.066a6.2 6.2 0 0 1-2.028.468m.026-3.666q.78 0 1.612-.494.208-.13.468-.39l.39-.364.442-.624a.9.9 0 0 1 .364-.26q0-2.184-.13-3.692l-.26-.182-.364-.286-.234-.156a3 3 0 0 0-.65-.104q-.936 0-1.82.728-.858.727-1.378 2.028-.416 1.014-.416 1.82 0 .702.312 1.17.338.468.936.676.39.13.728.13m13.5 2.158a1.78 1.78 0 0 1-.572 1.274 1.65 1.65 0 0 1-1.274.494 1.73 1.73 0 0 1-1.274-.52 1.82 1.82 0 0 1-.52-1.3q.051 0 .026-7.488l-.026-7.956q0-.754.52-1.274a1.78 1.78 0 0 1 1.3-.546q.753 0 1.274.546.546.52.546 1.274v5.174q.026 3.095.026 5.616 0 2.496-.026 4.706m5.693-1.742q.572.571.572 1.378t-.572 1.378a1.88 1.88 0 0 1-1.378.572q-.806 0-1.378-.572a1.88 1.88 0 0 1-.572-1.378q0-.806.572-1.378a1.88 1.88 0 0 1 1.378-.572q.806 0 1.378.572" fill="#000"></path></svg></div><p class="styles_description__WIAVI">The open-source LLM evaluation framework.</p><a class="styles_githubBtn__7NvGZ" href="https://github.com/confident-ai/deepeval" target="_blank"><svg height="26" viewbox="0 0 511.986 511" width="26" xml:space="preserve" xmlns="http://www.w3.org/2000/svg"><path d="M499.574 188.504c-3.199-9.922-11.988-16.938-22.398-17.899L335.82 157.762 279.926 26.926a26.02 26.02 0 0 0-23.934-15.766c-10.433 0-19.82 6.207-23.937 15.809l-55.89 130.816-141.38 12.84a25.94 25.94 0 0 0-22.379 17.879 26.03 26.03 0 0 0 7.617 27.648l106.86 93.696L95.37 448.62a26.06 26.06 0 0 0 10.137 26.879 25.66 25.66 0 0 0 15.23 4.992c4.63 0 9.239-1.234 13.356-3.71l121.898-72.895 121.875 72.875a25.96 25.96 0 0 0 28.61-1.239 26.06 26.06 0 0 0 10.132-26.882l-31.507-138.77 106.859-93.7a26.05 26.05 0 0 0 7.613-27.667zm0 0" data-original="#ffc107" fill="#ffc107"></path><path d="M114.617 491.137a27.16 27.16 0 0 1-15.957-5.184c-8.855-6.398-12.992-17.43-10.582-28.094l32.938-145.066L9.312 214.828a27.2 27.2 0 0 1-7.976-28.906 27.23 27.23 0 0 1 23.402-18.73l147.82-13.419 58.41-136.746C235.279 6.98 245.09.492 255.993.492s20.715 6.488 25.024 16.512l58.41 136.77 147.797 13.417c10.882.98 20.054 8.344 23.425 18.711 3.372 10.387.254 21.739-7.98 28.907l-111.68 97.941 32.938 145.066c2.414 10.668-1.727 21.696-10.578 28.094-8.813 6.38-20.567 6.914-29.891 1.324l-127.465-76.16-127.445 76.203a27 27 0 0 1-13.93 3.86zm141.375-112.871c4.844 0 9.64 1.3 13.953 3.859l120.278 71.938-31.086-136.942a27.19 27.19 0 0 1 8.62-26.516l105.473-92.523-139.543-12.672a27.1 27.1 0 0 1-22.593-16.469L255.992 39.895 200.844 168.96c-3.903 9.238-12.563 15.531-22.59 16.43l-139.52 12.671 105.47 92.52c7.554 6.594 10.839 16.77 8.62 26.54l-31.082 136.94 120.278-71.937c4.328-2.559 9.128-3.86 13.972-3.86zM171.406 156.44v.02zm169.153-.066v.023zm0 0" data-original="#000000"></path></svg>Star on GitHub</a></div></div><img alt="featured Image" class="styles_featuredImage__LHPsF" loading="eager" src="https://images.ctfassets.net/otwaplf7zuwf/65nURaRhfXuOMqxF3sbH0y/d195c11f1d3b6d4d899e70f7819293e0/image.png"/><div class="styles_sideBarWrap__9MJUd"><div class="styles_sidebar__HWsOS" id="sidebar"><span class="styles_sidebarHeading__Oeazn">In this story<!-- --> <img alt="down-wards facing arrow" data-nimg="1" decoding="async" height="12" loading="lazy" src="https://www.confident-ai.com/icons/arrow-down-black.svg" style="color:transparent" width="12"/></span><ul class="styles_list__bjylu"><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#tldr" target="_blank">TL;DR</a></li><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#what-are-llm-evaluation-metrics" target="_blank">What are LLM Evaluation Metrics?</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#what-makes-great-metrics" target="_blank">What makes great metrics?</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#different-ways-to-compute-metric-scores" target="_blank">Different Ways to Compute Metric Scores</a></li><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#statistical-scorers" target="_blank">Statistical Scorers</a></li><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#model-based-scorers" target="_blank">Model-Based Scorers</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#g-eval" target="_blank">G-Eval</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#dag-deep-acyclic-graph" target="_blank">DAG (Deep Acyclic Graph)</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#prometheus" target="_blank">Prometheus</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#combining-statistical-and-model-based-scorers" target="_blank">Combining Statistical and Model-Based Scorers</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#qag-score" target="_blank">QAG Score</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#gptscore" target="_blank">GPTScore</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#selfcheckgpt" target="_blank">SelfCheckGPT</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#choosing-your-evaluation-metrics" target="_blank">Choosing Your Evaluation Metrics</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#single-or-multi-turn" target="_blank">Single or Multi-Turn?</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#the-5-metric-rule" target="_blank">The 5 Metric Rule</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#ai-agent-metrics" target="_blank">AI Agent Metrics</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#task-completion" target="_blank">Task Completion</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#argument-correctness" target="_blank">Argument Correctness</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#tool-correctness" target="_blank">Tool Correctness</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#rag-metrics" target="_blank">RAG Metrics</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#faithfulness" target="_blank">Faithfulness</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#answer-relevancy" target="_blank">Answer Relevancy</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#contextual-precision" target="_blank">Contextual Precision</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#contextual-recall" target="_blank">Contextual Recall</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#contextual-relevancy" target="_blank">Contextual Relevancy</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#fine-tuning-metrics" target="_blank">Fine-Tuning Metrics</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#hallucination" target="_blank">Hallucination</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#toxicity" target="_blank">Toxicity</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#bias" target="_blank">Bias</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#use-case-specific-metrics" target="_blank">Use Case Specific Metrics</a></li><ul><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#helpfulness" target="_blank">Helpfulness</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#prompt-alignment" target="_blank">Prompt Alignment</a></li><li class="styles_childLink__3A7i5"><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#summarization" target="_blank">Summarization</a></li></ul><li><a class="styles_deepEval__P5_yR" href="https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation#conclusion" target="_blank">Conclusion</a></li></ul></div></div><div class="styles_ArticleContent__c3LJ5 styles_deepEval__2ZFQ1" id="article-content"><p>It is no secret that evaluating the outputs of Large Language Models (LLMs) is essential for anyone building robust LLM applications. Whether you're fine-tuning for accuracy, enhancing contextual relevance in a RAG pipeline, or increasing task completion rate in an AI agent, choosing the right evaluation metrics is critical. Yet, LLM evaluation remains notoriously difficult—especially when it comes to deciding <i>what</i> to measure and <i>how</i>.</p><p>Having built one of the most adopted LLM evaluation framework myself, this article will teach you everything you need to know about LLM evaluation metrics, with code samples included. Ready for the long list? Let’s begin.</p><p>(Update: For metrics evaluating AI agents, heck out <a href="https://www.confident-ai.com/blog/definitive-ai-agent-evaluation-guide" target="_blank">this new article</a>)</p><h2 id="tldr">TL;DR</h2><p>Key takeaways:</p><ul><li><p><b>LLM metrics </b>measures output quality across dimensions like correctness and relevance.</p></li><li><p><b>Common mistakes</b>: relying on traditional scorers like BLEU/ROUGE, where semantic nuance in LLM outputs is not captured.</p></li><li><p><b>LLM-as-a-judge </b>is the most reliable method—using an LLM to evaluate with natural language rubrics, but requires various techniques like G-Eval.</p></li><li><p><b>Evaluation metrics </b>in the context of LLM evaluation can be categorized as either <b>single or multi-turn</b>, targeting end-to-end LLM systems or at a component-level.</p></li><li><p><b>Metrics for AI agents</b>, RAG, chatbots, and foundational models are all different and has to be complimented with use case specific ones <!-- -->(e.g. Text-SQL, writing assistants).</p></li><li><p><b>DeepEval </b>(100% OS ⭐<a href="https://github.com/confident-ai/deepeval" target="_blank">https://github.com/confident-ai/deepeval</a>) allows anyone to implement SOTA LLM metrics in 5 lines of code.</p></li></ul><h2 id="what-are-llm-evaluation-metrics">What are LLM Evaluation Metrics?</h2><p>LLM evaluation metrics such as answer correctness, semantic similarity, and hallucination, are metrics that score an LLM system's output based on criteria you care about. They are critical to LLM evaluation, as they help quantify the performance of different LLM systems, <b>which can just be the LLM itself.</b></p><figure><img alt="An LLM Evaluation Metric Architecture" class="" loading="lazy" src="https://images.ctfassets.net/otwaplf7zuwf/2tNy3bcdnxBV6ced1QEjcW/149cebc79f9215159e79d1ac9836bc5f/image.png" style="max-width:100%"/><figcaption>An LLM Evaluation Metric Architecture</figcaption></figure><p>Here are the most important and common metrics that you will likely need before launching your LLM system into production:</p><ol><li><p><b>Answer Relevancy: </b>Determines whether an LLM output is able to address the given input in an informative and concise manner.</p></li><li><p><b>Task Completion: </b>Determines whether an LLM agent is able to complete the task it was set out to do.</p></li><li><p><b>Correctness: </b>Determines whether an LLM output is factually correct based on some ground truth.</p></li><li><p><b>Hallucination: </b>Determines whether an LLM output contains fake or made-up information.</p></li><li><p><b>Tool Correctness: </b>Determines whether an LLM agent is able to call the correct tools for a given task.</p></li><li><p><b>Contextual Relevancy: </b>Determines whether the retriever in a RAG-based LLM system is able to extract the most relevant information for your LLM as context.</p></li><li><p><b>Responsible Metrics: </b>Includes metrics such as bias and toxicity, which determines whether an LLM output contains (generally) harmful and offensive content.</p></li><li><p><b>Task-Specific Metrics: </b>Includes metrics such as summarization, which usually contains a custom criteria depending on the use-case.</p></li></ol><p>While most metrics are <b>generic</b> and necessarily, they are not sufficient to target specific use-cases. This is why you'll want at least one custom task-specific metric to make your LLM evaluation pipeline production ready (as you'll see later in the G-Eval and DAG sections). For example, if your LLM application is designed to summarize pages of news articles, you’ll need a custom LLM evaluation metric that scores based on: </p><ol><li><p>Whether the summary contains enough information from the original text.</p></li><li><p>Whether the summary contains any contradictions or hallucinations from the original text.</p></li></ol><p>Moreover, if your LLM application has a RAG-based architecture, you’ll probably need to score for the quality of the retrieval context as well. The point is, an LLM evaluation metric assesses an LLM application based on the tasks it was designed to do.<i> (Note that an LLM application can simply be the LLM itself!)</i></p><p><i>In fact, this is why </i><a href="https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method" target="_blank"><i>LLM-as-a-Judge</i></a><i> is the preferred way to compute LLM evaluation metrics, which we will talk more in-depth later:</i></p><figure><img alt="Single-Turn LLM-as-a-Judge" class="" loading="lazy" src="https://images.ctfassets.net/otwaplf7zuwf/10Hab9KEzbWQp0oJSSE0wn/ffe478562afe285b8b42026769176f04/single-turn-llm-judge.png" style="max-width:100%"/><figcaption>Single-Turn LLM-as-a-Judge</figcaption></figure><h4 id="what-makes-great-metrics">What makes great metrics?</h4><p>That brings us to one of the most important points - your choice of LLM evaluation metrics should cover <b>both the evaluation criteria of the LLM use case and the LLM system architecture:</b></p><ul><li><p><b>LLM Use Case: </b>Custom metrics specific to the task, consistent across different implementations.</p></li><li><p><b>LLM System Architecture: </b>Generic metrics (e.g., faithfulness for RAG, task completion for agents) that depend on how the system is built.</p></li></ul><p>If you decide to change your LLM system completely tomorrow for the same LLM use case, your custom metrics shouldn't change at all, and vice versa. We'll talk more about the best strategy to choose your metrics later (spoiler: you don't want to have more than 5 metrics), but before that let's go through what makes great metrics great.</p><p>Great evaluation metrics are:</p><ol><li><p><b>Quantitative. </b>Metrics should always compute a score when evaluating the task at hand. This approach enables you to set a minimum passing threshold to determine if your LLM application is “good enough” and allows you to monitor how these scores change over time as you iterate and improve your implementation.</p></li><li><p><b>Reliable. </b>As unpredictable as LLM outputs can be, the last thing you want is for an LLM evaluation metric to be equally flaky. So, although metrics evaluated using LLMs (aka. <a href="https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method" target="_blank">LLM-as-a-judge</a> or LLM-Evals), such as G-Eval and especially for DAG, are more accurate than traditional scoring methods, they are often inconsistent, which is where most LLM-Evals fall short.</p></li><li><p><b>Accurate. </b>Reliable scores are meaningless if they don’t truly represent the performance of your LLM application. In fact, the secret to making a good LLM evaluation metric great is to make it align with human expectations as much as possible.</p></li></ol><p>So the question becomes, how can LLM evaluation metrics compute reliable and accurate scores? </p><h2 id="different-ways-to-compute-metric-scores">Different Ways to Compute Metric Scores</h2><p><a href="https://www.confident-ai.com/blog/llm-testing-in-2024-top-methods-and-strategies" target="_blank">In one of my previous articles</a>, I talked about how LLM outputs are notoriously difficult to evaluate. Fortunately, there are numerous established methods available for calculating metric scores — some utilize neural networks, including embedding models and LLMs, while others are based entirely on statistical analysis.</p><figure><img alt="Types of metric scorers" class="" loading="lazy" src="https://images.ctfassets.net/otwaplf7zuwf/318a5bHCph0uVwng9NnYqJ/b483c114b5434e7a00bcf0a4c985edc4/image.png" style="max-width:100%"/><figcaption>Types of metric scorers</figcaption></figure><p>We’ll go through each method and talk about the best approach by the end of this section, so read on to find out! </p><h2 id="statistical-scorers">Statistical Scorers</h2><p>Before we begin, I want to start by saying statistical scoring methods in my opinion are non-essential to learn about, so feel free to skip straight to the “G-Eval” section if you’re in a rush. This is because statistical methods performs poorly whenever reasoning is required, making it too inaccurate as a scorer for most LLM evaluation criteria.</p><p>To quickly go through them:</p><ul><li><p>The <b>BLEU (BiLingual Evaluation Understudy) </b>scorer evaluates the output of your LLM application against annotated ground truths (or, expected outputs). It calculates the precision for each matching n-gram (n consecutive words) between an LLM output and expected output to calculate their geometric mean and applies a brevity penalty if needed.</p></li><li><p>The <b>ROUGE (Recall-Oriented Understudy for Gisting Evaluation) </b>scorer is s primarily used for evaluating text summaries from NLP models, and calculates recall by comparing the overlap of n-grams between LLM outputs and expected outputs. It determines the proportion (0–1) of n-grams in the reference that are present in the LLM output.</p></li><li><p>The <b>METEOR (Metric for Evaluation of Translation with Explicit Ordering) </b>scorer is more comprehensive since it calculates scores by assessing both precision (n-gram matches) and recall (n-gram overlaps), adjusted for word order differences between LLM outputs and expected outputs. It also leverages external linguistic databases like WordNet to account for synonyms. The final score is the harmonic mean of precision and recall, with a penalty for ordering discrepancies.</p></li><li><p><b>Levenshtein distance </b>(or edit distance, you probably recognize this as a LeetCode hard DP problem) scorer calculates the minimum number of single-character edits (insertions, deletions, or substitutions) required to change one word or text string into another, which can be useful for evaluating spelling corrections, or other tasks where the precise alignment of characters is critical.</p></li></ul><p>Since purely statistical scorers hardly not take any semantics into account and have extremely limited reasoning capabilities, they are not accurate enough for evaluating LLM outputs that are often long and complex. However, there are exceptions. For example, you'll learn later that the tool correctness metric which assess an LLM agent's tool calling accuracy (scroll down to the "Agentic Metrics" section at the bottom), uses exact-match with some conditional logic, but this is rare and should not be taken as the standard for LLM evals. </p><div class="styles_feature____Fel undefined"><div class="styles_inner__jw4kC"><div class="styles_imageWrap__uTW_Y"><svg fill="none" height="100%" viewbox="0 0 311 300" width="100%" xmlns="http://www.w3.org/2000/svg"><rect fill="#10002A" height="280" rx="140" stroke="#6E00FF" stroke-width="20" width="290.274" x="10" y="10"></rect><path d="M82 113.79v71.712c0 .828.7 1.48 1.527 1.427 17.021-1.09 35.226-7.066 48.081-18.859v-14.133h-18.729c-2.31 0-4.297-1.777-4.404-4.085a4.29 4.29 0 0 1 4.284-4.491h18.849v-14.136c-12.783-11.731-30.988-17.777-48.081-18.863A1.433 1.433 0 0 0 82 113.79m96.392 17.438v14.133h18.727c2.308 0 4.295 1.772 4.405 4.078a4.29 4.29 0 0 1-4.282 4.498h-18.85v14.135c12.783 11.732 30.989 17.777 48.081 18.863a1.43 1.43 0 0 0 1.527-1.427v-71.712c0-.828-.701-1.48-1.527-1.427-17.065 1.092-35.248 7.081-48.081 18.859m-38.208 1.872v33.097h29.632V133.1z" fill="#fff"></path></svg></div><div class="styles_textWrap__3K4sO"><div class="styles_heading__tmHca"><div class="styles_logo__Wb_hf"><svg fill="none" height="80" viewbox="0 0 311 300" width="80" xmlns="http://www.w3.org/2000/svg"><rect fill="#10002A" height="280" rx="140" stroke="#6E00FF" stroke-width="20" width="290.274" x="10" y="10"></rect><path d="M82 113.79v71.712c0 .828.7 1.48 1.527 1.427 17.021-1.09 35.226-7.066 48.081-18.859v-14.133h-18.729c-2.31 0-4.297-1.777-4.404-4.085a4.29 4.29 0 0 1 4.284-4.491h18.849v-14.136c-12.783-11.731-30.988-17.777-48.081-18.863A1.433 1.433 0 0 0 82 113.79m96.392 17.438v14.133h18.727c2.308 0 4.295 1.772 4.405 4.078a4.29 4.29 0 0 1-4.282 4.498h-18.85v14.135c12.783 11.732 30.989 17.777 48.081 18.863a1.43 1.43 0 0 0 1.527-1.427v-71.712c0-.828-.701-1.48-1.527-1.427-17.065 1.092-35.248 7.081-48.081 18.859m-38.208 1.872v33.097h29.632V133.1z" fill="#fff"></path></svg></div><h2>Confident AI: The DeepEval LLM Evaluation Platform</h2></div><p class="styles_description__PD0Ty">The leading platform to evaluate and test LLM applications on the cloud, native to DeepEval.</p><div class="styles_features__x9nln"><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Regression test and evaluate LLM apps.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Easily A|B test prompts and models.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Edit and manage datasets on the cloud.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>LLM observability with online evals.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Publicly sharable testing reports.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Automated human feedback collection.</span></div></div><div class="styles_btnWrap__RVhNW"><a class="styles_btn__Ok0Nx styles_contained__fGk_9 styles_xl__NOY9j styles_purple200__TZY9E" href="https://app.confident-ai.com/auth/signup" style="border-radius:8px;box-shadow:0 2px 5px #0000006b;text-decoration:none;line-height:1.473686em" target="_blank">Try Now for Free</a><a class="styles_ctaLink__j1zcR" href="https://github.com/confident-ai/deepeval" target="_blank">Checkout DeepEval<svg fill="none" height="18" viewbox="0 0 128 127" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M55.5 81L47.5 72.5L107 11.5H83V0H127.5V43H114.5V21.5L55.5 81Z" fill="#003EAA"></path><path d="M88 64.5H101V127H0V26H62.5V37.5H12V115.5H88V64.5Z" fill="#003EAA"></path></svg></a></div></div></div></div></div><div class="styles_ArticleContent__c3LJ5 styles_deepEval__2ZFQ1" id="article-content"><h2 id="model-based-scorers">Model-Based Scorers</h2><p>Scorers that are purely statistical are reliable but inaccurate, as they struggle to take semantics into account. In this section, it is more of the opposite — scorers that purely rely on NLP models are comparably more accurate, but are also more unreliable due to their probabilistic nature.</p><p>This shouldn't be a surprise but, <a href="https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method" target="_blank">scorers that are not LLM-based perform worse than LLM-as-a-judge</a>, also due to the same reason stated for statistical scorers. Non-LLM scorers include:</p><ul><li><p>The <b>NLI </b>scorer, which uses Natural Language Inference models (which is a type of NLP classification model) to classify whether an LLM output is logically consistent (entailment), contradictory, or unrelated (neutral) with respect to a given reference text. The score typically ranges between entailment (with a value of 1) and contradiction (with a value of 0), providing a measure of logical coherence.</p></li><li><p>The <b>BLEURT (Bilingual Evaluation Understudy with Representations from Transformers)  </b>scorer, which uses pre-trained models like BERT to score LLM outputs on some expected outputs.</p></li></ul><p>Apart from inconsistent scores, the reality is there are several shortcomings of these approaches. For example, NLI scorers can also struggle with accuracy when processing long texts, while BLEURT are limited by the quality and representativeness of its training data.</p><p>So here we go, lets talk about<a href="https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method" target="_blank"> LLM judges</a> instead.</p><h4 id="g-eval">G-Eval</h4><p>G-Eval is a recently developed framework from a <a href="https://arxiv.org/pdf/2303.16634.pdf" target="_blank">paper</a> titled “NLG Evaluation using GPT-4 with Better Human Alignment” that <b>uses LLMs to evaluate LLM outputs (aka. LLM-Evals), and is one the best ways to create task-specific metrics.</b></p><figure><img alt="G-Eval Algorithm" class="styles_round__chyrN" loading="lazy" src="https://images.ctfassets.net/otwaplf7zuwf/1RRyRJrxCQguGsxmu7hBv8/ca082c740cdf997e878ee5c842a8a0ac/image.png" style="max-width:100%"/><figcaption>G-Eval Algorithm</figcaption></figure><p>G-Eval (<a href="https://www.deepeval.com/docs/metrics-llm-evals" target="_blank">docs here</a>) first generates a series of evaluation steps using chain of thoughts (CoTs) before using the generated steps to determine the final score via a form-filling paradigm (this is just a fancy way of saying G-Eval requires several pieces of information to work). For example, evaluating LLM output coherence using G-Eval involves constructing a prompt that contains the criteria and text to be evaluated to generate evaluation steps, before using an LLM to output a score from 1 to 5 based on these steps. </p><p>Let’s run through the G-Eval algorithm using this example. First, to generate evaluation steps:</p><ol><li><p>Introduce an evaluation task to the LLM of your choice (eg. rate this output from 1–5 based on coherence)</p></li><li><p>Give a definition for your criteria (eg. “Coherence — the collective quality of all sentences in the actual output”).</p></li></ol><p><i>(Note that in the original G-Eval paper, the authors only used GPT-3.5 and GPT-4 for experiments, and having personally played around with different LLMs for G-Eval, I would highly recommend you stick with these models.)</i></p><p>After generating a series of evaluation steps:</p><ol><li><p>Create a prompt by concatenating the evaluation steps with all the arguments listed in your evaluation steps (eg., if you’re looking to evaluate coherence for an LLM output, the LLM output would be a required argument).</p></li><li><p>At the end of the prompt, ask it to generate a score between 1–5, where 5 is better than 1.</p></li><li><p>(Optional) Take the probabilities of the output tokens from the LLM to normalize the score and take their weighted summation as the final result.</p></li></ol><p>Step 3 is optional because to get the probability of the output tokens, you would need access to the raw model embeddings, which is not something guaranteed to be available for all model interfaces. This step however was introduced in the paper because it offers more fine-grained scores and minimizes bias in LLM scoring (as stated in the paper, 3 is known to have a higher token probability for a 1–5 scale).</p><p>Here are the results from the paper, which shows how G-Eval outperforms all traditional, non-LLM evals that were mentioned earlier in this article:</p><figure><img alt="A higher Spearman and Kendall-Tau correlation represents higher alignment with human judgement." class="styles_round__chyrN" loading="lazy" src="https://images.ctfassets.net/otwaplf7zuwf/23kqNq0EYkSvZ60juJZuLv/d21ec7c44eb58113dab8c946815e9cbd/image.png" style="max-width:100%"/><figcaption>A higher Spearman and Kendall-Tau correlation represents higher alignment with human judgement.</figcaption></figure><p>G-Eval is great because as an LLM-Eval it is able to take the full semantics of LLM outputs into account, making it much more accurate. And this makes a lot of sense — think about it, how can non-LLM Evals, which uses scorers that are far less capable than LLMs, possibly understand the full scope of text generated by LLMs?</p><p>Although G-Eval correlates much more with human judgment when compared to its counterparts, it can still be unreliable, as asking an LLM to come up with a score is indisputably arbitrary.</p><p>That being said, given how flexible G-Eval’s evaluation criteria can be, I’ve personally implemented G-Eval as a metric for <a href="https://github.com/confident-ai/deepeval" target="_blank">DeepEval, an open-source LLM evaluation framework</a> I’ve been working on (which includes the normalization technique from the original paper).</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-bash"><span class="token comment"># Install</span>
pip <span class="token function">install</span> deepeval
<span class="token comment"># Set OpenAI API key as env variable</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">OPENAI_API_KEY</span><span class="token operator">=</span><span class="token string">"..."</span></code></pre></div><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>test_case <span class="token keyword">import</span> LLMTestCase<span class="token punctuation">,</span> LLMTestCaseParams
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> GEval

test_case <span class="token operator">=</span> LLMTestCase<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"input to your LLM"</span><span class="token punctuation">,</span> actual_output<span class="token operator">=</span><span class="token string">"your LLM output"</span><span class="token punctuation">)</span>
coherence_metric <span class="token operator">=</span> GEval<span class="token punctuation">(</span>
    name<span class="token operator">=</span><span class="token string">"Coherence"</span><span class="token punctuation">,</span>
    criteria<span class="token operator">=</span><span class="token string">"Coherence - the collective quality of all sentences in the actual output"</span><span class="token punctuation">,</span>
    evaluation_params<span class="token operator">=</span><span class="token punctuation">[</span>LLMTestCaseParams<span class="token punctuation">.</span>ACTUAL_OUTPUT<span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

coherence_metric<span class="token punctuation">.</span>measure<span class="token punctuation">(</span>test_case<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>coherence_metric<span class="token punctuation">.</span>score<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>coherence_metric<span class="token punctuation">.</span>reason<span class="token punctuation">)</span></code></pre></div><p>G-Eval is one of the most popular ways to create LLM-as-a-judge metrics as it is simple, easy, and accurate. If you're interested, you can learn everything about G-Eval in full <a href="https://www.confident-ai.com/blog/g-eval-the-definitive-guide" target="_blank">here.</a> </p><h4 id="dag-deep-acyclic-graph">DAG (Deep Acyclic Graph)</h4><p>G-Eval is great in the case of evaluation where <b>subjectivity</b> is involved. But when you have a clear success criteria, you'll want to use a scorer that is decision-based. Imagine this: you have a text summarization use case, where you wish to format a patient's medical history in a hospital setting. You'll need various headings in the summarization, in the correct order, and only assign it a perfect score if everything is formatted correctly. In this case, where it is extremely clear what you want the score to be for a certain combination of constraints, the DAG scorer is perfect.</p><figure><img alt="DAG Decisioin Tree-Based Evaluation Architecture " class="styles_round__chyrN" loading="lazy" src="https://images.ctfassets.net/otwaplf7zuwf/4QJ8WHi9boMfwGQZ3uXpr2/eab679d258fdaaa1f8bee0a0a34c39e2/Screenshot_2025-10-10_at_4.41.53_PM.png" style="max-width:100%"/><figcaption>DAG Decisioin Tree-Based Evaluation Architecture </figcaption></figure><p>As the name suggests, the <a href="https://deepeval.com/docs/metrics-dag" target="_blank">DAG (deep acyclic graph) scorer</a> is a decision tree powered by LLM-as-a-judge, where each node is an LLM judgement and each edge is a decision. In the end, depending on the evaluation path taken, a final hard-coded score is returned (although you can also use G-Eval as a leaf node to return scores). </p><p>By breaking evaluation into fine-grained steps, we achieve deterministically. Another use case for DAG is, to filter away edge cases where your LLM output don't even meet the minimum requirement for evaluation. Back to our summarization example, this means an incorrect formatting, and often times you'll find yourself using G-Eval as a leaf node instead of a hard-coded score to return.</p><p>You can read more about why DAG works <a href="https://www.confident-ai.com/blog/why-llm-as-a-judge-is-the-best-llm-evaluation-method" target="_blank">this article here where I talk about LLM-as-a-judge</a> (highly recommended), but here is an example architecture of a DAG for text summarization:</p><p>And here is the corresponding code in DeepEval (documentation <a href="https://deepeval.com/docs/metrics-dag" target="_blank">here</a>): </p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>test_case <span class="token keyword">import</span> LLMTestCase
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>dag <span class="token keyword">import</span> <span class="token punctuation">(</span>
    DeepAcyclicGraph<span class="token punctuation">,</span>
    TaskNode<span class="token punctuation">,</span>
    BinaryJudgementNode<span class="token punctuation">,</span>
    NonBinaryJudgementNode<span class="token punctuation">,</span>
    VerdictNode<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> DAGMetric

correct_order_node <span class="token operator">=</span> NonBinaryJudgementNode<span class="token punctuation">(</span>
    criteria<span class="token operator">=</span><span class="token string">"Are the summary headings in the correct order: 'intro' =&gt; 'body' =&gt; 'conclusion'?"</span><span class="token punctuation">,</span>
    children<span class="token operator">=</span><span class="token punctuation">[</span>
        VerdictNode<span class="token punctuation">(</span>verdict<span class="token operator">=</span><span class="token string">"Yes"</span><span class="token punctuation">,</span> score<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        VerdictNode<span class="token punctuation">(</span>verdict<span class="token operator">=</span><span class="token string">"Two are out of order"</span><span class="token punctuation">,</span> score<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        VerdictNode<span class="token punctuation">(</span>verdict<span class="token operator">=</span><span class="token string">"All out of order"</span><span class="token punctuation">,</span> score<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

correct_headings_node <span class="token operator">=</span> BinaryJudgementNode<span class="token punctuation">(</span>
    criteria<span class="token operator">=</span><span class="token string">"Does the summary headings contain all three: 'intro', 'body', and 'conclusion'?"</span><span class="token punctuation">,</span>
    children<span class="token operator">=</span><span class="token punctuation">[</span>
        VerdictNode<span class="token punctuation">(</span>verdict<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> score<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        VerdictNode<span class="token punctuation">(</span>verdict<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> child<span class="token operator">=</span>correct_order_node<span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

extract_headings_node <span class="token operator">=</span> TaskNode<span class="token punctuation">(</span>
    instructions<span class="token operator">=</span><span class="token string">"Extract all headings in `actual_output`"</span><span class="token punctuation">,</span>
    evaluation_params<span class="token operator">=</span><span class="token punctuation">[</span>LLMTestCaseParams<span class="token punctuation">.</span>ACTUAL_OUTPUT<span class="token punctuation">]</span><span class="token punctuation">,</span>
    output_label<span class="token operator">=</span><span class="token string">"Summary headings"</span><span class="token punctuation">,</span>
    children<span class="token operator">=</span><span class="token punctuation">[</span>correct_headings_node<span class="token punctuation">,</span> correct_order_node<span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># create the DAG</span>
dag <span class="token operator">=</span> DeepAcyclicGraph<span class="token punctuation">(</span>root_nodes<span class="token operator">=</span><span class="token punctuation">[</span>extract_headings_node<span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment"># create the metric</span>
format_correctness <span class="token operator">=</span> DAGMetric<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"Format Correctness"</span><span class="token punctuation">,</span> dag<span class="token operator">=</span>dag<span class="token punctuation">)</span>

<span class="token comment"># create a test case</span>
test_case <span class="token operator">=</span> LLMTestCase<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"your-original-text"</span><span class="token punctuation">,</span> actual_output<span class="token operator">=</span><span class="token string">"your-summary"</span><span class="token punctuation">)</span>

<span class="token comment"># evaluate</span>
format_correctness<span class="token punctuation">.</span>measure<span class="token punctuation">(</span>test_case<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>format_correctness<span class="token punctuation">.</span>score<span class="token punctuation">,</span> format_correctness<span class="token punctuation">.</span>reason<span class="token punctuation">)</span></code></pre></div><p>The DAG metric is currently the most customizable metric available, and I built it to serve a lot of edge cases that wasn't covered by popular metrics such as answer relevancy, faithfulness, and even custom metrics such as G-Eval.</p><p>‍<a href="https://www.confident-ai.com/blog/how-i-built-deterministic-llm-evaluation-metrics-for-deepeval" target="_blank">Here is a fun read</a> more on the rationale behind building DeepEval's DAG metric.</p><h4 id="prometheus">Prometheus</h4><p>Prometheus is a fully open-source LLM that is comparable to GPT-4’s evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are provided. It is also use case agnostic, similar to G-Eval. Prometheus is a language model using <a href="https://huggingface.co/meta-llama/Llama-2-13b-chat-hf" target="_blank">Llama-2-Chat</a> as a base model and fine-tuned on 100K feedback (generated by GPT-4) within the <a href="https://huggingface.co/datasets/kaist-ai/Feedback-Collection" target="_blank">Feedback Collection</a>.</p><p>Here are the brief results from the <a href="https://arxiv.org/pdf/2310.08491.pdf" target="_blank">prometheus research paper.</a></p><figure><img alt="The reason why GPT-4’s or Prometheus’s feedback was not chosen over the other. Prometheus generates less abstract and general feedback, but tends to write overly critical ones." class="" loading="lazy" src="https://images.ctfassets.net/otwaplf7zuwf/3hmjdD4nB0r8nnfo1JAeKK/c849569791f3a32b244ced7297fcaf63/image.png" style="max-width:100%"/><figcaption>The reason why GPT-4’s or Prometheus’s feedback was not chosen over the other. Prometheus generates less abstract and general feedback, but tends to write overly critical ones.</figcaption></figure><p>Prometheus follows the same principles as G-Eval. However, there are several differences:</p><ol><li><p>While G-Eval is a framework that uses GPT-3.5/4, Prometheus is an LLM fine-tuned for evaluation.</p></li><li><p>While G-Eval generates the score rubric/evaluation steps via CoTs, the score rubric for Prometheus is provided in the prompt instead.</p></li><li><p>Prometheus requires reference/example evaluation results.</p></li></ol><p>Although I personally haven’t tried it, <a href="https://huggingface.co/kaist-ai/prometheus-13b-v1.0" target="_blank">Prometheus is available on hugging face</a>. The reason why I haven’t tried implementing it is because Prometheus was designed to make evaluation open-source instead of depending on proprietary models such as OpenAI’s GPTs. For someone aiming to build the best LLM-Evals available, it wasn’t a good fit.</p><div class="styles_feature____Fel undefined"><div class="styles_inner__jw4kC"><div class="styles_imageWrap__uTW_Y"><svg fill="none" height="100%" viewbox="0 0 311 300" width="100%" xmlns="http://www.w3.org/2000/svg"><rect fill="#10002A" height="280" rx="140" stroke="#6E00FF" stroke-width="20" width="290.274" x="10" y="10"></rect><path d="M82 113.79v71.712c0 .828.7 1.48 1.527 1.427 17.021-1.09 35.226-7.066 48.081-18.859v-14.133h-18.729c-2.31 0-4.297-1.777-4.404-4.085a4.29 4.29 0 0 1 4.284-4.491h18.849v-14.136c-12.783-11.731-30.988-17.777-48.081-18.863A1.433 1.433 0 0 0 82 113.79m96.392 17.438v14.133h18.727c2.308 0 4.295 1.772 4.405 4.078a4.29 4.29 0 0 1-4.282 4.498h-18.85v14.135c12.783 11.732 30.989 17.777 48.081 18.863a1.43 1.43 0 0 0 1.527-1.427v-71.712c0-.828-.701-1.48-1.527-1.427-17.065 1.092-35.248 7.081-48.081 18.859m-38.208 1.872v33.097h29.632V133.1z" fill="#fff"></path></svg></div><div class="styles_textWrap__3K4sO"><div class="styles_heading__tmHca"><div class="styles_logo__Wb_hf"><svg fill="none" height="80" viewbox="0 0 311 300" width="80" xmlns="http://www.w3.org/2000/svg"><rect fill="#10002A" height="280" rx="140" stroke="#6E00FF" stroke-width="20" width="290.274" x="10" y="10"></rect><path d="M82 113.79v71.712c0 .828.7 1.48 1.527 1.427 17.021-1.09 35.226-7.066 48.081-18.859v-14.133h-18.729c-2.31 0-4.297-1.777-4.404-4.085a4.29 4.29 0 0 1 4.284-4.491h18.849v-14.136c-12.783-11.731-30.988-17.777-48.081-18.863A1.433 1.433 0 0 0 82 113.79m96.392 17.438v14.133h18.727c2.308 0 4.295 1.772 4.405 4.078a4.29 4.29 0 0 1-4.282 4.498h-18.85v14.135c12.783 11.732 30.989 17.777 48.081 18.863a1.43 1.43 0 0 0 1.527-1.427v-71.712c0-.828-.701-1.48-1.527-1.427-17.065 1.092-35.248 7.081-48.081 18.859m-38.208 1.872v33.097h29.632V133.1z" fill="#fff"></path></svg></div><h2>Confident AI: The DeepEval LLM Evaluation Platform</h2></div><p class="styles_description__PD0Ty">The leading platform to evaluate and test LLM applications on the cloud, native to DeepEval.</p><div class="styles_features__x9nln"><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Regression test and evaluate LLM apps.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Easily A|B test prompts and models.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Edit and manage datasets on the cloud.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>LLM observability with online evals.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Publicly sharable testing reports.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Automated human feedback collection.</span></div></div><div class="styles_btnWrap__RVhNW"><a class="styles_btn__Ok0Nx styles_contained__fGk_9 styles_xl__NOY9j styles_purple200__TZY9E" href="https://app.confident-ai.com/auth/signup" style="border-radius:8px;box-shadow:0 2px 5px #0000006b;text-decoration:none;line-height:1.473686em" target="_blank">Try Now for Free</a><a class="styles_ctaLink__j1zcR" href="https://github.com/confident-ai/deepeval" target="_blank">Checkout DeepEval<svg fill="none" height="18" viewbox="0 0 128 127" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M55.5 81L47.5 72.5L107 11.5H83V0H127.5V43H114.5V21.5L55.5 81Z" fill="#003EAA"></path><path d="M88 64.5H101V127H0V26H62.5V37.5H12V115.5H88V64.5Z" fill="#003EAA"></path></svg></a></div></div></div></div></div><div class="styles_ArticleContent__c3LJ5 styles_deepEval__2ZFQ1" id="article-content"><h2 id="combining-statistical-and-model-based-scorers">Combining Statistical and Model-Based Scorers</h2><p>By now, we’ve seen how statistical methods are reliable but inaccurate, and how non-LLM model-based approaches are less reliable but more accurate. Similar to the previous section, there are non-LLM scorers such as:</p><ul><li><p>The <b>BERTScore </b>scorer, which relies on pre-trained language models like BERT and computes the cosine similarity between the contextual embeddings of words in the reference and the generated texts. These similarities are then aggregated to produce a final score. A higher BERTScore indicates a greater degree of semantic overlap between the LLM output and the reference text.</p></li><li><p>The <b>MoverScore </b>scorer, which first uses embedding models, specifically pre-trained language models like BERT to obtain deeply contextualized word embeddings for both the reference text and the generated text before using something called the Earth Mover’s Distance (EMD) to compute the minimal cost that must be paid to transform the distribution of words in an LLM output to the distribution of words in the reference text.</p></li></ul><p>Both the BERTScore and MoverScore scorer is vulnerable to contextual awareness and bias due to their reliance on contextual embeddings from pre-trained models like BERT. But what about LLM-Evals?</p><h4 id="qag-score">QAG Score</h4><p>QAG (Question Answer Generation) Score is a scorer that leverages LLMs’ high reasoning capabilities to reliably evaluate LLM outputs. It uses confined answers (usually either a ‘yes’ or ‘no’) to close-ended questions (which can be generated or preset) to compute a final metric score. It is reliable because it does NOT use LLMs to directly generate scores. For example, if you want to compute a score for faithfulness (which measures whether an LLM output was hallucinated or not), you would:</p><ol><li><p>Use an LLM to extract all claims made in an LLM output.</p></li><li><p>For each claim, ask the ground truth whether it agrees (‘yes’) or not (‘no’) with the claim made.</p></li></ol><p>So for this example LLM output:</p><blockquote><p>Martin Luther King Jr., the renowned civil rights leader, was assassinated on April 4, 1968, at the Lorraine Motel in Memphis, Tennessee. He was in Memphis to support striking sanitation workers and was fatally shot by James Earl Ray, an escaped convict, while standing on the motel’s second-floor balcony.</p></blockquote><p>A claim would be:</p><blockquote><p>Martin Luther King Jr. was assassinated on April 4th, 1968</p></blockquote><p>And a corresponding close-ended question would be:</p><blockquote><p>Was Martin Luther King Jr. assassinated on April 4th, 1968?</p></blockquote><p>You would then take this question, and ask whether the ground truth agrees with the claim. In the end, you will have a number of ‘yes’ and ‘no’ answers, which you can use to compute a score via some mathematical formula of your choice.</p><p>In the case of faithfulness, if we define it as as the proportion of claims in an LLM output that are accurate and consistent with the ground truth, it can easily be calculated by dividing the number of accurate (truthful) claims by the total number of claims made by the LLM. Since we are not using LLMs to directly generate evaluation scores but still leveraging its superior reasoning ability, we get scores that are both accurate and reliable.</p><h4 id="gptscore">GPTScore</h4><p>Unlike G-Eval which directly performs the evaluation task with a form-filling paradigm, <a href="https://arxiv.org/pdf/2302.04166.pdf" target="_blank">GPTScore uses the conditional probability of generating the target text as an evaluation metric.</a></p><figure><img alt="GPTScore Algorithm" class="" loading="lazy" src="https://images.ctfassets.net/otwaplf7zuwf/6aAh4MywUx4sZgDNNfbXcL/c7bd214ef932b8025a712a8ffb45c0bc/image.png" style="max-width:100%"/><figcaption>GPTScore Algorithm</figcaption></figure><h4 id="selfcheckgpt">SelfCheckGPT</h4><p>SelfCheckGPT is an odd one. <a href="https://arxiv.org/pdf/2303.08896.pdf" target="_blank">It is a simple sampling-based approach that is used to fact-check LLM outputs.</a> It assumes that hallucinated outputs are not reproducible, whereas if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts.</p><p>SelfCheckGPT is an interesting approach because it makes detecting hallucination a reference-less process, which is extremely useful in a production setting.</p><figure><img alt="SelfCheckGPT Algorithm" class="" loading="lazy" src="https://images.ctfassets.net/otwaplf7zuwf/4MwKMFQDqU4jOhd4sQMvq9/213395efc7c1391e8c94238ee4903475/image.png" style="max-width:100%"/><figcaption>SelfCheckGPT Algorithm</figcaption></figure><p>However, although you’ll notice that G-Eval and Prometheus is use case agnostic, SelfCheckGPT is not. It is only suitable for hallucination detection, and not for evaluating other use cases such as summarization, coherence, etc. </p><h2 id="choosing-your-evaluation-metrics">Choosing Your Evaluation Metrics</h2><p>The choice of which LLM evaluation metric to use depends on the use case and architecture of your LLM application. Our experience tells us that you don't want more than 5 LLM evaluation metrics in your evaluation pipeline. As you'll see later, most metrics look extremely attractive - I mean, who doesn't want to prevent biases for their internal RAG QA app? </p><h4 id="single-or-multi-turn">Single or Multi-Turn?</h4><p>So there’s actually one more reason why traditional model-based and statistical scorers don’t work that I haven’t been talking about, and it is because traditional scorers cannot evaluate multi-turn use cases.</p><p>Throughout this article up until now, what we’ve actually been discussing are single-turn metrics only. This means we are only evaluating a single end-to-end interaction with your LLM system. This covers use cases such as single-turn agents systems, RAG pipelines, but not chatbots.</p><p>Multi-turn LLM systems involves use cases such as RAG chatbots, conversational agents, and voice AI agents. Metrics that excel for multi-turn evaluation involves taking the entire turn history into context before running evals. Here’s a visual example of a multi-turn G-Eval metric:</p><figure><img alt="Multi-Turn LLM-as-a-Judge" class="" loading="lazy" src="https://images.ctfassets.net/otwaplf7zuwf/17MTvuql2ksP5av1rQJIFW/51da753a7087facba5fcbf62157cd4e1/multi-turn-llm-judge.png" style="max-width:100%"/><figcaption>Multi-Turn LLM-as-a-Judge</figcaption></figure><p>When choosing your metrics, the first thing to identify is whether your use case is multi or single-turn. Multi-turn use cases are more difficult to evaluate, and for AI agents, it is not uncommon to confuse a single-turn agent for a multi-turn one when agents are talking to other swarms of agents. If you want to sanity check yourself and not fall into the same trap, <a href="https://www.confident-ai.com/blog/definitive-ai-agent-evaluation-guide" target="_blank">click here to learn more.</a></p><h4 id="the-5-metric-rule">The 5 Metric Rule</h4><p>The truth is, when you're evaluating everything, you're evaluating nothing at all. Too much data != good. You'll want:</p><ul><li><p>1-2 custom metrics (G-Eval or DAG) that are use case specific</p></li><li><p>2-3 generic metrics (RAG, agentic, or conversational) that are system specific</p></li></ul><p>These are rough numbers and it depends on the complexity of your system. </p><figure><img alt="Metrics selection flowchart taken from [Confident AI docs. ](https://documentation.confident-ai.com)" class="" loading="lazy" src="https://images.ctfassets.net/otwaplf7zuwf/7LQqgoaUIVCxJjI6vuunf7/b4c59330767b43596b8864a35f664f39/image.png" style="max-width:100%"/><figcaption>Metrics selection flowchart taken from <a href="https://documentation.confident-ai.com" rel="noopener noreferrer" target="_blank">Confident AI docs. </a></figcaption></figure><p>For example, if you’re building a RAG-based customer support chatbot on top of OpenAI’s models with tool calling capabilities, you’ll want 3 RAG metrics (eg., faithfulness, answer relevancy, contextual relevancy) and 1 agentic metric (e.g. tool correctness) to evaluate the system, and 1 custom metric built using G-Eval that evaluates something like brand voice or helpfulness.</p><p>Another useful tip of deciding whether to use G-Eval or DAG is, if the criteria is purely subjective, use G-Eval. Otherwise use DAG. I say "purely", because you can also use G-Eval as one of the nodes in DAG.</p><p>In this final section, we’ll be going over the evaluation metrics you absolutely need to know. <i>(And as a bonus, the implementation of each.)</i></p><h2 id="ai-agent-metrics">AI Agent Metrics</h2><p>If you're into AI agent evaluation, I would highly recommend this <a href="https://www.confident-ai.com/blog/definitive-ai-agent-evaluation-guide" target="_blank">complete AI agent evaluation guide</a> that goes in much more depth. Here, we'll go through the common metrics you'll likely require if your system involves agentic workflows. But first let's understand what are AI agents and what makes them different.</p><p>AI agents <b>can be single or multi-turn</b> LLM systems that uses an LLM to invoke tools in order to complete a task at hand. Visually, this is what an AI agent looks like:</p><figure><img alt="Single-turn AI agent with tools and ability to handoff to other agents" class="styles_round__chyrN" loading="lazy" src="https://images.ctfassets.net/otwaplf7zuwf/U833Rl3xfX0xq7UCDbpgA/b57e854f9f8444639b12773f9cee77f8/ai-agent.png" style="max-width:100%"/><figcaption>Single-turn AI agent with tools and ability to handoff to other agents</figcaption></figure><p>The idea is we will evaluate the end-to-end degree of task completion as well as its ability to call the correct tools with the correct arguments. Another thing to note is, since agents are much more complex in architecture we simply cannot use simple "test cases" for metrics. A better approach would be to "trace" your AI agent so you can construct multiple test cases across your agent for metrics to act on:</p><figure><img alt="Metrics Applied on a Span (component) Level" class="styles_round__chyrN" loading="lazy" src="https://images.ctfassets.net/otwaplf7zuwf/r4Kt5g2y2AUyRm0i10I1K/867bb5822a27a870de15fce87a2a316d/component-level-evals.png" style="max-width:100%"/><figcaption>Metrics Applied on a Span (component) Level</figcaption></figure><p>The following examples will use LLM tracing as code examples as a quick overview. <a href="https://www.confident-ai.com/blog/definitive-ai-agent-evaluation-guide" target="_blank">Click here</a> to learn more about agentic evals in-depth.</p><h4 id="task-completion">Task Completion</h4><p>Task completion is <b>single-turn, end-to-end</b> agentic metric that uses LLM-as-a-judge to evaluate whether your LLM agent is able to accomplish its given task. The given task is inferred from the input it was provided with to kickstart the agentic workflow, while the entire execution process is used to determine the degree of completion of such task.</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>tracing <span class="token keyword">import</span> observe
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> TaskCompletionMetric

<span class="token decorator annotation punctuation">@observe</span><span class="token punctuation">(</span>metrics<span class="token operator">=</span><span class="token punctuation">[</span>TaskCompletionMetric<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">trip_planner_agent</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    destination <span class="token operator">=</span> <span class="token string">"Paris"</span>
    days <span class="token operator">=</span> <span class="token number">2</span>

    <span class="token decorator annotation punctuation">@observe</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">restaurant_finder</span><span class="token punctuation">(</span>city<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token string">"Le Jules Verne"</span><span class="token punctuation">,</span> <span class="token string">"Angelina Paris"</span><span class="token punctuation">,</span> <span class="token string">"Septime"</span><span class="token punctuation">]</span>

    <span class="token decorator annotation punctuation">@observe</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">itinerary_generator</span><span class="token punctuation">(</span>destination<span class="token punctuation">,</span> days<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token string">"Eiffel Tower"</span><span class="token punctuation">,</span> <span class="token string">"Louvre Museum"</span><span class="token punctuation">,</span> <span class="token string">"Montmartre"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span>days<span class="token punctuation">]</span>

    itinerary <span class="token operator">=</span> itinerary_generator<span class="token punctuation">(</span>destination<span class="token punctuation">,</span> days<span class="token punctuation">)</span>
    restaurants <span class="token operator">=</span> restaurant_finder<span class="token punctuation">(</span>destination<span class="token punctuation">)</span>

    <span class="token keyword">return</span> itinerary <span class="token operator">+</span> restaurants</code></pre></div><p>I know this might be a new concept, especially on LLM tracing, so here are some useful resources on DeepEval's docs that you can learn more about:</p><ul><li><p><a href="https://deepeval.com/docs/evaluation-llm-tracing" target="_blank">LLM tracing</a></p></li><li><p><a href="https://deepeval.com/docs/metrics-task-completion" target="_blank">Task completion metric</a></p></li></ul><h4 id="argument-correctness">Argument Correctness</h4><p>Argument correctness is a <b>component-level</b> LLM-as-a-judge metric that evaluates an LLM’s ability to call tools by generating the correct arguments. It works by assessing whether the input parameters make sense depending on the input to an AI agent:</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> openai <span class="token keyword">import</span> OpenAI
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>tracing <span class="token keyword">import</span> observe
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> ArgumentCorrectness

<span class="token decorator annotation punctuation">@observe</span><span class="token punctuation">(</span>metrics<span class="token operator">=</span><span class="token punctuation">[</span>ArgumentCorrectness<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">trip_planner_agent</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    client <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>

   <span class="token decorator annotation punctuation">@observe</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">"tool"</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">web_search</span><span class="token punctuation">(</span>query<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token string">"Results from web"</span>

    res <span class="token operator">=</span> client<span class="token punctuation">.</span>chat<span class="token punctuation">.</span>completions<span class="token punctuation">.</span>create<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>
    res <span class="token operator">=</span> web_search<span class="token punctuation">(</span>res<span class="token punctuation">)</span> <span class="token comment"># Modify this to check for res type</span>
    <span class="token keyword">return</span> res</code></pre></div><p>You can find the docs for this metric <a href="https://deepeval.com/docs/metrics-argument-correctness" target="_blank">here.</a> </p><h4 id="tool-correctness">Tool Correctness</h4><p>Tool correctness is a <b>component-level agentic</b> metric that assesses the quality of your agentic systems, and is the most unusual metric here because it is based on exact matching and not any LLM-as-a-judge. A common misconception here, similar to the argument correctness metric, is that it assesses tools called. </p><p>While this is true to some degree, it actually assess an LLM's ability to pick the right tools and actually call it, instead of the tool calling itself. It is computed by comparing the tools called for a given input to the expected tools that should be called:</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>test_case <span class="token keyword">import</span> LLMTestCase<span class="token punctuation">,</span> ToolCall
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> ToolCorrectnessMetric

test_case <span class="token operator">=</span> LLMTestCase<span class="token punctuation">(</span>
    <span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"What if these shoes don't fit?"</span><span class="token punctuation">,</span>
    actual_output<span class="token operator">=</span><span class="token string">"We offer a 30-day full refund at no extra cost."</span><span class="token punctuation">,</span>
    <span class="token comment"># Replace this with the tools that was actually used by your LLM agent</span>
    tools_called<span class="token operator">=</span><span class="token punctuation">[</span>ToolCall<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"WebSearch"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> ToolCall<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"ToolQuery"</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    expected_tools<span class="token operator">=</span><span class="token punctuation">[</span>ToolCall<span class="token punctuation">(</span>name<span class="token operator">=</span><span class="token string">"WebSearch"</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
metric <span class="token operator">=</span> ToolCorrectnessMetric<span class="token punctuation">(</span><span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>measure<span class="token punctuation">(</span>test_case<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>score<span class="token punctuation">,</span> metric<span class="token punctuation">.</span>reason<span class="token punctuation">)</span></code></pre></div><p>In this example, the tools are "WebSearch" and "ToolQuery". You can find the docs for this metric <a href="https://deepeval.com/docs/metrics-tool-correctness" target="_blank">here.</a> </p><h2 id="rag-metrics">RAG Metrics</h2><p>For those don’t already know what RAG (Retrieval Augmented Generation) is, <a href="https://www.confident-ai.com/blog/rag-evaluation-metrics-answer-relevancy-faithfulness-and-more" target="_blank">here is a great read</a>. But in a nutshell, RAG serves as a method to supplement LLMs with extra context to generate tailored outputs, and is great for building chatbots. It is made up of two components — the retriever, and the generator.</p><figure><img alt="A RAG Pipeline Architecture" class="styles_round__chyrN" loading="lazy" src="https://images.ctfassets.net/otwaplf7zuwf/2aXqN1u0QPT1ST23Na7r8Y/03a9cb3f9206ed66362adef4ebcd3631/image.png" style="max-width:100%"/><figcaption>A RAG Pipeline Architecture</figcaption></figure><p>Here’s how a RAG workflow typically works:</p><ol><li><p>Your RAG system receives an input.</p></li><li><p>The <b>retriever </b>uses this input to perform a vector search in your knowledge base (which nowadays in most cases is a vector database).</p></li><li><p>The <b>generator </b>receives the retrieval context and the user input as additional context to generate a tailor output.</p></li></ol><p>Here’s one thing to remember — <b>high quality LLM outputs is the product of a great retriever and generator. </b>For this reason, great RAG metrics focuses on evaluating either your RAG retriever or generator in a reliable and accurate way. (In fact, <a href="https://arxiv.org/pdf/2309.15217.pdf" target="_blank">RAG metrics were originally designed to be reference-less metrics</a>, meaning they don’t require ground truths, making them usable even in a production setting.)</p><p>(PS. For those looking to unit test RAG systems in CI/CD pipelines, <a href="https://www.confident-ai.com/blog/how-to-evaluate-rag-applications-in-ci-cd-pipelines-with-deepeval" target="_blank">click here.</a>)</p><h4 id="faithfulness">Faithfulness</h4><p>Faithfulness is a RAG metric that evaluates whether the LLM/generator in your RAG pipeline is generating LLM outputs that factually aligns with the information presented in the retrieval context. But which scorer should we use for the faithfulness metric?</p><p><b>Spoiler alert: The QAG Scorer is the best scorer for RAG metrics since it excels for evaluation tasks where the objective is clear.</b> </p><p>For faithfulness, if you define it as the proportion of truthful claims made in an LLM output with regards to the retrieval context, we can calculate faithfulness using QAG by following this algorithm:</p><ol><li><p>Use LLMs to extract all claims made in the output.</p></li><li><p>For each claim, check whether the it agrees or contradicts with each individual node in the retrieval context. In this case, the close-ended question in QAG will be something like: “Does the given claim agree with the reference text”, where the “reference text” will be each individual retrieved node. (</p><p><i>Note that you need to confine the answer to either a ‘yes’, ‘no’, or ‘idk’. The ‘idk’ state represents the edge case where the retrieval context does not contain relevant information to give a yes/no answer.)</i></p></li><li><p>Add up the total number of truthful claims (‘yes’ and ‘idk’), and divide it by the total number of claims made.</p></li></ol><p>This method ensures accuracy by using LLM’s advanced reasoning capabilities while avoiding unreliability in LLM generated scores, making it a better scoring method than G-Eval.</p><p>If you feel this is too complicated to implement, you can use <a href="https://github.com/confident-ai/deepeval" target="_blank">DeepEval. It’s an open-source package I built and offers all the evaluation metrics you need for LLM evaluation, including the faithfulness metric</a>.</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-bash"><span class="token comment"># Install</span>
pip <span class="token function">install</span> deepeval
<span class="token comment"># Set OpenAI API key as env variable</span>
<span class="token builtin class-name">export</span> <span class="token assign-left variable">OPENAI_API_KEY</span><span class="token operator">=</span><span class="token string">"..."</span></code></pre></div><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> FaithfulnessMetric
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>test_case <span class="token keyword">import</span> LLMTestCase

test_case<span class="token operator">=</span>LLMTestCase<span class="token punctuation">(</span>
  <span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"..."</span><span class="token punctuation">,</span> 
  actual_output<span class="token operator">=</span><span class="token string">"..."</span><span class="token punctuation">,</span>
  retrieval_context<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"..."</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>
metric <span class="token operator">=</span> FaithfulnessMetric<span class="token punctuation">(</span>threshold<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>measure<span class="token punctuation">(</span>test_case<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>score<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>reason<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>is_successful<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre></div><p>DeepEval treats evaluation as test cases. Here, actual_output is simply your LLM output. Also, since faithfulness is an LLM-Eval, you’re able to get a reasoning for the final calculated score. </p><div class="styles_feature____Fel undefined"><div class="styles_inner__jw4kC"><div class="styles_imageWrap__uTW_Y"><svg fill="none" height="100%" viewbox="0 0 311 300" width="100%" xmlns="http://www.w3.org/2000/svg"><rect fill="#10002A" height="280" rx="140" stroke="#6E00FF" stroke-width="20" width="290.274" x="10" y="10"></rect><path d="M82 113.79v71.712c0 .828.7 1.48 1.527 1.427 17.021-1.09 35.226-7.066 48.081-18.859v-14.133h-18.729c-2.31 0-4.297-1.777-4.404-4.085a4.29 4.29 0 0 1 4.284-4.491h18.849v-14.136c-12.783-11.731-30.988-17.777-48.081-18.863A1.433 1.433 0 0 0 82 113.79m96.392 17.438v14.133h18.727c2.308 0 4.295 1.772 4.405 4.078a4.29 4.29 0 0 1-4.282 4.498h-18.85v14.135c12.783 11.732 30.989 17.777 48.081 18.863a1.43 1.43 0 0 0 1.527-1.427v-71.712c0-.828-.701-1.48-1.527-1.427-17.065 1.092-35.248 7.081-48.081 18.859m-38.208 1.872v33.097h29.632V133.1z" fill="#fff"></path></svg></div><div class="styles_textWrap__3K4sO"><div class="styles_heading__tmHca"><div class="styles_logo__Wb_hf"><svg fill="none" height="80" viewbox="0 0 311 300" width="80" xmlns="http://www.w3.org/2000/svg"><rect fill="#10002A" height="280" rx="140" stroke="#6E00FF" stroke-width="20" width="290.274" x="10" y="10"></rect><path d="M82 113.79v71.712c0 .828.7 1.48 1.527 1.427 17.021-1.09 35.226-7.066 48.081-18.859v-14.133h-18.729c-2.31 0-4.297-1.777-4.404-4.085a4.29 4.29 0 0 1 4.284-4.491h18.849v-14.136c-12.783-11.731-30.988-17.777-48.081-18.863A1.433 1.433 0 0 0 82 113.79m96.392 17.438v14.133h18.727c2.308 0 4.295 1.772 4.405 4.078a4.29 4.29 0 0 1-4.282 4.498h-18.85v14.135c12.783 11.732 30.989 17.777 48.081 18.863a1.43 1.43 0 0 0 1.527-1.427v-71.712c0-.828-.701-1.48-1.527-1.427-17.065 1.092-35.248 7.081-48.081 18.859m-38.208 1.872v33.097h29.632V133.1z" fill="#fff"></path></svg></div><h2>Confident AI: The DeepEval LLM Evaluation Platform</h2></div><p class="styles_description__PD0Ty">The leading platform to evaluate and test LLM applications on the cloud, native to DeepEval.</p><div class="styles_features__x9nln"><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Regression test and evaluate LLM apps.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Easily A|B test prompts and models.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Edit and manage datasets on the cloud.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>LLM observability with online evals.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Publicly sharable testing reports.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Automated human feedback collection.</span></div></div><div class="styles_btnWrap__RVhNW"><a class="styles_btn__Ok0Nx styles_contained__fGk_9 styles_xl__NOY9j styles_purple200__TZY9E" href="https://app.confident-ai.com/auth/signup" style="border-radius:8px;box-shadow:0 2px 5px #0000006b;text-decoration:none;line-height:1.473686em" target="_blank">Try Now for Free</a><a class="styles_ctaLink__j1zcR" href="https://github.com/confident-ai/deepeval" target="_blank">Checkout DeepEval<svg fill="none" height="18" viewbox="0 0 128 127" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M55.5 81L47.5 72.5L107 11.5H83V0H127.5V43H114.5V21.5L55.5 81Z" fill="#003EAA"></path><path d="M88 64.5H101V127H0V26H62.5V37.5H12V115.5H88V64.5Z" fill="#003EAA"></path></svg></a></div></div></div></div></div><div class="styles_ArticleContent__c3LJ5 styles_deepEval__2ZFQ1" id="article-content"><h4 id="answer-relevancy">Answer Relevancy</h4><p>Answer relevancy is a RAG metric that assesses whether your RAG generator outputs concise answers, and can be calculated by determining the proportion of sentences in an LLM output that a relevant to the input (ie. divide the number relevant sentences by the total number of sentences).</p><p>The key to build a robust answer relevancy metric is to take the retrieval context into account, since additional context may justify a seemingly irrelevant sentence’s relevancy. Here’s an implementation of the answer relevancy metric:</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> AnswerRelevancyMetric
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>test_case <span class="token keyword">import</span> LLMTestCase

test_case<span class="token operator">=</span>LLMTestCase<span class="token punctuation">(</span>
  <span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"..."</span><span class="token punctuation">,</span> 
  actual_output<span class="token operator">=</span><span class="token string">"..."</span><span class="token punctuation">,</span>
  retrieval_context<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"..."</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>
metric <span class="token operator">=</span> AnswerRelevancyMetric<span class="token punctuation">(</span>threshold<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>measure<span class="token punctuation">(</span>test_case<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>score<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>reason<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>is_successful<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre></div><p><i>(Remember, we’re using QAG for all RAG metrics)</i></p><h4 id="contextual-precision">Contextual Precision</h4><p>Contextual Precision is a RAG metric that assesses the quality of your RAG pipeline’s retriever. When we’re talking about contextual metrics, we’re mainly concerned about the relevancy of the retrieval context. A high contextual precision score means nodes that are relevant in the retrieval contextual are ranked higher than irrelevant ones. This is important because LLMs gives more weighting to information in nodes that appear earlier in the retrieval context, which affects the quality of the final output.</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> ContextualPrecisionMetric
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>test_case <span class="token keyword">import</span> LLMTestCase

test_case<span class="token operator">=</span>LLMTestCase<span class="token punctuation">(</span>
  <span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"..."</span><span class="token punctuation">,</span> 
  actual_output<span class="token operator">=</span><span class="token string">"..."</span><span class="token punctuation">,</span>
  <span class="token comment"># Expected output is the "ideal" output of your LLM, it is an</span>
  <span class="token comment"># extra parameter that's needed for contextual metrics</span>
  expected_output<span class="token operator">=</span><span class="token string">"..."</span><span class="token punctuation">,</span>
  retrieval_context<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"..."</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>
metric <span class="token operator">=</span> ContextualPrecisionMetric<span class="token punctuation">(</span>threshold<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>measure<span class="token punctuation">(</span>test_case<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>score<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>reason<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>is_successful<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre></div><h4 id="contextual-recall">Contextual Recall</h4><p>Contextual Precision is an additional metric for evaluating a Retriever-Augmented Generator (RAG). It is calculated by determining the proportion of sentences in the expected output or ground truth that can be attributed to nodes in the retrieval context. A higher score represents a greater alignment between the retrieved information and the expected output, indicating that the retriever is effectively sourcing relevant and accurate content to aid the generator in producing contextually appropriate responses.</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> ContextualRecallMetric
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>test_case <span class="token keyword">import</span> LLMTestCase

test_case<span class="token operator">=</span>LLMTestCase<span class="token punctuation">(</span>
  <span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"..."</span><span class="token punctuation">,</span> 
  actual_output<span class="token operator">=</span><span class="token string">"..."</span><span class="token punctuation">,</span>
  <span class="token comment"># Expected output is the "ideal" output of your LLM, it is an</span>
  <span class="token comment"># extra parameter that's needed for contextual metrics</span>
  expected_output<span class="token operator">=</span><span class="token string">"..."</span><span class="token punctuation">,</span>
  retrieval_context<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"..."</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>
metric <span class="token operator">=</span> ContextualRecallMetric<span class="token punctuation">(</span>threshold<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>measure<span class="token punctuation">(</span>test_case<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>score<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>reason<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>is_successful<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre></div><h4 id="contextual-relevancy">Contextual Relevancy</h4><p>Probably the simplest metric to understand, contextual relevancy is simply the proportion of sentences in the retrieval context that are relevant to a given input.</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> ContextualRelevancyMetric
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>test_case <span class="token keyword">import</span> LLMTestCase

test_case<span class="token operator">=</span>LLMTestCase<span class="token punctuation">(</span>
  <span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"..."</span><span class="token punctuation">,</span> 
  actual_output<span class="token operator">=</span><span class="token string">"..."</span><span class="token punctuation">,</span>
  retrieval_context<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"..."</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>
metric <span class="token operator">=</span> ContextualRelevancyMetric<span class="token punctuation">(</span>threshold<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>measure<span class="token punctuation">(</span>test_case<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>score<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>reason<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>is_successful<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre></div><h2 id="fine-tuning-metrics">Fine-Tuning Metrics</h2><p>When I say “fine-tuning metrics”, what I really mean is metrics that assess the LLM itself, rather than the entire system. Putting aside cost and performance benefits, LLMs are often fine-tuned to either:</p><ol><li><p>Incorporate additional contextual knowledge.</p></li><li><p>Adjust its behavior.</p></li></ol><p>If you're looking to fine-tune your own models, here is a<a href="https://www.confident-ai.com/blog/the-ultimate-guide-to-fine-tune-llama-2-with-llm-evaluations" target="_blank"> step-by-step tutorial on how to fine-tune LLaMA-2</a> in under 2 hours, all within Google Colab, with evaluations.</p><h4 id="hallucination">Hallucination</h4><p>Some of you might recognize this being the same as the faithfulness metric. Although similar, hallucination in fine-tuning is more complicated since it is often difficult to pinpoint the exact ground truth for a given output. To go around this problem, we can take advantage of SelfCheckGPT’s zero-shot approach to sample the proportion of hallucinated sentences in an LLM output.</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> HallucinationMetric
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>test_case <span class="token keyword">import</span> LLMTestCase

test_case<span class="token operator">=</span>LLMTestCase<span class="token punctuation">(</span>
  <span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"..."</span><span class="token punctuation">,</span> 
  actual_output<span class="token operator">=</span><span class="token string">"..."</span><span class="token punctuation">,</span>
  <span class="token comment"># Note that 'context' is not the same as 'retrieval_context'.</span>
  <span class="token comment"># While retrieval context is more concerned with RAG pipelines,</span>
  <span class="token comment"># context is the ideal retrieval results for a given input,</span>
  <span class="token comment"># and typically resides in the dataset used to fine-tune your LLM</span>
  context<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"..."</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
metric <span class="token operator">=</span> HallucinationMetric<span class="token punctuation">(</span>threshold<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>measure<span class="token punctuation">(</span>test_case<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>score<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>is_successful<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre></div><p>However, this approach can get very expensive, so for now I would suggest using an NLI scorer and manually provide some context as the ground truth instead. </p><h4 id="toxicity">Toxicity</h4><p>The toxicity metric evaluates the extent to which a text contains offensive, harmful, or inappropriate language. Off-the-shelf pre-trained models like Detoxify, which utilize the BERT scorer, can be employed to score toxicity.</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> ToxicityMetric
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>test_case <span class="token keyword">import</span> LLMTestCase

metric <span class="token operator">=</span> ToxicityMetric<span class="token punctuation">(</span>threshold<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>
test_case <span class="token operator">=</span> LLMTestCase<span class="token punctuation">(</span>
    <span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"What if these shoes don't fit?"</span><span class="token punctuation">,</span>
    <span class="token comment"># Replace this with the actual output from your LLM application</span>
    actual_output <span class="token operator">=</span> <span class="token string">"We offer a 30-day full refund at no extra cost."</span>
<span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>measure<span class="token punctuation">(</span>test_case<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>score<span class="token punctuation">)</span></code></pre></div><p>However, this method can be inaccurate since words “associated with swearing, insults or profanity are present in a comment, is likely to be classified as toxic, regardless of the tone or the intent of the author e.g. humorous/self-deprecating”.</p><p>In this case, you might want to consider using G-Eval instead to define a custom criteria for toxicity. In fact, the use case agnostic nature of G-Eval the main reason why I like it so much.</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> GEval
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>test_case <span class="token keyword">import</span> LLMTestCase

test_case <span class="token operator">=</span> LLMTestCase<span class="token punctuation">(</span>
    <span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"What if these shoes don't fit?"</span><span class="token punctuation">,</span>
    <span class="token comment"># Replace this with the actual output from your LLM application</span>
    actual_output <span class="token operator">=</span> <span class="token string">"We offer a 30-day full refund at no extra cost."</span>
<span class="token punctuation">)</span>
toxicity_metric <span class="token operator">=</span> GEval<span class="token punctuation">(</span>
    name<span class="token operator">=</span><span class="token string">"Toxicity"</span><span class="token punctuation">,</span>
    criteria<span class="token operator">=</span><span class="token string">"Toxicity - determine if the actual outout contains any non-humorous offensive, harmful, or inappropriate language"</span><span class="token punctuation">,</span>
    evaluation_params<span class="token operator">=</span><span class="token punctuation">[</span>LLMTestCaseParams<span class="token punctuation">.</span>ACTUAL_OUTPUT<span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>measure<span class="token punctuation">(</span>test_case<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>score<span class="token punctuation">)</span></code></pre></div><h4 id="bias">Bias</h4><p>The bias metric evaluates aspects such as political, gender, and social biases in textual content. This is particularly crucial for applications where a custom LLM is involved in decision-making processes. For example, aiding in bank loan approvals with unbiased recommendations, or in recruitment, where it assists in determining if a candidate should be shortlisted for an interview.</p><p>Similar to toxicity, bias can be evaluated using G-Eval. (But don’t get me wrong, QAG can also be a viable scorer for metrics like toxicity and bias.)</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> GEval
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>test_case <span class="token keyword">import</span> LLMTestCase

test_case <span class="token operator">=</span> LLMTestCase<span class="token punctuation">(</span>
    <span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"What if these shoes don't fit?"</span><span class="token punctuation">,</span>
    <span class="token comment"># Replace this with the actual output from your LLM application</span>
    actual_output <span class="token operator">=</span> <span class="token string">"We offer a 30-day full refund at no extra cost."</span>
<span class="token punctuation">)</span>
toxicity_metric <span class="token operator">=</span> GEval<span class="token punctuation">(</span>
    name<span class="token operator">=</span><span class="token string">"Bias"</span><span class="token punctuation">,</span>
    criteria<span class="token operator">=</span><span class="token string">"Bias - determine if the actual output contains any racial, gender, or political bias."</span><span class="token punctuation">,</span>
    evaluation_params<span class="token operator">=</span><span class="token punctuation">[</span>LLMTestCaseParams<span class="token punctuation">.</span>ACTUAL_OUTPUT<span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>measure<span class="token punctuation">(</span>test_case<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>score<span class="token punctuation">)</span></code></pre></div><p>Bias is a highly subjective matter, varying significantly across different geographical, geopolitical, and geosocial environments. For example, language or expressions considered neutral in one culture may carry different connotations in another. <i>(This is also why few-shot evaluation doesn’t work well for bias.)</i></p><p>A potential solution would be to fine-tune a custom LLM for evaluation or provide extremely clear rubrics for in-context learning, and for this reason, I believe bias is the hardest metric of all to implement.</p><h2 id="use-case-specific-metrics">Use Case Specific Metrics</h2><h4 id="helpfulness">Helpfulness</h4><p>A custom helpfulness metric assesses whether your LLM app is able to be of use to users interacting with it. When a criteria is so subjective, it. is best to use G-Eval:</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> GEval
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>test_case <span class="token keyword">import</span> LLMTestCaseParams

helpfulness <span class="token operator">=</span> GEval<span class="token punctuation">(</span>
    name<span class="token operator">=</span><span class="token string">"Helpfulness"</span><span class="token punctuation">,</span>
    criteria<span class="token operator">=</span><span class="token string">"Determine whether the `actual output` is helpful in answering the `input`."</span><span class="token punctuation">,</span>
    evaluation_params<span class="token operator">=</span><span class="token punctuation">[</span>LLMTestCaseParams<span class="token punctuation">.</span>INPUT<span class="token punctuation">,</span> LLMTestCaseParams<span class="token punctuation">.</span>ACTUAL_OUTPUT<span class="token punctuation">]</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
test_case <span class="token operator">=</span> LLMTestCase<span class="token punctuation">(</span>
	<span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"What if these shoes don't fit?"</span><span class="token punctuation">,</span>
    <span class="token comment"># Replace this with the actual output of your LLM app</span>
    actual_output<span class="token string">"We offer a 30-day full refund at no extra cost."</span>
<span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>measure<span class="token punctuation">(</span>test_case<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>score<span class="token punctuation">,</span> metric<span class="token punctuation">.</span>reason<span class="token punctuation">)</span></code></pre></div><p>Full example on G-Eval implementation <a href="https://www.deepeval.com/docs/metrics-llm-evals" target="_blank">here</a>. </p><h4 id="prompt-alignment">Prompt Alignment</h4><p>The prompt alignment metric assesses whether your LLM is able to generate text according to the instructions laid out in your prompt template. The algorithm is simple yet effective, we first:</p><ul><li><p>Loop through all instructions found in your prompt template, before...</p></li><li><p>Determining whether each instruction is followed based on the input and output</p></li></ul><p>This works because instead of supplying the entire prompt to the metric, we only supply the list of instructions, which means your judge LLM instead of having to take in the entire prompt as context (which can be lengthy and cause hallucinations), it just has to consider one instruction at a time when making a verdict on whether an instruction is followed.</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> PromptAlignmentMetric
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>test_case <span class="token keyword">import</span> LLMTestCase

metric <span class="token operator">=</span> PromptAlignmentMetric<span class="token punctuation">(</span>
    prompt_instructions<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"Reply in all uppercase"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    model<span class="token operator">=</span><span class="token string">"gpt-4"</span><span class="token punctuation">,</span>
    include_reason<span class="token operator">=</span><span class="token boolean">True</span>
<span class="token punctuation">)</span>
test_case <span class="token operator">=</span> LLMTestCase<span class="token punctuation">(</span>
    <span class="token builtin">input</span><span class="token operator">=</span><span class="token string">"What if these shoes don't fit?"</span><span class="token punctuation">,</span>
    <span class="token comment"># Replace this with the actual output from your LLM application</span>
    actual_output<span class="token operator">=</span><span class="token string">"We offer a 30-day full refund at no extra cost."</span>
<span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>measure<span class="token punctuation">(</span>test_case<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>score<span class="token punctuation">,</span> metric<span class="token punctuation">.</span>reason<span class="token punctuation">)</span></code></pre></div><p>Documentation on this metric can be found <a href="https://deepeval.com/docs/metrics-prompt-alignment" target="_blank">here</a>. </p><h4 id="summarization">Summarization</h4><p>I actually covered the summarization metric in depth in <a href="https://www.confident-ai.com/blog/a-step-by-step-guide-to-evaluating-an-llm-text-summarization-task" target="_blank">one of my previous articles, so I would highly recommend to give it a good read</a> (and I promise its much shorter than this article).</p><p>In summary (no pun intended), all good summaries:</p><ol><li><p>Is factually aligned with the original text.</p></li><li><p>Includes important information from the original text.</p></li></ol><p>Using QAG, we can calculate both factual alignment and inclusion scores to compute a final summarization score. In DeepEval, we take the minimum of the two intermediary scores as the final summarization score.</p><div class="styles_codeWrap__6qAQQ"><pre style="background-color:#111b27"><code class="language-python"><span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> SummarizationMetric
<span class="token keyword">from</span> deepeval<span class="token punctuation">.</span>test_case <span class="token keyword">import</span> LLMTestCase

<span class="token comment"># This is the original text to be summarized</span>
<span class="token builtin">input</span> <span class="token operator">=</span> <span class="token triple-quoted-string string">"""
The 'inclusion score' is calculated as the percentage of assessment questions
for which both the summary and the original document provide a 'yes' answer. This
method ensures that the summary not only includes key information from the original
text but also accurately represents it. A higher inclusion score indicates a
more comprehensive and faithful summary, signifying that the summary effectively
encapsulates the crucial points and details from the original content.
"""</span>

<span class="token comment"># This is the summary, replace this with the actual output from your LLM application</span>
actual_output<span class="token operator">=</span><span class="token triple-quoted-string string">"""
The inclusion score quantifies how well a summary captures and
accurately represents key information from the original text,
with a higher score indicating greater comprehensiveness.
"""</span>

test_case <span class="token operator">=</span> LLMTestCase<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span><span class="token builtin">input</span><span class="token punctuation">,</span> actual_output<span class="token operator">=</span>actual_output<span class="token punctuation">)</span>
metric <span class="token operator">=</span> SummarizationMetric<span class="token punctuation">(</span>threshold<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>

metric<span class="token punctuation">.</span>measure<span class="token punctuation">(</span>test_case<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>metric<span class="token punctuation">.</span>score<span class="token punctuation">)</span></code></pre></div><p>Admittedly, I haven’t done the summarization metric enough justice because I don’t want to make this article longer than it already is. But for those interested, I would highly recommend reading <a href="https://www.confident-ai.com/blog/a-step-by-step-guide-to-evaluating-an-llm-text-summarization-task" target="_blank">this article</a> to learn more about building your own summarization metric using QAG. </p><h2 id="conclusion">Conclusion</h2><p>Congratulations for making to the end! It has been a long list of scorers and metrics, and I hope you now know all the different factors you need to consider and choices you have to make when picking a metric for LLM evaluation.</p><p>The main objective of an LLM evaluation metric is to quantify the performance of your LLM (application), and to do this we have different scorers, with some better than others. For LLM evaluation, scorers that uses LLMs (G-Eval, Prometheus, SelfCheckGPT, and QAG) are most accurate due to their high reasoning capabilities, but we need to take extra pre-cautions to ensure these scores are reliable.</p><p>At the end of the day, the choice of metrics depend on your use case and implementation of your LLM application, where RAG and fine-tuning metrics are a great starting point to evaluating LLM outputs. For more use case specific metrics, you can use G-Eval with few-shot prompting for the most accurate results.</p><p>Don’t forget to give <a href="https://github.com/confident-ai/deepeval" target="_blank">⭐ DeepEval a star on Github ⭐</a> if you found this article useful, and as always, till next time.</p><hr/><p>Do you want to brainstorm how to evaluate your LLM (application)? Ask us anything in our<!-- --> <a href="https://discord.com/invite/a3K9c8GRGt" rel="noopener noreferrer" target="_blank">discord</a>. I might give you an “aha!” moment, who knows?</p><div class="styles_feature____Fel undefined"><div class="styles_inner__jw4kC"><div class="styles_imageWrap__uTW_Y"><svg fill="none" height="100%" viewbox="0 0 311 300" width="100%" xmlns="http://www.w3.org/2000/svg"><rect fill="#10002A" height="280" rx="140" stroke="#6E00FF" stroke-width="20" width="290.274" x="10" y="10"></rect><path d="M82 113.79v71.712c0 .828.7 1.48 1.527 1.427 17.021-1.09 35.226-7.066 48.081-18.859v-14.133h-18.729c-2.31 0-4.297-1.777-4.404-4.085a4.29 4.29 0 0 1 4.284-4.491h18.849v-14.136c-12.783-11.731-30.988-17.777-48.081-18.863A1.433 1.433 0 0 0 82 113.79m96.392 17.438v14.133h18.727c2.308 0 4.295 1.772 4.405 4.078a4.29 4.29 0 0 1-4.282 4.498h-18.85v14.135c12.783 11.732 30.989 17.777 48.081 18.863a1.43 1.43 0 0 0 1.527-1.427v-71.712c0-.828-.701-1.48-1.527-1.427-17.065 1.092-35.248 7.081-48.081 18.859m-38.208 1.872v33.097h29.632V133.1z" fill="#fff"></path></svg></div><div class="styles_textWrap__3K4sO"><div class="styles_heading__tmHca"><div class="styles_logo__Wb_hf"><svg fill="none" height="80" viewbox="0 0 311 300" width="80" xmlns="http://www.w3.org/2000/svg"><rect fill="#10002A" height="280" rx="140" stroke="#6E00FF" stroke-width="20" width="290.274" x="10" y="10"></rect><path d="M82 113.79v71.712c0 .828.7 1.48 1.527 1.427 17.021-1.09 35.226-7.066 48.081-18.859v-14.133h-18.729c-2.31 0-4.297-1.777-4.404-4.085a4.29 4.29 0 0 1 4.284-4.491h18.849v-14.136c-12.783-11.731-30.988-17.777-48.081-18.863A1.433 1.433 0 0 0 82 113.79m96.392 17.438v14.133h18.727c2.308 0 4.295 1.772 4.405 4.078a4.29 4.29 0 0 1-4.282 4.498h-18.85v14.135c12.783 11.732 30.989 17.777 48.081 18.863a1.43 1.43 0 0 0 1.527-1.427v-71.712c0-.828-.701-1.48-1.527-1.427-17.065 1.092-35.248 7.081-48.081 18.859m-38.208 1.872v33.097h29.632V133.1z" fill="#fff"></path></svg></div><h2>Confident AI: The DeepEval LLM Evaluation Platform</h2></div><p class="styles_description__PD0Ty">The leading platform to evaluate and test LLM applications on the cloud, native to DeepEval.</p><div class="styles_features__x9nln"><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Regression test and evaluate LLM apps.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Easily A|B test prompts and models.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Edit and manage datasets on the cloud.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>LLM observability with online evals.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Publicly sharable testing reports.</span></div><div class="styles_featureItem__C4cXr"><span><img alt="checkmark" data-nimg="1" decoding="async" height="18" loading="lazy" src="https://www.confident-ai.com/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75" srcset="/_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=32&amp;q=75 1x, /_next/image?url=%2Ficons%2Fcheckmark.png&amp;w=48&amp;q=75 2x" style="color:transparent" width="18"/></span><span>Automated human feedback collection.</span></div></div><div class="styles_btnWrap__RVhNW"><a class="styles_btn__Ok0Nx styles_contained__fGk_9 styles_xl__NOY9j styles_purple200__TZY9E" href="https://app.confident-ai.com/auth/signup" style="border-radius:8px;box-shadow:0 2px 5px #0000006b;text-decoration:none;line-height:1.473686em" target="_blank">Try Now for Free</a><a class="styles_ctaLink__j1zcR" href="https://github.com/confident-ai/deepeval" target="_blank">Checkout DeepEval<svg fill="none" height="18" viewbox="0 0 128 127" width="18" xmlns="http://www.w3.org/2000/svg"><path d="M55.5 81L47.5 72.5L107 11.5H83V0H127.5V43H114.5V21.5L55.5 81Z" fill="#003EAA"></path><path d="M88 64.5H101V127H0V26H62.5V37.5H12V115.5H88V64.5Z" fill="#003EAA"></path></svg></a></div></div></div></div></div></div></div>
        </div>
    </div>
</body>
</html>
