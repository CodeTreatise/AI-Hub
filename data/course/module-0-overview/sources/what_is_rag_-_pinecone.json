{
  "name": "What is RAG - Pinecone",
  "url": "https://www.pinecone.io/learn/retrieval-augmented-generation/",
  "pages": [
    {
      "url": "https://www.pinecone.io/learn/retrieval-augmented-generation/",
      "title": "What is RAG - Pinecone",
      "content_html": "<article><section><div id=\"banner\"><div class=\"border-b\"><div class=\"container\"><div class=\"relative px-4 sm:px-8 flex flex-col gap-8 pb-4 pt-16 md:gap-16 md:pb-8 md:pt-24 xl:flex-row\"><div class=\"w-full xl:w-[13.5rem]\"><a class=\"text-text-secondary hover:text-text-primary border-b-border group inline-block border-b text-sm/[1] leading-none transition-colors duration-300\" href=\"https://www.pinecone.io/learn/\" target=\"_blank\"><span class=\"inline-block transition-transform duration-300 group-hover:-translate-x-1\">←</span> <span>Learn</span></a></div><div class=\"flex-1 space-y-8\"><div><h1 class=\"xxs:text-[38px] sm:text-h1/[1.1] text-text-primary text-[30px]\">Retrieval-Augmented Generation (RAG)</h1></div><div class=\"flex items-start gap-0 xl:items-center\"><div class=\"border-border h-6 w-6 shrink-0 overflow-hidden rounded-full border bg-black align-top\" style=\"position:relative;transform:translate(0px, 0px);z-index:1\"><img alt=\"Jenna Pederson\" class=\"h-full w-full object-cover\" data-nimg=\"1\" decoding=\"async\" height=\"24\" loading=\"lazy\" sizes=\"24px\" src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=3840&amp;q=75\" srcset=\"/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=16&amp;q=75 16w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=32&amp;q=75 32w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=48&amp;q=75 48w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=64&amp;q=75 64w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=96&amp;q=75 96w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=128&amp;q=75 128w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=256&amp;q=75 256w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=384&amp;q=75 384w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=3840&amp;q=75 3840w\" style=\"color:transparent\" width=\"24\"/></div><span class=\"text-text-secondary max-w-[21.25rem] self-center text-sm/[1.2] ml-2\"><span><a class=\"hover:text-text-primary transition-colors\" href=\"https://www.pinecone.io/author/jenna-pederson/\" target=\"_blank\">Jenna Pederson</a></span></span></div></div><div class=\"flex w-full flex-wrap justify-between gap-4 xl:w-[13.5rem] xl:flex-col\"><div class=\"space-y-4\"><span class=\"text-text-secondary block text-sm/[1.2]\">Jun 12, 2025</span><a class=\"text-text-secondary shrink-0 self-start border p-2 text-xs/[1.2] transition-colors hover:border-brand-blue hover:text-brand-blue\" href=\"https://www.pinecone.io/learn/category/core-components/\" target=\"_blank\">Core Components</a></div><div class=\"mt-auto\"><div class=\"text-text-secondary flex flex-col items-start gap-3 text-sm/[1.2]\">Share:<!-- --> <div class=\"flex items-start\"><a aria-label=\"Share to Twitter\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent\" href=\"https://twitter.com/intent/tweet?url=https://www.pinecone.io/learn/retrieval-augmented-generation\" target=\"_blank\"><svg fill=\"none\" height=\"12\" viewbox=\"0 0 14 12\" width=\"14\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M10.6367 0.5625H12.5508L8.33984 5.40234L13.3164 11.9375H9.43359L6.37109 7.97266L2.89844 11.9375H0.957031L5.46875 6.79688L0.710938 0.5625H4.70312L7.4375 4.19922L10.6367 0.5625ZM9.95312 10.7891H11.0195L4.12891 1.65625H2.98047L9.95312 10.7891Z\"></path></svg></a><a aria-label=\"Share to LinkedIn\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent\" href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://www.pinecone.io/learn/retrieval-augmented-generation\" target=\"_blank\"><svg fill=\"none\" height=\"13\" viewbox=\"0 0 13 13\" width=\"13\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M11.875 0.125C12.3398 0.125 12.75 0.535156 12.75 1.02734V11.5C12.75 11.9922 12.3398 12.375 11.875 12.375H1.34766C0.882812 12.375 0.5 11.9922 0.5 11.5V1.02734C0.5 0.535156 0.882812 0.125 1.34766 0.125H11.875ZM4.19141 10.625V4.80078H2.38672V10.625H4.19141ZM3.28906 3.98047C3.86328 3.98047 4.32812 3.51562 4.32812 2.94141C4.32812 2.36719 3.86328 1.875 3.28906 1.875C2.6875 1.875 2.22266 2.36719 2.22266 2.94141C2.22266 3.51562 2.6875 3.98047 3.28906 3.98047ZM11 10.625V7.42578C11 5.86719 10.6445 4.63672 8.8125 4.63672C7.9375 4.63672 7.33594 5.12891 7.08984 5.59375H7.0625V4.80078H5.33984V10.625H7.14453V7.75391C7.14453 6.98828 7.28125 6.25 8.23828 6.25C9.16797 6.25 9.16797 7.125 9.16797 7.78125V10.625H11Z\"></path></svg></a><a aria-label=\"Share to Hacker News\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group undefined\" href=\"https://news.ycombinator.com/submitlink?u=https://www.pinecone.io/learn/retrieval-augmented-generation\" target=\"_blank\"><svg fill=\"none\" height=\"13\" viewbox=\"0 0 13 13\" width=\"13\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M12.75 0.125V12.375H0.5V0.125H12.75ZM6.95312 7.125L9.05859 3.13281H8.15625L6.92578 5.62109C6.78906 5.89453 6.67969 6.14062 6.57031 6.35938L6.24219 5.62109L4.98438 3.13281H4.02734L6.13281 7.07031V9.66797H6.95312V7.125Z\"></path></svg></a></div></div></div></div><div aria-hidden=\"true\" class=\"pointer-events-none absolute inset-0\" role=\"presentation\"><span class=\"from-background absolute bottom-0 left-0 top-0 h-full w-[1px] border-l\"></span><span class=\"from-background absolute bottom-0 right-0 top-0 h-full w-[1px] border-r\"></span></div></div></div></div></div></section><div class=\"container\"><div class=\"relative px-4 sm:px-8 px-0 pb-10\"><div class=\"flex flex-col justify-between lg:flex-row\"><div class=\"mt-4 w-full flex-col gap-10 overflow-hidden md:max-w-full\"><div class=\"prose max-w-none\"><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Not only are foundation models stuck in the past, but they intentionally produce natural-sounding and varied responses. Both of these can lead to confidently inaccurate and irrelevant output. This behavior is known as “hallucination.”</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">In this article, we’ll explore the limitations of foundation models and how retrieval-augmented generation (RAG) can address these limitations so chat, search, and agentic workflows can all benefit.</p><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"Limitations-of-foundation-models\">Limitations of foundation models</h2><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Products built on top of foundation models alone are brilliant yet flawed as foundation models have multiple limitations:</p><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Knowledge-cutoffs\">Knowledge cutoffs</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">When you ask current models about recent events – like asking about last week’s NBA basketball game or how to use features in the latest iPhone model - they may confidently provide outdated or completely fabricated information, the hallucinations we mentioned earlier.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Models are trained on massive datasets containing years of human knowledge and creative output from code repositories, books, websites, conversations, scientific papers, and more. But after a model is trained, this data is frozen at a specific point in time, the “cutoff”. This cutoff creates a <em>knowledge gap</em>, leading them to generate plausible but incorrect responses when asked about recent developments.</p><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Lack-depth-in-domain-specific-knowledge\">Lack depth in domain-specific knowledge</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Foundation models have broad knowledge, but can lack depth in specialized domains. High quality datasets might not exist publicly for a domain, not necessarily because they are private, but because they are highly specialized. Consider a medical model that knows about anatomy, disease, and surgical techniques, but struggles with rare genetic conditions and cutting edge therapies. This data might exist publicly to be used during training, but it may not appear enough to train the model correctly. It also requires expert-level knowledge during the training process to contextualize the information.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">This limitation can result in responses that are incomplete or irrelevant.</p><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Lack-private-or-proprietary-data\">Lack private or proprietary data</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">In the case of general-purpose, public models, the data (<em>your</em> data) does not exist publicly and is inaccessible during training. This means that models don’t know the specifics of your business, whether that be internal company processes and policies, personnel data or email communications, or even the trade secrets of your company. And for good reason: if this data had been included in the training, anyone using the model would potentially gain access to your company’s private and proprietary data.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Again, this limitation can result in incomplete or irrelevant responses, limiting the usefulness of the model for your custom business purpose.</p><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Loses-trust\">Loses trust</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Models typically cannot cite their sources related to a specific response. Without citations or references, the user either has to trust the response or validate the claim themselves. Given that models are trained on vast amounts of public data, there is a chance that the generated response is the result of an unauthoritative source.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">When inaccurate, irrelevant, and useless information is generated, users will lose trust in the model itself, even when this behavior is inherent in how foundation models work.</p><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Output-generation-is-probabilistic\">Output generation is probabilistic</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Hallucinations are often a symptom of the limitations just described. However, models are trained on a diverse set of data that can contain contradictions, errors, and ambiguous data (in addition to the correct data). Because of this, models assign probabilities to all possible continuations, even the wrong ones. Because of sampling randomness like temperature and top k combined with how a user constructs a prompt (maybe it’s too vague or contains misleading context), models may choose the wrong continuation. The result is output that can contain hallucinations.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Additionally, models don’t always distinguish between what they know vs what they don’t know, sounding confident even when incomplete, inaccurate, or irrelevant. Hallucinations can produce unwanted behaviors and even be dangerous. For example, an inaccurate but highly convincing medical report could lead to life-threatening treatments or no treatment at all.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">These foundation model limitations can impact your business bottom line and erode the trust of your users. Retrieval-augmented generation can address these limitations.</p><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"What-is-Retrieval-Augmented-Generation\">What is Retrieval-Augmented Generation?</h2><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Retrieval-augmented generation, or RAG, is a technique that uses authoritative, external data to improve the accuracy, relevancy, and usefulness of a model’s output. It does this through the following four core components, which we’ll cover in more detail later in this article:</p><ol class=\"!marker:text-text-primary ml-8! list-decimal! [&amp;_ul]:list-circle! mt-4 [&amp;_ol]:mt-0 [&amp;_ul]:mt-0\"><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Ingestion: authoritative data like company proprietary data is loaded into a data source, like a Pinecone vector database</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Retrieval: relevant data is retrieved from an external data source based on a user query</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Augmentation: the retrieved data and the user query are combined into a prompt to provide the model with context for the generation step</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Generation: the model generates output from the augmented prompt, using the context to drive a more accurate and relevant response.</span></li></ol><div class=\"flex flex-col items-center align-middle md:px-0 mt-7 pb-3\"><div class=\"w-full max-w-full\"><div class=\"flex flex-col items-center justify-center\"><div class=\"flex hover:cursor-zoom-in max-w-full items-center justify-center\" style=\"height:auto;cursor:zoom-in\"><img alt=\"Shows traditional RAG from user query and chunking/embedding to vector search and rerank to generated LLM output.\" class=\"max-w-full h-auto hover:cursor-zoom-in hover:opacity-80 transition-opacity duration-300\" data-nimg=\"1\" decoding=\"async\" height=\"2620\" loading=\"lazy\" src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Ff6fe392bb5287791a2c6052f1eeb3072ad0b7e36-2236x2620.png&amp;w=3840&amp;q=75\" srcset=\"/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Ff6fe392bb5287791a2c6052f1eeb3072ad0b7e36-2236x2620.png&amp;w=3840&amp;q=75 1x\" style=\"color:transparent\" width=\"2236\"/></div></div></div><div class=\"w-full text-center text-sm text-text-secondary mt-4\">Traditional RAG</div></div><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">By combining relevant data from an external data source with the user’s query and providing it to the model as context for the generation step, the model will use it to generate a more accurate and relevant output.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">RAG provides the following benefits:</p><ol class=\"!marker:text-text-primary ml-8! list-decimal! [&amp;_ul]:list-circle! mt-4 [&amp;_ol]:mt-0 [&amp;_ul]:mt-0\"><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Access to real-time data and proprietary or domain-specific data: bring in knowledge relevant to your situation - current events, news, social media, customer data, proprietary data</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Builds trust: more relevant and accurate results are more likely to earn trust and source citations allow human review</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">More control: control over which sources are used, real-time data access, authorization to data, guardrails/safety/compliance, traceability/source citations, retrieval strategies, cost, tune each component independently of the others</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Cost-effective compared to alternatives like training/re-training your own model, fine-tuning, or stuffing the context window: foundation models are costly to produce and require specialized knowledge to create, as is fine-tuning; the larger the context sent to the model, the higher the cost</span></li></ol><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"RAG-in-support-of-agentic-workflows\">RAG in support of agentic workflows</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">But this traditional RAG approach is simple, often with a vector database and a one-shot prompt with context sent to the model to generate output. With the rise of AI agents, agents are now orchestrators of the core RAG components to:</p><ul class=\"!marker:text-text-primary ml-8! list-disc! [&amp;_ul]:list-circle! mt-4 [&amp;_ol]:mt-0 [&amp;_ul]:mt-0\"><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-circle:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">construct more effective queries</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-circle:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">access additional retrieval tools</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-circle:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">evaluate the accuracy and relevance of the retrieved context</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-circle:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">apply reasoning to validate retrieved information, to trust or discard it.</span></li></ul><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">These operations can be performed by an agent or agents as part of a larger, iterative plan. Agents as orchestrators of RAG bring even more opportunities for review, revision of queries, reasoning or validation of context, allowing them to make better decisions, take more informed actions, and generate more accurate and relevant output.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Now that we’ve covered what RAG is, let’s take a deeper dive into how it works.</p><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"How-does-Retrieval-Augmented-Generation-work\">How does Retrieval-Augmented Generation work?</h2><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">RAG brings accuracy and relevancy to LLM output by relying on authoritative data sources like proprietary, domain-specific data and real-time information. But before we dig into how it does that, let’s ask the questions: do you even need RAG and how will you know it’s working?</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">This is where ground truth evaluations come in. In order to properly deploy any application, you need to know when it's working. AI applications are no different, and so identifying a set of queries and their expected answers is critical to knowing if your application is working. Maintaining that evaluation set is also critical to knowing where to improve over time, and whether those improvements are working. RAG itself is just one optimization, but there are others like query rewriting, chunk expansion, knowledge graphs and more.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">With a good baseline, you can move on to implementing the four main components of RAG:</p><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Ingestion\">Ingestion</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">In simple traditional RAG, you’ll retrieve data from a vector database like Pinecone, using semantic search to find the true meaning of the user’s query and retrieve relevant information instead of simply matching keywords in the query. We’ll use Pinecone as an example here, but the concept applies to all vector databases.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">But before we can retrieve the data, you have to ingest the data. Here are steps to get data into your database:</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"><em></em></p><h5 class=\"text-h5 text-text-primary\" id=\"Chunk-the-data\" style=\"margin-block:36px\"><em>Chunk the data</em></h5><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">During the ingestion step, you’ll load your authoritative data as vectors into Pinecone. You may have structured or unstructured data in the form of text, PDFs, emails, internal wikis, or databases. After cleaning the data, you may need to chunk it by dividing each piece of data, or document, into smaller chunks. Depending on the kind of data you have, the types of queries your users have, and how the results will be used in your application, you’ll need to choose a <a class=\"cursor-pointer underline-offset-4 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://www.pinecone.io/learn/chunking-strategies/\" target=\"_blank\">chunking strategy</a>.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><h5 class=\"text-h5 text-text-primary\" id=\"Create-vector-embeddings\" style=\"margin-block:36px\"><em>Create vector embeddings</em></h5><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Then, using an embedding model, you’ll embed each chunk and load it into the vector database. The embedding model is a special type of LLM that converts the data chunk into a vector embedding, a numerical representation of the data’s meaning. This allows computers to search for similar items based on the vector representation of the stored data.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><h5 class=\"text-h5 text-text-primary\" id=\"Load-data-into-a-vector-database\" style=\"margin-block:36px\"><em>Load data into a vector database</em></h5><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Once you have vectors, you’ll load them into a vector database. This ingestion step most likely happens offline, independently of your application and your user’s workflow. However, if your data changes, for instance, product inventory is updated, you can update the index in real-time to provide up-to-date information to your users.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Now that your vector database contains the vector embeddings of your source data, the next step is retrieval.</p><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Retrieval\">Retrieval</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">A simple approach to retrieval would use semantic search alone. But by using hybrid search, combining both semantic search (with dense vectors) and lexical search (with <a class=\"cursor-pointer underline-offset-4 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://www.pinecone.io/learn/sparse-retrieval/\" target=\"_blank\">sparse vectors</a>), you can improve the retrieval results even more. This becomes relevant when your users don’t always use the same language to talk about a topic (semantic search) and they refer to internal, domain-specific language (lexical or keyword search) like acronyms, product names, or team names.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">During retrieval, we’ll create a vector embedding from the user’s query to use for searching against the vectors in the database. In <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://docs.pinecone.io/guides/search/hybrid-search\" target=\"_blank\">hybrid search</a>, you’ll query either a single hybrid index or both a dense and a sparse index. Then we combine and de-duplicate the results and use a reranking model to <a class=\"cursor-pointer underline-offset-4 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://www.pinecone.io/learn/refine-with-rerank/\" target=\"_blank\">rerank</a> them based on a unified relevance score, and return the most relevant matches.</p><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Augmentation\">Augmentation</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Now that you have the most relevant matches from the retrieval step, you’ll create an augmented prompt with both the search results and the user’s query to send to the LLM. This is where the magic happens.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">An augmented prompt might look like this:</p><!--$--><div class=\"my-50\"><div class=\"codeblock-light bg-text-secondary relative grid w-full rounded-lg text-[0.75rem] leading-[1.3125rem]\"><pre class=\"relative language-text line-numbers overflow-auto\"><code class=\"language-text line-numbers\">QUESTION:\n&lt;the user's question&gt;\n\nCONTEXT:\n&lt;the search results to use as context&gt;\n\nUsing the CONTEXT provided, answer the QUESTION. Keep your answer grounded in the facts of the CONTEXT. If the CONTEXT doesn't contain the answer to the QUESTION, say you don't know.</code></pre><div class=\"z-1 absolute right-3 top-3\"><button aria-label=\"Copy code to clipboard\" class=\"group rounded-md border p-1.5 transition-opacity hover:opacity-100 border-text-secondary/40\"><svg class=\"w-4\" viewbox=\"4.25 2.25 15.5 19.5\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary opacity-40\" d=\"M19.53 8 14 2.47a.75.75 0 0 0-.53-.22H11A2.75 2.75 0 0 0 8.25 5v1.25H7A2.75 2.75 0 0 0 4.25 9v10A2.75 2.75 0 0 0 7 21.75h7A2.75 2.75 0 0 0 16.75 19v-1.25H17A2.75 2.75 0 0 0 19.75 15V8.5a.75.75 0 0 0-.22-.5Zm-5.28-3.19 2.94 2.94h-2.94V4.81Zm1 14.19A1.25 1.25 0 0 1 14 20.25H7A1.25 1.25 0 0 1 5.75 19V9A1.25 1.25 0 0 1 7 7.75h1.25V15A2.75 2.75 0 0 0 11 17.75h4.25V19ZM17 16.25h-6A1.25 1.25 0 0 1 9.75 15V5A1.25 1.25 0 0 1 11 3.75h1.75V8.5a.76.76 0 0 0 .75.75h4.75V15A1.25 1.25 0 0 1 17 16.25Z\"></path></svg></button></div></div></div><!--/$--><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">By sending both the search results and the user’s question as context to the LLM, you are encouraging it to use the more accurate and relevant info from the search results during the next generation step.</p><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Generation\">Generation</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Using the augmented prompt, the LLM now has access to the most pertinent and grounding facts from your vector database so your application can provide an accurate answer for your user, reducing the likelihood of hallucination.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">But RAG is no longer simply about searching for the right piece of information to inform a model response. With agentic RAG, it's about deciding which questions to ask, which tools to use, when to use them, and then aggregating results to ground answers.</p><div class=\"flex flex-col items-center align-middle md:px-0 mt-7 pb-3\"><div class=\"w-full max-w-full\"><div class=\"flex flex-col items-center justify-center\"><div class=\"flex hover:cursor-zoom-in max-w-full items-center justify-center\" style=\"height:auto;cursor:zoom-in\"><img alt=\"Shows agentic RAG from user query to agent to tool selection with search via Pinecone vector database to generated LLM output.\" class=\"max-w-full h-auto hover:cursor-zoom-in hover:opacity-80 transition-opacity duration-300\" data-nimg=\"1\" decoding=\"async\" height=\"1484\" loading=\"lazy\" src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fa40d852ad69f85abb76405032ff8c939da7be987-1420x1484.png&amp;w=3840&amp;q=75\" srcset=\"/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fa40d852ad69f85abb76405032ff8c939da7be987-1420x1484.png&amp;w=1920&amp;q=75 1x, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fa40d852ad69f85abb76405032ff8c939da7be987-1420x1484.png&amp;w=3840&amp;q=75 2x\" style=\"color:transparent\" width=\"1420\"/></div></div></div><div class=\"w-full text-center text-sm text-text-secondary mt-4\">Agentic RAG</div></div><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">In this simple version, the LLM is the agent and decides which retrieval tools to use and when, and how to query those tools.</p><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"Wrapping-up\">Wrapping up</h2><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Retrieval-augmented generation has evolved from a buzzword to an indispensable foundation for AI applications. It blends the broad capabilities of foundation models with your company’s authoritative and proprietary knowledge. With AI agents handling more complex use cases, from those <a class=\"cursor-pointer underline-offset-4 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://www.pinecone.io/customers/aquant/\" target=\"_blank\">supporting professionals servicing complex manufacturing equipment</a> to delivering <a class=\"cursor-pointer underline-offset-4 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://www.pinecone.io/customers/customgpt-ai/\" target=\"_blank\">domain-specific agents at scale</a>, RAG is not just relevant in 2025. It’s critical for building accurate, relevant, and responsible AI applications that go beyond information retrieval. As AI agents become more autonomous and handle more complex workflows, they’ll need to ground their reasoning in your private and domain-specific data through RAG. The question is no longer <a class=\"cursor-pointer underline-offset-4 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://www.pinecone.io/learn/rag-2025/\" target=\"_blank\">whether to implement RAG</a>, but how to architect it most effectively for your unique use case and data requirements.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Want to dig into a RAG code example? Create a <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://app.pinecone.io/\" target=\"_blank\">free Pinecone account</a> and check out our <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://docs.pinecone.io/examples/notebooks\" target=\"_blank\">example notebooks</a> to implement retrieval-augmented generation with Pinecone or <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://docs.pinecone.io/guides/assistant/quickstart\" target=\"_blank\">get started with Pinecone Assistant</a>, to build production-grade chat and agent-based applications quickly.</p></div><div class=\"mt-10 block md:hidden\"><div class=\"text-text-secondary flex flex-col items-start gap-3 text-sm/[1.2]\">Share:<!-- --> <div class=\"flex items-start\"><a aria-label=\"Share to Twitter\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent\" href=\"https://twitter.com/intent/tweet?url=https://www.pinecone.io/learn/retrieval-augmented-generation\" target=\"_blank\"><svg fill=\"none\" height=\"12\" viewbox=\"0 0 14 12\" width=\"14\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M10.6367 0.5625H12.5508L8.33984 5.40234L13.3164 11.9375H9.43359L6.37109 7.97266L2.89844 11.9375H0.957031L5.46875 6.79688L0.710938 0.5625H4.70312L7.4375 4.19922L10.6367 0.5625ZM9.95312 10.7891H11.0195L4.12891 1.65625H2.98047L9.95312 10.7891Z\"></path></svg></a><a aria-label=\"Share to LinkedIn\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent\" href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://www.pinecone.io/learn/retrieval-augmented-generation\" target=\"_blank\"><svg fill=\"none\" height=\"13\" viewbox=\"0 0 13 13\" width=\"13\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M11.875 0.125C12.3398 0.125 12.75 0.535156 12.75 1.02734V11.5C12.75 11.9922 12.3398 12.375 11.875 12.375H1.34766C0.882812 12.375 0.5 11.9922 0.5 11.5V1.02734C0.5 0.535156 0.882812 0.125 1.34766 0.125H11.875ZM4.19141 10.625V4.80078H2.38672V10.625H4.19141ZM3.28906 3.98047C3.86328 3.98047 4.32812 3.51562 4.32812 2.94141C4.32812 2.36719 3.86328 1.875 3.28906 1.875C2.6875 1.875 2.22266 2.36719 2.22266 2.94141C2.22266 3.51562 2.6875 3.98047 3.28906 3.98047ZM11 10.625V7.42578C11 5.86719 10.6445 4.63672 8.8125 4.63672C7.9375 4.63672 7.33594 5.12891 7.08984 5.59375H7.0625V4.80078H5.33984V10.625H7.14453V7.75391C7.14453 6.98828 7.28125 6.25 8.23828 6.25C9.16797 6.25 9.16797 7.125 9.16797 7.78125V10.625H11Z\"></path></svg></a><a aria-label=\"Share to Hacker News\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group undefined\" href=\"https://news.ycombinator.com/submitlink?u=https://www.pinecone.io/learn/retrieval-augmented-generation\" target=\"_blank\"><svg fill=\"none\" height=\"13\" viewbox=\"0 0 13 13\" width=\"13\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M12.75 0.125V12.375H0.5V0.125H12.75ZM6.95312 7.125L9.05859 3.13281H8.15625L6.92578 5.62109C6.78906 5.89453 6.67969 6.14062 6.57031 6.35938L6.24219 5.62109L4.98438 3.13281H4.02734L6.13281 7.07031V9.66797H6.95312V7.125Z\"></path></svg></a></div></div></div><div class=\"mt-10\"><div class=\"inline-block border px-4 py-3\"><p class=\"text-text-secondary text-sm\">Was this article helpful?</p><div class=\"mt-2 flex gap-4\"><button class=\"text-text-secondary hover:text-text-primary flex cursor-pointer items-center gap-2 text-sm transition-colors\"><svg class=\"lucide lucide-thumbs-up h-4 w-4\" fill=\"none\" height=\"24\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewbox=\"0 0 24 24\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M7 10v12\"></path><path d=\"M15 5.88 14 10h5.83a2 2 0 0 1 1.92 2.56l-2.33 8A2 2 0 0 1 17.5 22H4a2 2 0 0 1-2-2v-8a2 2 0 0 1 2-2h2.76a2 2 0 0 0 1.79-1.11L12 2h0a3.13 3.13 0 0 1 3 3.88Z\"></path></svg>Yes</button><button class=\"text-text-secondary hover:text-text-primary flex cursor-pointer items-center gap-2 text-sm transition-colors\"><svg class=\"lucide lucide-thumbs-down h-4 w-4\" fill=\"none\" height=\"24\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewbox=\"0 0 24 24\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M17 14V2\"></path><path d=\"M9 18.12 10 14H4.17a2 2 0 0 1-1.92-2.56l2.33-8A2 2 0 0 1 6.5 2H20a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2h-2.76a2 2 0 0 0-1.79 1.11L12 22h0a3.13 3.13 0 0 1-3-3.88Z\"></path></svg>No</button></div></div></div></div></div><div aria-hidden=\"true\" class=\"pointer-events-none absolute left-0 top-0 w-full\" role=\"presentation\"><span class=\"z-1 absolute border-l top-0 left-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent left-0 top-1 h-[10rem]\"></span><span class=\"z-1 absolute border-l top-0 right-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent right-0 top-1 h-[10rem]\"></span></div><div aria-hidden=\"true\" class=\"pointer-events-none absolute bottom-0 left-0 w-full\" role=\"presentation\"><span class=\"z-1 absolute border-l bottom-0 left-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-b from-background z-2 absolute w-[1px] from-10% to-transparent left-0 bottom-1 h-[10rem]\"></span><span class=\"z-1 absolute border-l bottom-0 right-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-b from-background z-2 absolute w-[1px] from-10% to-transparent right-0 bottom-1 h-[10rem]\"></span></div></div></div><div class=\"container\"><div class=\"relative px-4 sm:px-8 border-border border-t !px-0\"><div class=\"w-full pt-24 pb-20 border-border flex-col justify-start items-start gap-8 inline-flex\"><div class=\"self-stretch min-h-[54px] px-8 flex-col justify-start items-start gap-2 flex\"><div class=\"self-stretch text-brand-blue text-xs font-medium font-['GT Planar VCTR'] uppercase leading-[18px] tracking-widest\">Recommended for you</div><div class=\"self-stretch text-text-primary text-[32px] font-bold font-['GT Planar VCTR'] leading-[38.40px]\">Further Reading</div></div><div class=\"w-full grid grid-cols-1 md:grid-cols-3 divide-y md:divide-y-0 md:divide-x divide-border border border-border\"><a class=\"group gap-4 p-8 bg-background hover:bg-background-light flex flex-col justify-between transition-colors duration-300\" href=\"https://www.pinecone.io/learn/rag-2025/\" target=\"_blank\"><div class=\"flex flex-col gap-4\"><div class=\"flex items-start justify-between\"><div class=\"text-text-secondary text-sm leading-[21px]\">Learn</div><div class=\"text-text-secondary text-sm leading-[21px]\">Jun 25, 2025</div></div><div class=\"text-text-primary text-xl font-medium leading-relaxed min-h-[150px]\">Beyond the hype: Why RAG remains essential for modern AI</div></div><div class=\"flex items-center gap-2\"><div class=\"flex\"><div class=\"relative h-6 w-6\" style=\"margin-left:0\"><img alt=\"Jenna Pederson\" class=\"border-border rounded-3xl border object-cover\" data-nimg=\"fill\" decoding=\"async\" loading=\"lazy\" sizes=\"24px\" src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=3840&amp;q=75\" srcset=\"/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=16&amp;q=75 16w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=32&amp;q=75 32w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=48&amp;q=75 48w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=64&amp;q=75 64w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=96&amp;q=75 96w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=128&amp;q=75 128w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=256&amp;q=75 256w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=384&amp;q=75 384w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16c1f35f0b2506329238790b2099f66c71cf6507-1995x1997.jpg&amp;w=3840&amp;q=75 3840w\" style=\"position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent\"/></div></div><div class=\"text-text-secondary text-sm leading-[16.8px]\"><span>Jenna Pederson</span></div></div></a><a class=\"group gap-4 p-8 bg-background hover:bg-background-light flex flex-col justify-between transition-colors duration-300\" href=\"https://www.pinecone.io/learn/building-reliable-curated-accurate-rag/\" target=\"_blank\"><div class=\"flex flex-col gap-4\"><div class=\"flex items-start justify-between\"><div class=\"text-text-secondary text-sm leading-[21px]\">Learn</div><div class=\"text-text-secondary text-sm leading-[21px]\">Oct 25, 2024</div></div><div class=\"text-text-primary text-xl font-medium leading-relaxed min-h-[150px]\">Building a reliable, curated, and accurate RAG system with Cleanlab and Pinecone</div></div><div class=\"flex items-center gap-2\"><div class=\"flex\"><div class=\"relative h-6 w-6\" style=\"margin-left:0\"><img alt=\"Matt Turk\" class=\"border-border rounded-3xl border object-cover\" data-nimg=\"fill\" decoding=\"async\" loading=\"lazy\" sizes=\"24px\" src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=3840&amp;q=75\" srcset=\"/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=16&amp;q=75 16w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=32&amp;q=75 32w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=48&amp;q=75 48w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=64&amp;q=75 64w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=96&amp;q=75 96w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=128&amp;q=75 128w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=256&amp;q=75 256w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=384&amp;q=75 384w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fcec2cd36ff024a950bcbf54ab221874ec59ac3fe-512x512.png&amp;w=3840&amp;q=75 3840w\" style=\"position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent\"/></div></div><div class=\"text-text-secondary text-sm leading-[16.8px]\"><span>Matt Turk</span></div></div></a><a class=\"group gap-4 p-8 bg-background hover:bg-background-light flex flex-col justify-between transition-colors duration-300\" href=\"https://www.pinecone.io/blog/rag-study/\" target=\"_blank\"><div class=\"flex flex-col gap-4\"><div class=\"flex items-start justify-between\"><div class=\"text-text-secondary text-sm leading-[21px]\">Engineering</div><div class=\"text-text-secondary text-sm leading-[21px]\">Jan 16, 2024</div></div><div class=\"text-text-primary text-xl font-medium leading-relaxed min-h-[150px]\">RAG makes LLMs better and equal</div></div><div class=\"flex items-center gap-2\"><div class=\"flex\"><div class=\"relative h-6 w-6\" style=\"margin-left:0\"><img alt=\"Amnon Catav\" class=\"border-border rounded-3xl border object-cover\" data-nimg=\"fill\" decoding=\"async\" loading=\"lazy\" sizes=\"24px\" src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=3840&amp;q=75\" srcset=\"/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=16&amp;q=75 16w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=32&amp;q=75 32w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=48&amp;q=75 48w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=64&amp;q=75 64w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=96&amp;q=75 96w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=128&amp;q=75 128w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=256&amp;q=75 256w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=384&amp;q=75 384w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F540a006e4ee5ca13c8a9a2c6d31c876152774dfd-600x600.jpg&amp;w=3840&amp;q=75 3840w\" style=\"position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent\"/></div><div class=\"relative h-6 w-6\" style=\"margin-left:-4px\"><img alt=\"Roy Miara\" class=\"border-border rounded-3xl border object-cover\" data-nimg=\"fill\" decoding=\"async\" loading=\"lazy\" sizes=\"24px\" src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=3840&amp;q=75\" srcset=\"/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=16&amp;q=75 16w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=32&amp;q=75 32w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=48&amp;q=75 48w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=64&amp;q=75 64w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=96&amp;q=75 96w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=128&amp;q=75 128w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=256&amp;q=75 256w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=384&amp;q=75 384w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F714fe2c46c9c9d2cd2b7758c24243a20909d4733-800x800.png&amp;w=3840&amp;q=75 3840w\" style=\"position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent\"/></div></div><div class=\"text-text-secondary text-sm leading-[16.8px]\"><span>Amnon<!-- -->, </span><span>Roy<!-- -->, </span><span>Ilai<!-- -->, </span><span>Nathan<!-- -->, </span><span>Amir</span></div></div></a></div></div><div aria-hidden=\"true\" class=\"pointer-events-none absolute left-0 top-0 w-full\" role=\"presentation\"><span class=\"z-1 absolute border-l top-0 left-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent left-0 top-1 h-[10rem]\"></span><span class=\"z-1 absolute border-l top-0 right-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent right-0 top-1 h-[10rem]\"></span></div></div></div></article>",
      "text": "←\nLearn\nRetrieval-Augmented Generation (RAG)\nJenna Pederson\nJun 12, 2025\nCore Components\nShare:\nNot only are foundation models stuck in the past, but they intentionally produce natural-sounding and varied responses. Both of these can lead to confidently inaccurate and irrelevant output. This behavior is known as “hallucination.”\nIn this article, we’ll explore the limitations of foundation models and how retrieval-augmented generation (RAG) can address these limitations so chat, search, and agentic workflows can all benefit.\nLimitations of foundation models\nProducts built on top of foundation models alone are brilliant yet flawed as foundation models have multiple limitations:\nKnowledge cutoffs\nWhen you ask current models about recent events – like asking about last week’s NBA basketball game or how to use features in the latest iPhone model - they may confidently provide outdated or completely fabricated information, the hallucinations we mentioned earlier.\nModels are trained on massive datasets containing years of human knowledge and creative output from code repositories, books, websites, conversations, scientific papers, and more. But after a model is trained, this data is frozen at a specific point in time, the “cutoff”. This cutoff creates a\nknowledge gap\n, leading them to generate plausible but incorrect responses when asked about recent developments.\nLack depth in domain-specific knowledge\nFoundation models have broad knowledge, but can lack depth in specialized domains. High quality datasets might not exist publicly for a domain, not necessarily because they are private, but because they are highly specialized. Consider a medical model that knows about anatomy, disease, and surgical techniques, but struggles with rare genetic conditions and cutting edge therapies. This data might exist publicly to be used during training, but it may not appear enough to train the model correctly. It also requires expert-level knowledge during the training process to contextualize the information.\nThis limitation can result in responses that are incomplete or irrelevant.\nLack private or proprietary data\nIn the case of general-purpose, public models, the data (\nyour\ndata) does not exist publicly and is inaccessible during training. This means that models don’t know the specifics of your business, whether that be internal company processes and policies, personnel data or email communications, or even the trade secrets of your company. And for good reason: if this data had been included in the training, anyone using the model would potentially gain access to your company’s private and proprietary data.\nAgain, this limitation can result in incomplete or irrelevant responses, limiting the usefulness of the model for your custom business purpose.\nLoses trust\nModels typically cannot cite their sources related to a specific response. Without citations or references, the user either has to trust the response or validate the claim themselves. Given that models are trained on vast amounts of public data, there is a chance that the generated response is the result of an unauthoritative source.\nWhen inaccurate, irrelevant, and useless information is generated, users will lose trust in the model itself, even when this behavior is inherent in how foundation models work.\nOutput generation is probabilistic\nHallucinations are often a symptom of the limitations just described. However, models are trained on a diverse set of data that can contain contradictions, errors, and ambiguous data (in addition to the correct data). Because of this, models assign probabilities to all possible continuations, even the wrong ones. Because of sampling randomness like temperature and top k combined with how a user constructs a prompt (maybe it’s too vague or contains misleading context), models may choose the wrong continuation. The result is output that can contain hallucinations.\nAdditionally, models don’t always distinguish between what they know vs what they don’t know, sounding confident even when incomplete, inaccurate, or irrelevant. Hallucinations can produce unwanted behaviors and even be dangerous. For example, an inaccurate but highly convincing medical report could lead to life-threatening treatments or no treatment at all.\nThese foundation model limitations can impact your business bottom line and erode the trust of your users. Retrieval-augmented generation can address these limitations.\nWhat is Retrieval-Augmented Generation?\nRetrieval-augmented generation, or RAG, is a technique that uses authoritative, external data to improve the accuracy, relevancy, and usefulness of a model’s output. It does this through the following four core components, which we’ll cover in more detail later in this article:\nIngestion: authoritative data like company proprietary data is loaded into a data source, like a Pinecone vector database\nRetrieval: relevant data is retrieved from an external data source based on a user query\nAugmentation: the retrieved data and the user query are combined into a prompt to provide the model with context for the generation step\nGeneration: the model generates output from the augmented prompt, using the context to drive a more accurate and relevant response.\nTraditional RAG\nBy combining relevant data from an external data source with the user’s query and providing it to the model as context for the generation step, the model will use it to generate a more accurate and relevant output.\nRAG provides the following benefits:\nAccess to real-time data and proprietary or domain-specific data: bring in knowledge relevant to your situation - current events, news, social media, customer data, proprietary data\nBuilds trust: more relevant and accurate results are more likely to earn trust and source citations allow human review\nMore control: control over which sources are used, real-time data access, authorization to data, guardrails/safety/compliance, traceability/source citations, retrieval strategies, cost, tune each component independently of the others\nCost-effective compared to alternatives like training/re-training your own model, fine-tuning, or stuffing the context window: foundation models are costly to produce and require specialized knowledge to create, as is fine-tuning; the larger the context sent to the model, the higher the cost\nRAG in support of agentic workflows\nBut this traditional RAG approach is simple, often with a vector database and a one-shot prompt with context sent to the model to generate output. With the rise of AI agents, agents are now orchestrators of the core RAG components to:\nconstruct more effective queries\naccess additional retrieval tools\nevaluate the accuracy and relevance of the retrieved context\napply reasoning to validate retrieved information, to trust or discard it.\nThese operations can be performed by an agent or agents as part of a larger, iterative plan. Agents as orchestrators of RAG bring even more opportunities for review, revision of queries, reasoning or validation of context, allowing them to make better decisions, take more informed actions, and generate more accurate and relevant output.\nNow that we’ve covered what RAG is, let’s take a deeper dive into how it works.\nHow does Retrieval-Augmented Generation work?\nRAG brings accuracy and relevancy to LLM output by relying on authoritative data sources like proprietary, domain-specific data and real-time information. But before we dig into how it does that, let’s ask the questions: do you even need RAG and how will you know it’s working?\nThis is where ground truth evaluations come in. In order to properly deploy any application, you need to know when it's working. AI applications are no different, and so identifying a set of queries and their expected answers is critical to knowing if your application is working. Maintaining that evaluation set is also critical to knowing where to improve over time, and whether those improvements are working. RAG itself is just one optimization, but there are others like query rewriting, chunk expansion, knowledge graphs and more.\nWith a good baseline, you can move on to implementing the four main components of RAG:\nIngestion\nIn simple traditional RAG, you’ll retrieve data from a vector database like Pinecone, using semantic search to find the true meaning of the user’s query and retrieve relevant information instead of simply matching keywords in the query. We’ll use Pinecone as an example here, but the concept applies to all vector databases.\nBut before we can retrieve the data, you have to ingest the data. Here are steps to get data into your database:\nChunk the data\nDuring the ingestion step, you’ll load your authoritative data as vectors into Pinecone. You may have structured or unstructured data in the form of text, PDFs, emails, internal wikis, or databases. After cleaning the data, you may need to chunk it by dividing each piece of data, or document, into smaller chunks. Depending on the kind of data you have, the types of queries your users have, and how the results will be used in your application, you’ll need to choose a\nchunking strategy\n.\nCreate vector embeddings\nThen, using an embedding model, you’ll embed each chunk and load it into the vector database. The embedding model is a special type of LLM that converts the data chunk into a vector embedding, a numerical representation of the data’s meaning. This allows computers to search for similar items based on the vector representation of the stored data.\nLoad data into a vector database\nOnce you have vectors, you’ll load them into a vector database. This ingestion step most likely happens offline, independently of your application and your user’s workflow. However, if your data changes, for instance, product inventory is updated, you can update the index in real-time to provide up-to-date information to your users.\nNow that your vector database contains the vector embeddings of your source data, the next step is retrieval.\nRetrieval\nA simple approach to retrieval would use semantic search alone. But by using hybrid search, combining both semantic search (with dense vectors) and lexical search (with\nsparse vectors\n), you can improve the retrieval results even more. This becomes relevant when your users don’t always use the same language to talk about a topic (semantic search) and they refer to internal, domain-specific language (lexical or keyword search) like acronyms, product names, or team names.\nDuring retrieval, we’ll create a vector embedding from the user’s query to use for searching against the vectors in the database. In\nhybrid search\n, you’ll query either a single hybrid index or both a dense and a sparse index. Then we combine and de-duplicate the results and use a reranking model to\nrerank\nthem based on a unified relevance score, and return the most relevant matches.\nAugmentation\nNow that you have the most relevant matches from the retrieval step, you’ll create an augmented prompt with both the search results and the user’s query to send to the LLM. This is where the magic happens.\nAn augmented prompt might look like this:\nQUESTION:\n<the user's question>\n\nCONTEXT:\n<the search results to use as context>\n\nUsing the CONTEXT provided, answer the QUESTION. Keep your answer grounded in the facts of the CONTEXT. If the CONTEXT doesn't contain the answer to the QUESTION, say you don't know.\nBy sending both the search results and the user’s question as context to the LLM, you are encouraging it to use the more accurate and relevant info from the search results during the next generation step.\nGeneration\nUsing the augmented prompt, the LLM now has access to the most pertinent and grounding facts from your vector database so your application can provide an accurate answer for your user, reducing the likelihood of hallucination.\nBut RAG is no longer simply about searching for the right piece of information to inform a model response. With agentic RAG, it's about deciding which questions to ask, which tools to use, when to use them, and then aggregating results to ground answers.\nAgentic RAG\nIn this simple version, the LLM is the agent and decides which retrieval tools to use and when, and how to query those tools.\nWrapping up\nRetrieval-augmented generation has evolved from a buzzword to an indispensable foundation for AI applications. It blends the broad capabilities of foundation models with your company’s authoritative and proprietary knowledge. With AI agents handling more complex use cases, from those\nsupporting professionals servicing complex manufacturing equipment\nto delivering\ndomain-specific agents at scale\n, RAG is not just relevant in 2025. It’s critical for building accurate, relevant, and responsible AI applications that go beyond information retrieval. As AI agents become more autonomous and handle more complex workflows, they’ll need to ground their reasoning in your private and domain-specific data through RAG. The question is no longer\nwhether to implement RAG\n, but how to architect it most effectively for your unique use case and data requirements.\nWant to dig into a RAG code example? Create a\nfree Pinecone account\nand check out our\nexample notebooks\nto implement retrieval-augmented generation with Pinecone or\nget started with Pinecone Assistant\n, to build production-grade chat and agent-based applications quickly.\nShare:\nWas this article helpful?\nYes\nNo\nRecommended for you\nFurther Reading\nLearn\nJun 25, 2025\nBeyond the hype: Why RAG remains essential for modern AI\nJenna Pederson\nLearn\nOct 25, 2024\nBuilding a reliable, curated, and accurate RAG system with Cleanlab and Pinecone\nMatt Turk\nEngineering\nJan 16, 2024\nRAG makes LLMs better and equal\nAmnon\n,\nRoy\n,\nIlai\n,\nNathan\n,\nAmir",
      "code_blocks": [
        {
          "lang": "text",
          "code": "QUESTION:\n<the user's question>\n\nCONTEXT:\n<the search results to use as context>\n\nUsing the CONTEXT provided, answer the QUESTION. Keep your answer grounded in the facts of the CONTEXT. If the CONTEXT doesn't contain the answer to the QUESTION, say you don't know."
        }
      ],
      "headings": [
        {
          "level": 1,
          "text": "Retrieval-Augmented Generation (RAG)"
        },
        {
          "level": 2,
          "text": "Limitations of foundation models"
        },
        {
          "level": 3,
          "text": "Knowledge cutoffs"
        },
        {
          "level": 3,
          "text": "Lack depth in domain-specific knowledge"
        },
        {
          "level": 3,
          "text": "Lack private or proprietary data"
        },
        {
          "level": 3,
          "text": "Loses trust"
        },
        {
          "level": 3,
          "text": "Output generation is probabilistic"
        },
        {
          "level": 2,
          "text": "What is Retrieval-Augmented Generation?"
        },
        {
          "level": 3,
          "text": "RAG in support of agentic workflows"
        },
        {
          "level": 2,
          "text": "How does Retrieval-Augmented Generation work?"
        },
        {
          "level": 3,
          "text": "Ingestion"
        },
        {
          "level": 3,
          "text": "Retrieval"
        },
        {
          "level": 3,
          "text": "Augmentation"
        },
        {
          "level": 3,
          "text": "Generation"
        },
        {
          "level": 2,
          "text": "Wrapping up"
        }
      ]
    }
  ]
}