{
  "name": "Chunking Strategies - Pinecone",
  "url": "https://www.pinecone.io/learn/chunking-strategies/",
  "pages": [
    {
      "url": "https://www.pinecone.io/learn/chunking-strategies/",
      "title": "Chunking Strategies - Pinecone",
      "content_html": "<article><section><div id=\"banner\"><div class=\"border-b\"><div class=\"container\"><div class=\"relative px-4 sm:px-8 flex flex-col gap-8 pb-4 pt-16 md:gap-16 md:pb-8 md:pt-24 xl:flex-row\"><div class=\"w-full xl:w-[13.5rem]\"><a class=\"text-text-secondary hover:text-text-primary border-b-border group inline-block border-b text-sm/[1] leading-none transition-colors duration-300\" href=\"https://www.pinecone.io/learn/\" target=\"_blank\"><span class=\"inline-block transition-transform duration-300 group-hover:-translate-x-1\">←</span> <span>Learn</span></a></div><div class=\"flex-1 space-y-8\"><div><h1 class=\"xxs:text-[38px] sm:text-h1/[1.1] text-text-primary text-[30px]\">Chunking Strategies for LLM Applications</h1></div><div class=\"flex items-start gap-0 xl:items-center\"><div class=\"border-border h-6 w-6 shrink-0 overflow-hidden rounded-full border bg-black align-top\" style=\"position:relative;transform:translate(0px, 0px);z-index:1\"><img alt=\"Roie Schwaber-Cohen\" class=\"h-full w-full object-cover\" data-nimg=\"1\" decoding=\"async\" height=\"24\" loading=\"lazy\" sizes=\"24px\" src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=3840&amp;q=75\" srcset=\"/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=16&amp;q=75 16w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=32&amp;q=75 32w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=48&amp;q=75 48w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=64&amp;q=75 64w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=96&amp;q=75 96w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=128&amp;q=75 128w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=256&amp;q=75 256w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=384&amp;q=75 384w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F9f722be20c8f1800c6a43e8bc87256078071d90d-512x512.webp&amp;w=3840&amp;q=75 3840w\" style=\"color:transparent\" width=\"24\"/></div><div class=\"border-border h-6 w-6 shrink-0 overflow-hidden rounded-full border bg-black align-top\" style=\"position:relative;transform:translate(-5px, 0px);z-index:2\"><img alt=\"Arjun Patel\" class=\"h-full w-full object-cover\" data-nimg=\"1\" decoding=\"async\" height=\"24\" loading=\"lazy\" sizes=\"24px\" src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=3840&amp;q=75\" srcset=\"/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=16&amp;q=75 16w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=32&amp;q=75 32w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=48&amp;q=75 48w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=64&amp;q=75 64w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=96&amp;q=75 96w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=128&amp;q=75 128w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=256&amp;q=75 256w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=384&amp;q=75 384w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F16f2fd4e6da812e1789f3086b62f631f4f3cb38f-1081x1441.jpg&amp;w=3840&amp;q=75 3840w\" style=\"color:transparent\" width=\"24\"/></div><span class=\"text-text-secondary max-w-[21.25rem] self-center text-sm/[1.2]\"><span><a class=\"hover:text-text-primary transition-colors\" href=\"https://www.pinecone.io/author/roie-schwaber-cohen/\" target=\"_blank\">Roie Schwaber-Cohen</a>, </span><span><a class=\"hover:text-text-primary transition-colors\" href=\"https://www.pinecone.io/author/arjun-patel/\" target=\"_blank\">Arjun Patel</a></span></span></div></div><div class=\"flex w-full flex-wrap justify-between gap-4 xl:w-[13.5rem] xl:flex-col\"><div class=\"space-y-4\"><span class=\"text-text-secondary block text-sm/[1.2]\">Jun 28, 2025</span><a class=\"text-text-secondary shrink-0 self-start border p-2 text-xs/[1.2] transition-colors hover:border-brand-blue hover:text-brand-blue\" href=\"https://www.pinecone.io/learn/category/core-components/\" target=\"_blank\">Core Components</a></div><div class=\"mt-auto\"><div class=\"text-text-secondary flex flex-col items-start gap-3 text-sm/[1.2]\">Share:<!-- --> <div class=\"flex items-start\"><a aria-label=\"Share to Twitter\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent\" href=\"https://twitter.com/intent/tweet?url=https://www.pinecone.io/learn/chunking-strategies\" target=\"_blank\"><svg fill=\"none\" height=\"12\" viewbox=\"0 0 14 12\" width=\"14\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M10.6367 0.5625H12.5508L8.33984 5.40234L13.3164 11.9375H9.43359L6.37109 7.97266L2.89844 11.9375H0.957031L5.46875 6.79688L0.710938 0.5625H4.70312L7.4375 4.19922L10.6367 0.5625ZM9.95312 10.7891H11.0195L4.12891 1.65625H2.98047L9.95312 10.7891Z\"></path></svg></a><a aria-label=\"Share to LinkedIn\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent\" href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://www.pinecone.io/learn/chunking-strategies\" target=\"_blank\"><svg fill=\"none\" height=\"13\" viewbox=\"0 0 13 13\" width=\"13\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M11.875 0.125C12.3398 0.125 12.75 0.535156 12.75 1.02734V11.5C12.75 11.9922 12.3398 12.375 11.875 12.375H1.34766C0.882812 12.375 0.5 11.9922 0.5 11.5V1.02734C0.5 0.535156 0.882812 0.125 1.34766 0.125H11.875ZM4.19141 10.625V4.80078H2.38672V10.625H4.19141ZM3.28906 3.98047C3.86328 3.98047 4.32812 3.51562 4.32812 2.94141C4.32812 2.36719 3.86328 1.875 3.28906 1.875C2.6875 1.875 2.22266 2.36719 2.22266 2.94141C2.22266 3.51562 2.6875 3.98047 3.28906 3.98047ZM11 10.625V7.42578C11 5.86719 10.6445 4.63672 8.8125 4.63672C7.9375 4.63672 7.33594 5.12891 7.08984 5.59375H7.0625V4.80078H5.33984V10.625H7.14453V7.75391C7.14453 6.98828 7.28125 6.25 8.23828 6.25C9.16797 6.25 9.16797 7.125 9.16797 7.78125V10.625H11Z\"></path></svg></a><a aria-label=\"Share to Hacker News\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group undefined\" href=\"https://news.ycombinator.com/submitlink?u=https://www.pinecone.io/learn/chunking-strategies\" target=\"_blank\"><svg fill=\"none\" height=\"13\" viewbox=\"0 0 13 13\" width=\"13\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M12.75 0.125V12.375H0.5V0.125H12.75ZM6.95312 7.125L9.05859 3.13281H8.15625L6.92578 5.62109C6.78906 5.89453 6.67969 6.14062 6.57031 6.35938L6.24219 5.62109L4.98438 3.13281H4.02734L6.13281 7.07031V9.66797H6.95312V7.125Z\"></path></svg></a></div></div></div></div><div aria-hidden=\"true\" class=\"pointer-events-none absolute inset-0\" role=\"presentation\"><span class=\"from-background absolute bottom-0 left-0 top-0 h-full w-[1px] border-l\"></span><span class=\"from-background absolute bottom-0 right-0 top-0 h-full w-[1px] border-r\"></span></div></div></div></div></div></section><div class=\"container\"><div class=\"relative px-4 sm:px-8 px-0 pb-10\"><div class=\"flex flex-col justify-between lg:flex-row\"><div class=\"mt-4 w-full flex-col gap-10 overflow-hidden md:max-w-full\"><div class=\"prose max-w-none\"><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"What-is-chunking\">What is chunking?</h2><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">In the context of building LLM-related applications, <strong>chunking</strong> is the process of breaking down large text into smaller segments called chunks.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">It’s an essential preprocessing technique that helps optimize the relevance of the content ultimately stored in a vector database. The trick lies in finding chunks that are big enough to contain meaningful information, while small enough to enable performant applications and low latency responses for workloads such as retrieval augmented generation and agentic workflows.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">In this post, we’ll explore several chunking methods and discuss the tradeoffs needed when choosing a chunking size and method. Finally, we’ll give some recommendations for determining the best chunk size and method that will be appropriate for your application.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><div class=\"border border-border mt-8 bg-background-light max-w-[875px] w-full h-fit flex flex-col items-start justify-center py-8 px-8 md:px-8 box-border text-left text-[33px] text-text-primary\" id=\"in-article-cta\"><div class=\"h-[fit] flex flex-col items-start justify-center gap-[14px]\"><div class=\"relative leading-[120%] font-semibold\">Start using Pinecone for free</div><div class=\"relative text-body md:text-xl leading-[140%] text-text-secondary\">Pinecone is the developer-favorite <a class=\"text-text-primary underline! hover:text-text-secondary\" href=\"https://www.pinecone.io/learn/vector-database/\" target=\"_blank\">vector database</a> that's fast and easy to use at any scale.</div></div><div class=\"flex flex-row items-center gap-5 justify-end text-center text-sm sm:text-[1rem] xs::text-base text-black mt-5\"><a class=\"bg-brand-blue text-white dark:hover:text-background px-3 py-2 text-left sm:text-center font-semibold transition-colors duration-300 hover:bg-primary-dark focus:outline-offset-2 focus:outline-alpha1 xxs:px-4 xxs:py-[0.625rem]\" href=\"https://app.pinecone.io\" id=\"in-article-cta-primary\" target=\"_blank\">Sign up free</a><a class=\"flex items-center justify-center text-left sm:text-center text-text-primary transition-colors duration-300 hover:text-text-secondary hover:underline\" href=\"https://docs.pinecone.io/page/examples\" id=\"in-article-cta-secondary\" target=\"_blank\">View Examples<svg class=\"ml-1.5 inline-block w-4\" fill=\"none\" viewbox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-primary\" clip-rule=\"evenodd\" d=\"M18.11 3.788c-.463-.037-1.058-.038-1.91-.038H14a.75.75 0 0 1 0-1.5h2.232c.813 0 1.469 0 2 .043.546.045 1.026.14 1.47.366a3.75 3.75 0 0 1 1.64 1.639c.226.444.32.924.365 1.47.043.531.043 1.187.043 2V10a.75.75 0 0 1-1.5 0V7.8c0-.852 0-1.447-.038-1.91-.034-.414-.095-.668-.182-.86l-7.5 7.5a.75.75 0 1 1-1.06-1.06l7.5-7.5c-.192-.087-.446-.148-.86-.182zM7.767 4.25H11a.75.75 0 0 1 0 1.5H7.8c-.852 0-1.447 0-1.91.038-.453.037-.714.107-.911.207a2.25 2.25 0 0 0-.984.984c-.1.197-.17.458-.207.912-.037.462-.038 1.057-.038 1.909v6.4c0 .853 0 1.447.038 1.91.037.453.107.714.207.912.216.423.56.767.984.983.197.1.458.17.912.207.462.037 1.057.038 1.909.038h6.4c.853 0 1.447 0 1.91-.038.453-.037.714-.107.911-.207.424-.216.768-.56.984-.983.1-.198.17-.459.207-.913.037-.462.038-1.056.038-1.909V13a.75.75 0 0 1 1.5 0v3.232c0 .813 0 1.469-.043 2-.045.546-.14 1.026-.366 1.47a3.75 3.75 0 0 1-1.639 1.64c-.444.226-.924.32-1.47.365-.531.043-1.187.043-2 .043H7.768c-.813 0-1.469 0-2-.043-.546-.045-1.026-.14-1.47-.366a3.75 3.75 0 0 1-1.64-1.639c-.226-.444-.32-.924-.365-1.47-.043-.531-.043-1.187-.043-2V9.768c0-.813 0-1.469.043-2 .045-.546.14-1.026.366-1.47a3.75 3.75 0 0 1 1.639-1.64c.444-.226.924-.32 1.47-.365.531-.043 1.187-.043 2-.043z\" fill-rule=\"evenodd\"></path></svg></a></div></div><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"Why-do-we-need-chunking-for-our-applications\">Why do we need chunking for our applications?</h2><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">There are two big reasons why chunking is necessary for any application involving vector databases or LLMs: to ensure embedding models can fit the data into their context windows, and to ensure the chunks themselves contain the information necessary for search.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">All embedding models have context windows, which determine the amount of information in tokens that can be processed into a single fixed size vector. Exceeding this context window may means the excess tokens are truncated, or thrown away, before being processed into a vector. This is potentially harmful as important context could be removed from the representation of the text, which prevents it from being surfaced during a search.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Furthermore, it isn’t enough just to right-size your data for a model; the resulting chunks must contain information that is relevant to search over. If the chunk contains a set of sentences that aren’t useful without context, they may not be surfaced when querying!</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Chunking's-role-in-semantic-search\">Chunking’s role in semantic search</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">For example, in semantic search, we index a corpus of documents, with each document containing valuable information on a specific topic. Due to the way embedding models work, those documents will need to be chunked, and similarity is determined by chunk-level comparisons to the input query vector. Then, these similar chunks are returned back to the user. By finding an effective chunking strategy, we can ensure our search results accurately capture the essence of the user’s query.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">If our chunks are too small or too large, it may lead to imprecise search results or missed opportunities to surface relevant content. As a rule of thumb, if the chunk of text makes sense without the surrounding context to a human, it will make sense to the language model as well. Therefore, finding the optimal chunk size for the documents in the corpus is crucial to ensuring that the search results are accurate and relevant.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Chunking's-role-for-agentic-applications-and-retrieval-augmented-generation\">Chunking’s role for agentic applications and retrieval-augmented generation</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Agents may need access to up-to-date information from databases in order to call tools, make decisions, and respond to user queries. Chunks returned from searches over databases consume context during a session, and ground the agent’s responses.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">We use the embedded chunks to build the context based on a knowledge base the agent has access to. This context grounds the agent in trusted information.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Similar to how semantic search relies on a good chunking strategy to provide usable outputs, agentic applications need meaningful chunks of information in order to proceed. If an agent is misinformed, or provided information without sufficient context, it may waste tokens generating hallucinations or calling the wrong tools.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"The-Role-of-Chunking-for-Long-Context-LLMs\">The Role of Chunking for Long Context LLMs</h2><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">In some cases, like when using o1 or Claude 4 Sonnet with a 200k context window, un-chunked documents may still fit in context. Still, using large chunks may increase latency and cost in downstream responses. </p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Moreover, long context embedding and LLM models suffer fro<a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://arxiv.org/abs/2307.03172\" target=\"_blank\">m the lost-in-the-middle problem</a>, where relevant information buried inside long documents is missed, even when included in generation. The solution to this problem is ensuring the optimal amount of information is passed to a downstream LLM, which necessarily reduces latency and ensures quality.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"What-should-we-think-about-when-choosing-a-chunking-strategy\">What should we think about when choosing a chunking strategy?</h2><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Several variables play a role in determining the best chunking strategy, and these variables vary depending on the use case. Here are some key aspects to keep in mind:</p><ol class=\"!marker:text-text-primary ml-8! list-decimal! [&amp;_ul]:list-circle! mt-4 [&amp;_ol]:mt-0 [&amp;_ul]:mt-0\"><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"><strong>What kind of data is being chunked?</strong> Are you working with long documents, such as articles or books, or shorter content, like tweets, product descriptions, or chat messages? Small documents may not need to be chunked at all, while larger ones may exhibit certain structure that will inform chunking strategy, such as sub-headers or chapters.</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"><strong>Which embedding model are you using?</strong> Different embedding models have differing capacities for information, especially on specialized domains like code, finance, medical, or legal information. And, the way these models are trained can strongly affect how they perform in practice. After choosing an appropriate model for your domain, be sure to adapt your chunking strategy to align with expected document types the model has been trained on.</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"><strong>What are your expectations for the length and complexity of user queries?</strong> Will they be short and specific or long and complex? This may inform the way you choose to chunk your content as well so that there’s a closer correlation between the embedded query and embedded chunks.</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-decimal:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"><strong>How will the retrieved results be utilized within your specific application?</strong> For example, will they be used for semantic search, question answering, retrieval augmented generation, or even an agentic workflow? For example, the amount of information a human may review from a search result may be smaller or larger than what an LLM may need to generate a response. These users determine how your data should be represented within the vector database.</span></li></ol><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Answering these questions beforehand will allow you to choose a chunking strategy that balances performance and accuracy.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"Embedding-short-and-long-content\">Embedding short and long content</h2><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">When we embed content, we can expect distinct behaviors depending on whether the content is short (like sentences) or long (like paragraphs or entire documents).</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">When a <strong>sentence</strong> is embedded, the resulting vector focuses on the sentence’s specific meaning. This could be handy in situations where the vector search is used for (sentence-level) classification, recommendation systems, or applications that allow for searches over shorter summaries before longer documents are processed. The search process is then, finding sentences similar in meaning to query sentences or questions. In cases where sentences themselves are considered individual documents, you wouldn’t need to chunk at all!</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">When a <strong>full paragraph or document</strong> is embedded, the embedding process considers both the overall context and the relationships between the sentences and phrases within the text. This can result in a more comprehensive vector representation that captures the broader meaning and themes of the text. Larger input text sizes, on the other hand, may introduce noise or dilute the significance of individual sentences or phrases, making finding precise matches when querying the index more difficult. These chunks help support use cases such as question-answering, where answers may be a few paragraphs or more. Many modern AI applications work with longer documents, which almost always require chunking.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"Chunking-methods\">Chunking methods</h2><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Fixed-size-chunking\">Fixed-size chunking</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">This is the most common and straightforward approach to chunking: we simply decide the number of tokens in our chunk, and use this number to break up our documents into fixed size chunks. Usually, this number is the max context window size of the embedding model (such as 1024 for llama-text-embed-v2, or 8196 for text-embedding-3-small). Keep in mind that different embedding models may tokenize text differently, so you will need to estimate token counts accurately.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Fixed-sized chunking will be the best path in most cases, and we recommend starting here and iterating only after determining it insufficient.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id='\"Content-aware\"-Chunking'>“Content-aware” Chunking</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Although fixed-size chunking is quite easy to implement, it can ignore critical structure within documents that can be used to inform relevant chunks. Content-aware chunking refers to strategies that adhere to structure to help inform the meaning of our chunks.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><h4 class=\"text-h4-mobile lg:text-h4 text-text-primary\" id=\"Simple-Sentence-and-Paragraph-splitting\" style=\"margin-block:36px\">Simple Sentence and Paragraph splitting</h4><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">As we mentioned before, some embedding models are optimized for embedding sentence-level content. But sometimes, sentences need to be mined from larger text datasets that aren’t preprocessed. In these cases, it’s necessary to use sentence chunking, and there are several approaches and tools available to do this:</p><ul class=\"!marker:text-text-primary ml-8! list-disc! [&amp;_ul]:list-circle! mt-4 [&amp;_ol]:mt-0 [&amp;_ul]:mt-0\"><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-circle:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"><strong>Naive splitting:</strong> The most naive approach would be to split sentences by periods (“.”), new lines, or white space.</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-circle:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"><a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://www.nltk.org/\" target=\"_blank\"><strong>NLTK</strong></a>: The Natural Language Toolkit (NLTK) is a popular Python library for working with human language data. It provides a trained sentence tokenizer that can split the text into sentences, helping to create more meaningful chunks.</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-circle:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"><a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://spacy.io/\" target=\"_blank\"><strong>spaCy</strong></a>: spaCy is another powerful Python library for NLP tasks. It offers a sophisticated sentence segmentation feature that can efficiently divide the text into separate sentences, enabling better context preservation in the resulting chunks.</span></li></ul><h4 class=\"text-h4-mobile lg:text-h4 text-text-primary\" id=\"Recursive-Character-Level-Chunking\" style=\"margin-block:36px\">Recursive Character Level Chunking</h4><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">LangChain implements a <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://python.langchain.com/docs/how_to/recursive_text_splitter/\" target=\"_blank\">RecursiveCharacterTextSplitter</a> that tries to split text using separators in a given order. The default behavior of the splitter uses the [\"\\n\\n\", \"\\n\", \" \", \"\"] separators to break paragraphs, sentences and words depending on a given chunk size.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">This is a great middle ground between always splitting on a specific character and using a more semantic splitter, while also ensuring fixed chunk sizes when possible.</p><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Document-structure-based-chunking\">Document structure-based chunking</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">When chunking large documents such as PDFs, DOCX, HTML, code snippets, Markdown files and LaTex, specialized chunking methods can help preserve the original structure of the content during chunk creation.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><ul class=\"!marker:text-text-primary ml-8! list-disc! [&amp;_ul]:list-circle! mt-4 [&amp;_ol]:mt-0 [&amp;_ul]:mt-0\"><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-circle:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">PDF documents contain loads of headers, text, tables, and other bits and pieces that require preprocessing to chunk. LangChain has some handy utilities to help process these documents, while Pinecone Assistant can <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://docs.pinecone.io/guides/assistant/files-overview\" target=\"_blank\">chunk and processes these for you</a></span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-circle:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">HTML, from scraped web pages can contain tags (&lt;p&gt; for paragraphs, or &lt;title&gt; for titles) that can inform text to be broken up or identified, like on product pages or blog posts. Roll your own parser, or use <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://python.langchain.com/docs/how_to/split_html/\" target=\"_blank\">LangChain splitters here to process these for chunking</a></span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-circle:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"><a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://www.markdownguide.org/\" target=\"_blank\"><strong>Markdown</strong></a>: Markdown is a lightweight markup language commonly used for formatting text. By recognizing the Markdown syntax (e.g., headings, lists, and code blocks), you can intelligently divide the content based on its structure and hierarchy, resulting in more semantically coherent chunks.</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-circle:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"><a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://www.latex-project.org/\" target=\"_blank\"><strong>LaTex</strong></a>: LaTeX is a document preparation system and markup language often used for academic papers and technical documents. By parsing the LaTeX commands and environments, you can create chunks that respect the logical organization of the content (e.g., sections, subsections, and equations), leading to more accurate and contextually relevant results.</span></li></ul><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Semantic-Chunking\">Semantic Chunking</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">A new experimental technique for approaching chunking was first introduced by <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://www.linkedin.com/in/gregkamradt/\" target=\"_blank\">Greg Kamradt</a>. <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb\" target=\"_blank\">In his notebook</a>, Kamradt rightfully points to the fact that a global chunking size may be too trivial of a mechanism to take into account the <strong>meaning</strong> of segments within the document. If we use this type of mechanism, we can’t know if we’re combining segments that have anything to do with one another.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Luckily, if you’re building an application with LLMs, you most likely already have the ability to create <strong>embeddings -</strong> and embeddings can be used to extract the semantic meaning present in your data. This semantic analysis can be used to create chunks that are made up of sentences that talk about the same theme or topic.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Semantic chunking involves breaking a document into sentences, grouping each sentence with its surrounding sentences, and generating embeddings for these groups. By comparing the semantic distance between each group and its predecessor, you can identify where the topic or theme shifts, which defines the chunk boundaries. <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://github.com/pinecone-io/examples/blob/master/learn/generation/better-rag/02b-semantic-chunking.ipynb\" target=\"_blank\">You can learn more about applying semantic chunking with Pinecone here.</a></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Contextual-Chunking-with-LLMs\">Contextual Chunking with LLMs</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Sometimes, it’s not possible to chunk information from a larger complex document without losing the context entirely. This can happen when the documents are many hundreds of pages, change topics frequently, or require understanding from many related portions of the document. Anthropic introduced <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://www.anthropic.com/news/contextual-retrieval\" target=\"_blank\">contextual retrieval in 2024</a> to help address this problem.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Anthropic prompted a Claude instance with an entire document and it’s chunk, in order to generate a contextualized description, which is appended to the chunk and then embedded. The description helps retain the high-level summary meaning of the document to the chunk, which exposes this information to incoming queries. To avoid processing the document each time, it’s cached within the prompt for all necessary chunks. <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://www.youtube.com/watch?v=u-ocR-2P_YA\" target=\"_blank\">You can learn more about contextual retrieval in our video here</a> and <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://github.com/pinecone-io/contextual-webinar-rag/tree/main\" target=\"_blank\">our code example here.</a></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"Figuring-out-the-best-chunking-strategy-for-your-application\">Figuring out the best chunking strategy for your application</h2><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Here are some pointers to help decide a strategy if fixed chunking doesn’t easily apply to your use case.</p><ul class=\"!marker:text-text-primary ml-8! list-disc! [&amp;_ul]:list-circle! mt-4 [&amp;_ol]:mt-0 [&amp;_ul]:mt-0\"><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-circle:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"><strong>Selecting a Range of Chunk Sizes</strong> - Once your data is preprocessed, the next step is to choose a range of potential chunk sizes to test. As mentioned previously, the choice should take into account the nature of the content (e.g., short messages or lengthy documents), the embedding model you’ll use, and its capabilities (e.g., token limits). The objective is to find a balance between preserving context and maintaining accuracy. Start by exploring a variety of chunk sizes, including smaller chunks (e.g., 128 or 256 tokens) for capturing more granular semantic information and larger chunks (e.g., 512 or 1024 tokens) for retaining more context.</span></li><li class=\"pl-2! text-body-mobile lg:text-body !marker:text-text-primary list-circle:text-text-primary\"><span class=\"mt-0 block leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"><strong>Evaluating the Performance of Each Chunk Size</strong> - In order to test various chunk sizes, you can either use multiple indices or a single index with multiple <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://docs.pinecone.io/docs/namespaces\" target=\"_blank\">namespaces</a>. With a representative dataset, create the embeddings for the chunk sizes you want to test and save them in your index (or indices). You can then run a series of queries for which you can evaluate quality, and compare the performance of the various chunk sizes. This is most likely to be an iterative process, where you test different chunk sizes against different queries until you can determine the best-performing chunk size for your content and expected queries.</span></li></ul><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><h3 class=\"text-h3-mobile lg:text-h3 text-text-primary my-[40px]\" id=\"Post-processing-chunks-with-chunk-expansion\">Post-processing chunks with chunk expansion</h3><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">It’s important to remember that you aren’t entirely married to your chunking strategy. When querying chunked data in a vector database, the retrieved information is typically the top semantically similar chunks given a user query. But users, agents or LLMs may need more surrounding context in order to adequately interpret the chunk.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Chunk expansion is an easy way to post-process chunked data from a database, by retrieving neighboring chunks within a window for each chunk in a retrieved set of chunks. Chunks could be expanded to paragraphs, pages, or even whole documents depending on your use case.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Coupling a chunking strategy with a good chunk expansion on querying can ensure low latency searches without compromising on context.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"Wrapping-up\">Wrapping up</h2><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Chunking your content is may appear straightforward in most cases - but it could present some challenges when you start wandering off the beaten path. There’s no one-size-fits-all solution to chunking, so what works for one use case may not work for another.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"></p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Want to get started experimenting with chunking strategies? Create a <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://app.pinecone.io/\" target=\"_blank\">free Pinecone account</a> and check out our <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://docs.pinecone.io/examples/notebooks\" target=\"_blank\">example notebooks</a> to implement chunking via various applications like semantic search, retrieval augmented generation or agentic applications with Pinecone.</p></div><div class=\"mt-10 block md:hidden\"><div class=\"text-text-secondary flex flex-col items-start gap-3 text-sm/[1.2]\">Share:<!-- --> <div class=\"flex items-start\"><a aria-label=\"Share to Twitter\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent\" href=\"https://twitter.com/intent/tweet?url=https://www.pinecone.io/learn/chunking-strategies\" target=\"_blank\"><svg fill=\"none\" height=\"12\" viewbox=\"0 0 14 12\" width=\"14\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M10.6367 0.5625H12.5508L8.33984 5.40234L13.3164 11.9375H9.43359L6.37109 7.97266L2.89844 11.9375H0.957031L5.46875 6.79688L0.710938 0.5625H4.70312L7.4375 4.19922L10.6367 0.5625ZM9.95312 10.7891H11.0195L4.12891 1.65625H2.98047L9.95312 10.7891Z\"></path></svg></a><a aria-label=\"Share to LinkedIn\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent\" href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://www.pinecone.io/learn/chunking-strategies\" target=\"_blank\"><svg fill=\"none\" height=\"13\" viewbox=\"0 0 13 13\" width=\"13\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M11.875 0.125C12.3398 0.125 12.75 0.535156 12.75 1.02734V11.5C12.75 11.9922 12.3398 12.375 11.875 12.375H1.34766C0.882812 12.375 0.5 11.9922 0.5 11.5V1.02734C0.5 0.535156 0.882812 0.125 1.34766 0.125H11.875ZM4.19141 10.625V4.80078H2.38672V10.625H4.19141ZM3.28906 3.98047C3.86328 3.98047 4.32812 3.51562 4.32812 2.94141C4.32812 2.36719 3.86328 1.875 3.28906 1.875C2.6875 1.875 2.22266 2.36719 2.22266 2.94141C2.22266 3.51562 2.6875 3.98047 3.28906 3.98047ZM11 10.625V7.42578C11 5.86719 10.6445 4.63672 8.8125 4.63672C7.9375 4.63672 7.33594 5.12891 7.08984 5.59375H7.0625V4.80078H5.33984V10.625H7.14453V7.75391C7.14453 6.98828 7.28125 6.25 8.23828 6.25C9.16797 6.25 9.16797 7.125 9.16797 7.78125V10.625H11Z\"></path></svg></a><a aria-label=\"Share to Hacker News\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group undefined\" href=\"https://news.ycombinator.com/submitlink?u=https://www.pinecone.io/learn/chunking-strategies\" target=\"_blank\"><svg fill=\"none\" height=\"13\" viewbox=\"0 0 13 13\" width=\"13\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M12.75 0.125V12.375H0.5V0.125H12.75ZM6.95312 7.125L9.05859 3.13281H8.15625L6.92578 5.62109C6.78906 5.89453 6.67969 6.14062 6.57031 6.35938L6.24219 5.62109L4.98438 3.13281H4.02734L6.13281 7.07031V9.66797H6.95312V7.125Z\"></path></svg></a></div></div></div><div class=\"mt-10\"><div class=\"inline-block border px-4 py-3\"><p class=\"text-text-secondary text-sm\">Was this article helpful?</p><div class=\"mt-2 flex gap-4\"><button class=\"text-text-secondary hover:text-text-primary flex cursor-pointer items-center gap-2 text-sm transition-colors\"><svg class=\"lucide lucide-thumbs-up h-4 w-4\" fill=\"none\" height=\"24\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewbox=\"0 0 24 24\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M7 10v12\"></path><path d=\"M15 5.88 14 10h5.83a2 2 0 0 1 1.92 2.56l-2.33 8A2 2 0 0 1 17.5 22H4a2 2 0 0 1-2-2v-8a2 2 0 0 1 2-2h2.76a2 2 0 0 0 1.79-1.11L12 2h0a3.13 3.13 0 0 1 3 3.88Z\"></path></svg>Yes</button><button class=\"text-text-secondary hover:text-text-primary flex cursor-pointer items-center gap-2 text-sm transition-colors\"><svg class=\"lucide lucide-thumbs-down h-4 w-4\" fill=\"none\" height=\"24\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewbox=\"0 0 24 24\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M17 14V2\"></path><path d=\"M9 18.12 10 14H4.17a2 2 0 0 1-1.92-2.56l2.33-8A2 2 0 0 1 6.5 2H20a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2h-2.76a2 2 0 0 0-1.79 1.11L12 22h0a3.13 3.13 0 0 1-3-3.88Z\"></path></svg>No</button></div></div></div></div></div><div aria-hidden=\"true\" class=\"pointer-events-none absolute left-0 top-0 w-full\" role=\"presentation\"><span class=\"z-1 absolute border-l top-0 left-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent left-0 top-1 h-[10rem]\"></span><span class=\"z-1 absolute border-l top-0 right-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent right-0 top-1 h-[10rem]\"></span></div><div aria-hidden=\"true\" class=\"pointer-events-none absolute bottom-0 left-0 w-full\" role=\"presentation\"><span class=\"z-1 absolute border-l bottom-0 left-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-b from-background z-2 absolute w-[1px] from-10% to-transparent left-0 bottom-1 h-[10rem]\"></span><span class=\"z-1 absolute border-l bottom-0 right-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-b from-background z-2 absolute w-[1px] from-10% to-transparent right-0 bottom-1 h-[10rem]\"></span></div></div></div><div class=\"container\"><div class=\"relative px-4 sm:px-8 border-border border-t !px-0\"><div class=\"w-full pt-24 pb-20 border-border flex-col justify-start items-start gap-8 inline-flex\"><div class=\"self-stretch min-h-[54px] px-8 flex-col justify-start items-start gap-2 flex\"><div class=\"self-stretch text-brand-blue text-xs font-medium font-['GT Planar VCTR'] uppercase leading-[18px] tracking-widest\">Recommended for you</div><div class=\"self-stretch text-text-primary text-[32px] font-bold font-['GT Planar VCTR'] leading-[38.40px]\">Further Reading</div></div><div class=\"w-full grid grid-cols-1 md:grid-cols-3 divide-y md:divide-y-0 md:divide-x divide-border border border-border\"><a class=\"group gap-4 p-8 bg-background hover:bg-background-light flex flex-col justify-between transition-colors duration-300\" href=\"https://www.pinecone.io/learn/slab-architecture/\" target=\"_blank\"><div class=\"flex flex-col gap-4\"><div class=\"flex items-start justify-between\"><div class=\"text-text-secondary text-sm leading-[21px]\"></div><div class=\"text-text-secondary text-sm leading-[21px]\">Nov 4, 2025</div></div><div class=\"text-text-primary text-xl font-medium leading-relaxed min-h-[150px]\">Inside Pinecone: Slab Architecture</div></div><div class=\"text-text-secondary text-sm leading-[16.8px]\">11<!-- --> min read</div></a><a class=\"group gap-4 p-8 bg-background hover:bg-background-light flex flex-col justify-between transition-colors duration-300\" href=\"https://www.pinecone.io/learn/context-engineering/\" target=\"_blank\"><div class=\"flex flex-col gap-4\"><div class=\"flex items-start justify-between\"><div class=\"text-text-secondary text-sm leading-[21px]\"></div><div class=\"text-text-secondary text-sm leading-[21px]\">Jul 15, 2025</div></div><div class=\"text-text-primary text-xl font-medium leading-relaxed min-h-[150px]\">What is Context Engineering?</div></div><div class=\"text-text-secondary text-sm leading-[16.8px]\">8<!-- --> min read</div></a><a class=\"group gap-4 p-8 bg-background hover:bg-background-light flex flex-col justify-between transition-colors duration-300\" href=\"https://www.pinecone.io/learn/rag-2025/\" target=\"_blank\"><div class=\"flex flex-col gap-4\"><div class=\"flex items-start justify-between\"><div class=\"text-text-secondary text-sm leading-[21px]\"></div><div class=\"text-text-secondary text-sm leading-[21px]\">Jun 25, 2025</div></div><div class=\"text-text-primary text-xl font-medium leading-relaxed min-h-[150px]\">Beyond the hype: Why RAG remains essential for modern AI</div></div><div class=\"text-text-secondary text-sm leading-[16.8px]\">7<!-- --> min read</div></a></div></div><div aria-hidden=\"true\" class=\"pointer-events-none absolute left-0 top-0 w-full\" role=\"presentation\"><span class=\"z-1 absolute border-l top-0 left-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent left-0 top-1 h-[10rem]\"></span><span class=\"z-1 absolute border-l top-0 right-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent right-0 top-1 h-[10rem]\"></span></div></div></div></article>",
      "text": "←\nLearn\nChunking Strategies for LLM Applications\nRoie Schwaber-Cohen\n,\nArjun Patel\nJun 28, 2025\nCore Components\nShare:\nWhat is chunking?\nIn the context of building LLM-related applications,\nchunking\nis the process of breaking down large text into smaller segments called chunks.\nIt’s an essential preprocessing technique that helps optimize the relevance of the content ultimately stored in a vector database. The trick lies in finding chunks that are big enough to contain meaningful information, while small enough to enable performant applications and low latency responses for workloads such as retrieval augmented generation and agentic workflows.\nIn this post, we’ll explore several chunking methods and discuss the tradeoffs needed when choosing a chunking size and method. Finally, we’ll give some recommendations for determining the best chunk size and method that will be appropriate for your application.\nStart using Pinecone for free\nPinecone is the developer-favorite\nvector database\nthat's fast and easy to use at any scale.\nSign up free\nView Examples\nWhy do we need chunking for our applications?\nThere are two big reasons why chunking is necessary for any application involving vector databases or LLMs: to ensure embedding models can fit the data into their context windows, and to ensure the chunks themselves contain the information necessary for search.\nAll embedding models have context windows, which determine the amount of information in tokens that can be processed into a single fixed size vector. Exceeding this context window may means the excess tokens are truncated, or thrown away, before being processed into a vector. This is potentially harmful as important context could be removed from the representation of the text, which prevents it from being surfaced during a search.\nFurthermore, it isn’t enough just to right-size your data for a model; the resulting chunks must contain information that is relevant to search over. If the chunk contains a set of sentences that aren’t useful without context, they may not be surfaced when querying!\nChunking’s role in semantic search\nFor example, in semantic search, we index a corpus of documents, with each document containing valuable information on a specific topic. Due to the way embedding models work, those documents will need to be chunked, and similarity is determined by chunk-level comparisons to the input query vector. Then, these similar chunks are returned back to the user. By finding an effective chunking strategy, we can ensure our search results accurately capture the essence of the user’s query.\nIf our chunks are too small or too large, it may lead to imprecise search results or missed opportunities to surface relevant content. As a rule of thumb, if the chunk of text makes sense without the surrounding context to a human, it will make sense to the language model as well. Therefore, finding the optimal chunk size for the documents in the corpus is crucial to ensuring that the search results are accurate and relevant.\nChunking’s role for agentic applications and retrieval-augmented generation\nAgents may need access to up-to-date information from databases in order to call tools, make decisions, and respond to user queries. Chunks returned from searches over databases consume context during a session, and ground the agent’s responses.\nWe use the embedded chunks to build the context based on a knowledge base the agent has access to. This context grounds the agent in trusted information.\nSimilar to how semantic search relies on a good chunking strategy to provide usable outputs, agentic applications need meaningful chunks of information in order to proceed. If an agent is misinformed, or provided information without sufficient context, it may waste tokens generating hallucinations or calling the wrong tools.\nThe Role of Chunking for Long Context LLMs\nIn some cases, like when using o1 or Claude 4 Sonnet with a 200k context window, un-chunked documents may still fit in context. Still, using large chunks may increase latency and cost in downstream responses.\nMoreover, long context embedding and LLM models suffer fro\nm the lost-in-the-middle problem\n, where relevant information buried inside long documents is missed, even when included in generation. The solution to this problem is ensuring the optimal amount of information is passed to a downstream LLM, which necessarily reduces latency and ensures quality.\nWhat should we think about when choosing a chunking strategy?\nSeveral variables play a role in determining the best chunking strategy, and these variables vary depending on the use case. Here are some key aspects to keep in mind:\nWhat kind of data is being chunked?\nAre you working with long documents, such as articles or books, or shorter content, like tweets, product descriptions, or chat messages? Small documents may not need to be chunked at all, while larger ones may exhibit certain structure that will inform chunking strategy, such as sub-headers or chapters.\nWhich embedding model are you using?\nDifferent embedding models have differing capacities for information, especially on specialized domains like code, finance, medical, or legal information. And, the way these models are trained can strongly affect how they perform in practice. After choosing an appropriate model for your domain, be sure to adapt your chunking strategy to align with expected document types the model has been trained on.\nWhat are your expectations for the length and complexity of user queries?\nWill they be short and specific or long and complex? This may inform the way you choose to chunk your content as well so that there’s a closer correlation between the embedded query and embedded chunks.\nHow will the retrieved results be utilized within your specific application?\nFor example, will they be used for semantic search, question answering, retrieval augmented generation, or even an agentic workflow? For example, the amount of information a human may review from a search result may be smaller or larger than what an LLM may need to generate a response. These users determine how your data should be represented within the vector database.\nAnswering these questions beforehand will allow you to choose a chunking strategy that balances performance and accuracy.\nEmbedding short and long content\nWhen we embed content, we can expect distinct behaviors depending on whether the content is short (like sentences) or long (like paragraphs or entire documents).\nWhen a\nsentence\nis embedded, the resulting vector focuses on the sentence’s specific meaning. This could be handy in situations where the vector search is used for (sentence-level) classification, recommendation systems, or applications that allow for searches over shorter summaries before longer documents are processed. The search process is then, finding sentences similar in meaning to query sentences or questions. In cases where sentences themselves are considered individual documents, you wouldn’t need to chunk at all!\nWhen a\nfull paragraph or document\nis embedded, the embedding process considers both the overall context and the relationships between the sentences and phrases within the text. This can result in a more comprehensive vector representation that captures the broader meaning and themes of the text. Larger input text sizes, on the other hand, may introduce noise or dilute the significance of individual sentences or phrases, making finding precise matches when querying the index more difficult. These chunks help support use cases such as question-answering, where answers may be a few paragraphs or more. Many modern AI applications work with longer documents, which almost always require chunking.\nChunking methods\nFixed-size chunking\nThis is the most common and straightforward approach to chunking: we simply decide the number of tokens in our chunk, and use this number to break up our documents into fixed size chunks. Usually, this number is the max context window size of the embedding model (such as 1024 for llama-text-embed-v2, or 8196 for text-embedding-3-small). Keep in mind that different embedding models may tokenize text differently, so you will need to estimate token counts accurately.\nFixed-sized chunking will be the best path in most cases, and we recommend starting here and iterating only after determining it insufficient.\n“Content-aware” Chunking\nAlthough fixed-size chunking is quite easy to implement, it can ignore critical structure within documents that can be used to inform relevant chunks. Content-aware chunking refers to strategies that adhere to structure to help inform the meaning of our chunks.\nSimple Sentence and Paragraph splitting\nAs we mentioned before, some embedding models are optimized for embedding sentence-level content. But sometimes, sentences need to be mined from larger text datasets that aren’t preprocessed. In these cases, it’s necessary to use sentence chunking, and there are several approaches and tools available to do this:\nNaive splitting:\nThe most naive approach would be to split sentences by periods (“.”), new lines, or white space.\nNLTK\n: The Natural Language Toolkit (NLTK) is a popular Python library for working with human language data. It provides a trained sentence tokenizer that can split the text into sentences, helping to create more meaningful chunks.\nspaCy\n: spaCy is another powerful Python library for NLP tasks. It offers a sophisticated sentence segmentation feature that can efficiently divide the text into separate sentences, enabling better context preservation in the resulting chunks.\nRecursive Character Level Chunking\nLangChain implements a\nRecursiveCharacterTextSplitter\nthat tries to split text using separators in a given order. The default behavior of the splitter uses the [\"\\n\\n\", \"\\n\", \" \", \"\"] separators to break paragraphs, sentences and words depending on a given chunk size.\nThis is a great middle ground between always splitting on a specific character and using a more semantic splitter, while also ensuring fixed chunk sizes when possible.\nDocument structure-based chunking\nWhen chunking large documents such as PDFs, DOCX, HTML, code snippets, Markdown files and LaTex, specialized chunking methods can help preserve the original structure of the content during chunk creation.\nPDF documents contain loads of headers, text, tables, and other bits and pieces that require preprocessing to chunk. LangChain has some handy utilities to help process these documents, while Pinecone Assistant can\nchunk and processes these for you\nHTML, from scraped web pages can contain tags (<p> for paragraphs, or <title> for titles) that can inform text to be broken up or identified, like on product pages or blog posts. Roll your own parser, or use\nLangChain splitters here to process these for chunking\nMarkdown\n: Markdown is a lightweight markup language commonly used for formatting text. By recognizing the Markdown syntax (e.g., headings, lists, and code blocks), you can intelligently divide the content based on its structure and hierarchy, resulting in more semantically coherent chunks.\nLaTex\n: LaTeX is a document preparation system and markup language often used for academic papers and technical documents. By parsing the LaTeX commands and environments, you can create chunks that respect the logical organization of the content (e.g., sections, subsections, and equations), leading to more accurate and contextually relevant results.\nSemantic Chunking\nA new experimental technique for approaching chunking was first introduced by\nGreg Kamradt\n.\nIn his notebook\n, Kamradt rightfully points to the fact that a global chunking size may be too trivial of a mechanism to take into account the\nmeaning\nof segments within the document. If we use this type of mechanism, we can’t know if we’re combining segments that have anything to do with one another.\nLuckily, if you’re building an application with LLMs, you most likely already have the ability to create\nembeddings -\nand embeddings can be used to extract the semantic meaning present in your data. This semantic analysis can be used to create chunks that are made up of sentences that talk about the same theme or topic.\nSemantic chunking involves breaking a document into sentences, grouping each sentence with its surrounding sentences, and generating embeddings for these groups. By comparing the semantic distance between each group and its predecessor, you can identify where the topic or theme shifts, which defines the chunk boundaries.\nYou can learn more about applying semantic chunking with Pinecone here.\nContextual Chunking with LLMs\nSometimes, it’s not possible to chunk information from a larger complex document without losing the context entirely. This can happen when the documents are many hundreds of pages, change topics frequently, or require understanding from many related portions of the document. Anthropic introduced\ncontextual retrieval in 2024\nto help address this problem.\nAnthropic prompted a Claude instance with an entire document and it’s chunk, in order to generate a contextualized description, which is appended to the chunk and then embedded. The description helps retain the high-level summary meaning of the document to the chunk, which exposes this information to incoming queries. To avoid processing the document each time, it’s cached within the prompt for all necessary chunks.\nYou can learn more about contextual retrieval in our video here\nand\nour code example here.\nFiguring out the best chunking strategy for your application\nHere are some pointers to help decide a strategy if fixed chunking doesn’t easily apply to your use case.\nSelecting a Range of Chunk Sizes\n- Once your data is preprocessed, the next step is to choose a range of potential chunk sizes to test. As mentioned previously, the choice should take into account the nature of the content (e.g., short messages or lengthy documents), the embedding model you’ll use, and its capabilities (e.g., token limits). The objective is to find a balance between preserving context and maintaining accuracy. Start by exploring a variety of chunk sizes, including smaller chunks (e.g., 128 or 256 tokens) for capturing more granular semantic information and larger chunks (e.g., 512 or 1024 tokens) for retaining more context.\nEvaluating the Performance of Each Chunk Size\n- In order to test various chunk sizes, you can either use multiple indices or a single index with multiple\nnamespaces\n. With a representative dataset, create the embeddings for the chunk sizes you want to test and save them in your index (or indices). You can then run a series of queries for which you can evaluate quality, and compare the performance of the various chunk sizes. This is most likely to be an iterative process, where you test different chunk sizes against different queries until you can determine the best-performing chunk size for your content and expected queries.\nPost-processing chunks with chunk expansion\nIt’s important to remember that you aren’t entirely married to your chunking strategy. When querying chunked data in a vector database, the retrieved information is typically the top semantically similar chunks given a user query. But users, agents or LLMs may need more surrounding context in order to adequately interpret the chunk.\nChunk expansion is an easy way to post-process chunked data from a database, by retrieving neighboring chunks within a window for each chunk in a retrieved set of chunks. Chunks could be expanded to paragraphs, pages, or even whole documents depending on your use case.\nCoupling a chunking strategy with a good chunk expansion on querying can ensure low latency searches without compromising on context.\nWrapping up\nChunking your content is may appear straightforward in most cases - but it could present some challenges when you start wandering off the beaten path. There’s no one-size-fits-all solution to chunking, so what works for one use case may not work for another.\nWant to get started experimenting with chunking strategies? Create a\nfree Pinecone account\nand check out our\nexample notebooks\nto implement chunking via various applications like semantic search, retrieval augmented generation or agentic applications with Pinecone.\nShare:\nWas this article helpful?\nYes\nNo\nRecommended for you\nFurther Reading\nNov 4, 2025\nInside Pinecone: Slab Architecture\n11\nmin read\nJul 15, 2025\nWhat is Context Engineering?\n8\nmin read\nJun 25, 2025\nBeyond the hype: Why RAG remains essential for modern AI\n7\nmin read",
      "code_blocks": [],
      "headings": [
        {
          "level": 1,
          "text": "Chunking Strategies for LLM Applications"
        },
        {
          "level": 2,
          "text": "What is chunking?"
        },
        {
          "level": 2,
          "text": "Why do we need chunking for our applications?"
        },
        {
          "level": 3,
          "text": "Chunking’s role in semantic search"
        },
        {
          "level": 3,
          "text": "Chunking’s role for agentic applications and retrieval-augmented generation"
        },
        {
          "level": 2,
          "text": "The Role of Chunking for Long Context LLMs"
        },
        {
          "level": 2,
          "text": "What should we think about when choosing a chunking strategy?"
        },
        {
          "level": 2,
          "text": "Embedding short and long content"
        },
        {
          "level": 2,
          "text": "Chunking methods"
        },
        {
          "level": 3,
          "text": "Fixed-size chunking"
        },
        {
          "level": 3,
          "text": "“Content-aware” Chunking"
        },
        {
          "level": 4,
          "text": "Simple Sentence and Paragraph splitting"
        },
        {
          "level": 4,
          "text": "Recursive Character Level Chunking"
        },
        {
          "level": 3,
          "text": "Document structure-based chunking"
        },
        {
          "level": 3,
          "text": "Semantic Chunking"
        },
        {
          "level": 3,
          "text": "Contextual Chunking with LLMs"
        },
        {
          "level": 2,
          "text": "Figuring out the best chunking strategy for your application"
        },
        {
          "level": 3,
          "text": "Post-processing chunks with chunk expansion"
        },
        {
          "level": 2,
          "text": "Wrapping up"
        }
      ]
    }
  ]
}