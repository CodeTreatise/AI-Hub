{
  "name": "What are Embeddings - Pinecone",
  "url": "https://www.pinecone.io/learn/vector-embeddings/",
  "pages": [
    {
      "url": "https://www.pinecone.io/learn/vector-embeddings/",
      "title": "What are Embeddings - Pinecone",
      "content_html": "<article><section><div id=\"banner\"><div class=\"border-b\"><div class=\"container\"><div class=\"relative px-4 sm:px-8 flex flex-col gap-8 pb-4 pt-16 md:gap-16 md:pb-8 md:pt-24 xl:flex-row\"><div class=\"w-full xl:w-[13.5rem]\"><a class=\"text-text-secondary hover:text-text-primary border-b-border group inline-block border-b text-sm/[1] leading-none transition-colors duration-300\" href=\"https://www.pinecone.io/learn/\" target=\"_blank\"><span class=\"inline-block transition-transform duration-300 group-hover:-translate-x-1\">←</span> <span>Learn</span></a></div><div class=\"flex-1 space-y-8\"><div><h1 class=\"xxs:text-[38px] sm:text-h1/[1.1] text-text-primary text-[30px]\">What are Vector Embeddings</h1></div><div class=\"flex items-start gap-0 xl:items-center\"><div class=\"border-border h-6 w-6 shrink-0 overflow-hidden rounded-full border bg-black align-top\" style=\"position:relative;transform:translate(0px, 0px);z-index:1\"><img alt=\"Rajat Tripathi\" class=\"h-full w-full object-cover\" data-nimg=\"1\" decoding=\"async\" height=\"24\" loading=\"lazy\" sizes=\"24px\" src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=3840&amp;q=75\" srcset=\"/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=16&amp;q=75 16w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=32&amp;q=75 32w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=48&amp;q=75 48w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=64&amp;q=75 64w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=96&amp;q=75 96w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=128&amp;q=75 128w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=256&amp;q=75 256w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=384&amp;q=75 384w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=640&amp;q=75 640w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=750&amp;q=75 750w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=828&amp;q=75 828w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=1080&amp;q=75 1080w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=1200&amp;q=75 1200w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=1920&amp;q=75 1920w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=2048&amp;q=75 2048w, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fc555739cf15124cc66a57ee6ca69ffa8a30abff5-263x263.png&amp;w=3840&amp;q=75 3840w\" style=\"color:transparent\" width=\"24\"/></div><span class=\"text-text-secondary max-w-[21.25rem] self-center text-sm/[1.2] ml-2\"><span><a class=\"hover:text-text-primary transition-colors\" href=\"https://www.pinecone.io/author/rajat-tripathi/\" target=\"_blank\">Rajat Tripathi</a></span></span></div></div><div class=\"flex w-full flex-wrap justify-between gap-4 xl:w-[13.5rem] xl:flex-col\"><div class=\"space-y-4\"><span class=\"text-text-secondary block text-sm/[1.2]\">Jun 30, 2023</span><a class=\"text-text-secondary shrink-0 self-start border p-2 text-xs/[1.2] transition-colors hover:border-brand-blue hover:text-brand-blue\" href=\"https://www.pinecone.io/learn/category/core-components/\" target=\"_blank\">Core Components</a></div><div class=\"mt-auto\"><div class=\"text-text-secondary flex flex-col items-start gap-3 text-sm/[1.2]\">Share:<!-- --> <div class=\"flex items-start\"><a aria-label=\"Share to Twitter\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent\" href=\"https://twitter.com/intent/tweet?url=https://www.pinecone.io/learn/vector-embeddings\" target=\"_blank\"><svg fill=\"none\" height=\"12\" viewbox=\"0 0 14 12\" width=\"14\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M10.6367 0.5625H12.5508L8.33984 5.40234L13.3164 11.9375H9.43359L6.37109 7.97266L2.89844 11.9375H0.957031L5.46875 6.79688L0.710938 0.5625H4.70312L7.4375 4.19922L10.6367 0.5625ZM9.95312 10.7891H11.0195L4.12891 1.65625H2.98047L9.95312 10.7891Z\"></path></svg></a><a aria-label=\"Share to LinkedIn\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent\" href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://www.pinecone.io/learn/vector-embeddings\" target=\"_blank\"><svg fill=\"none\" height=\"13\" viewbox=\"0 0 13 13\" width=\"13\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M11.875 0.125C12.3398 0.125 12.75 0.535156 12.75 1.02734V11.5C12.75 11.9922 12.3398 12.375 11.875 12.375H1.34766C0.882812 12.375 0.5 11.9922 0.5 11.5V1.02734C0.5 0.535156 0.882812 0.125 1.34766 0.125H11.875ZM4.19141 10.625V4.80078H2.38672V10.625H4.19141ZM3.28906 3.98047C3.86328 3.98047 4.32812 3.51562 4.32812 2.94141C4.32812 2.36719 3.86328 1.875 3.28906 1.875C2.6875 1.875 2.22266 2.36719 2.22266 2.94141C2.22266 3.51562 2.6875 3.98047 3.28906 3.98047ZM11 10.625V7.42578C11 5.86719 10.6445 4.63672 8.8125 4.63672C7.9375 4.63672 7.33594 5.12891 7.08984 5.59375H7.0625V4.80078H5.33984V10.625H7.14453V7.75391C7.14453 6.98828 7.28125 6.25 8.23828 6.25C9.16797 6.25 9.16797 7.125 9.16797 7.78125V10.625H11Z\"></path></svg></a><a aria-label=\"Share to Hacker News\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group undefined\" href=\"https://news.ycombinator.com/submitlink?u=https://www.pinecone.io/learn/vector-embeddings\" target=\"_blank\"><svg fill=\"none\" height=\"13\" viewbox=\"0 0 13 13\" width=\"13\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M12.75 0.125V12.375H0.5V0.125H12.75ZM6.95312 7.125L9.05859 3.13281H8.15625L6.92578 5.62109C6.78906 5.89453 6.67969 6.14062 6.57031 6.35938L6.24219 5.62109L4.98438 3.13281H4.02734L6.13281 7.07031V9.66797H6.95312V7.125Z\"></path></svg></a></div></div></div></div><div aria-hidden=\"true\" class=\"pointer-events-none absolute inset-0\" role=\"presentation\"><span class=\"from-background absolute bottom-0 left-0 top-0 h-full w-[1px] border-l\"></span><span class=\"from-background absolute bottom-0 right-0 top-0 h-full w-[1px] border-r\"></span></div></div></div></div></div></section><div class=\"container\"><div class=\"relative px-4 sm:px-8 px-0 pb-10\"><div class=\"flex flex-col justify-between lg:flex-row\"><div class=\"mt-4 w-full flex-col gap-10 overflow-hidden md:max-w-full\"><div class=\"prose max-w-none\"><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"Introduction\">Introduction</h2><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Vector embeddings are one of the most fascinating and useful concepts in machine learning. They are central to many NLP, recommendation, and search algorithms. If you’ve ever used things like recommendation engines, voice assistants, language translators, you’ve come across systems that rely on embeddings.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">ML algorithms, like most software algorithms, need numbers to work with. Sometimes we have a dataset with columns of numeric values or values that can be translated into them (ordinal, categorical, etc). Other times we come across something more abstract like an entire document of text. We create vector embeddings, which are just lists of numbers, for data like this to perform various operations with them. A whole paragraph of text or any other object can be reduced to a vector. Even numerical data can be turned into vectors for easier operations.</p><div class=\"flex flex-col items-center align-middle md:px-0 mt-7 pb-3\"><div class=\"w-full max-w-full\"><div class=\"flex flex-col items-center justify-center\"><div class=\"flex hover:cursor-zoom-in max-w-full items-center justify-center\" style=\"height:auto;cursor:zoom-in\"><img alt=\"Vector Embeddings are a list of numbers\" class=\"max-w-full h-auto hover:cursor-zoom-in hover:opacity-80 transition-opacity duration-300\" data-nimg=\"1\" decoding=\"async\" height=\"849\" loading=\"lazy\" src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fe016bbd4d7d57ff27e261adf1e254d2d3c609aac-2447x849.png&amp;w=3840&amp;q=75\" srcset=\"/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fe016bbd4d7d57ff27e261adf1e254d2d3c609aac-2447x849.png&amp;w=3840&amp;q=75 1x\" style=\"color:transparent\" width=\"2447\"/></div></div></div></div><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">But there is something special about vectors that makes them so useful. This representation makes it possible to translate <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://en.wikipedia.org/wiki/Semantic_similarity\" target=\"_blank\">semantic similarity</a> as perceived by humans to proximity in a <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://en.wikipedia.org/wiki/Vector_space\" target=\"_blank\">vector space</a>.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">In other words, when we represent real-world objects and concepts such as images, audio recordings, news articles, user profiles, weather patterns, and political views as vector embeddings, the semantic similarity of these objects and concepts can be quantified by how close they are to each other as points in vector spaces. Vector embedding representations are thus suitable for common machine learning tasks such as clustering, recommendation, and classification.</p><div class=\"flex flex-col items-center align-middle md:px-0 mt-7 pb-3\"><div class=\"w-full max-w-full\"><div class=\"flex flex-col items-center justify-center\"><div class=\"flex hover:cursor-zoom-in max-w-full items-center justify-center\" style=\"height:auto;cursor:zoom-in\"><img alt=\"Semantic similarity in sentence embeddings.\" class=\"max-w-full h-auto hover:cursor-zoom-in hover:opacity-80 transition-opacity duration-300\" data-nimg=\"1\" decoding=\"async\" height=\"504\" loading=\"lazy\" src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fec6f20344b056465a17d98cc98b834eb364f93ce-1432x504.png&amp;w=3840&amp;q=75\" srcset=\"/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fec6f20344b056465a17d98cc98b834eb364f93ce-1432x504.png&amp;w=1920&amp;q=75 1x, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fec6f20344b056465a17d98cc98b834eb364f93ce-1432x504.png&amp;w=3840&amp;q=75 2x\" style=\"color:transparent\" width=\"1432\"/></div></div></div><div class=\"w-full text-center text-sm mt-4\">Source:<!-- --> <a class=\"text-brand-blue hover:text-brand-blue/80 transition-colors duration-300\" href=\"https://deepai.org/publication/in-search-for-linear-relations-in-sentence-embedding-spaces\" target=\"_blank\">DeepAI</a> </div></div><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">For example, in a clustering task, clustering algorithms assign similar points to the same cluster while keeping points from different clusters as dissimilar as possible. In a recommendation task, when making recommendations for an unseen object, the recommender system would look for objects that are most similar to the object in question, as measured by their similarity as vector embeddings. In a classification task, we classify the label of an unseen object by the major vote over labels of the most similar objects.</p><div class=\"border border-border mt-8 bg-background-light max-w-[875px] w-full h-fit flex flex-col items-start justify-center py-8 px-8 box-border text-left text-[33px] text-text-primary\" id=\"in-article-cta\"><div class=\"h-[fit] flex flex-col items-start justify-center gap-[14px]\"><div class=\"relative leading-[120%] font-semibold\">Start using Pinecone for free</div><div class=\"relative text-body md:text-xl leading-[140%] text-text-secondary\">Pinecone is the developer-favorite <a class=\"text-text-primary underline! hover:text-text-secondary\" href=\"https://www.pinecone.io/learn/vector-database/\" target=\"_blank\">vector database</a> that's fast and easy to use at any scale.</div></div><div class=\"flex flex-row w-full items-center gap-5 justify-end text-center text-sm sm:text-[1rem] xs::text-base text-black mt-5\"><div class=\"w-full\"></div></div></div><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"Creating-Vector-Embeddings\">Creating Vector Embeddings</h2><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">One way of creating vector embeddings is to engineer the vector values using domain knowledge. This is known as feature engineering. For example, in medical imaging, we use medical expertise to quantify a set of features such as shape, color, and regions in an image that capture the semantics. However, engineering vector embeddings requires domain knowledge, and it is too expensive to scale.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Instead of engineering vector embeddings, we often train models to translate objects to vectors. A deep neural network is a common tool for training such models. The resulting embeddings are usually high dimensional (up to two thousand dimensions) and dense (all values are non-zero). For text data, models such as <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://en.wikipedia.org/wiki/Word2vec\" target=\"_blank\">Word2Vec</a>, <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://en.wikipedia.org/wiki/GloVe_(machine_learning)\" target=\"_blank\">GLoVE</a>, and <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://en.wikipedia.org/wiki/BERT_(language_model)\" target=\"_blank\">BERT</a> transform words, sentences, or paragraphs into vector embeddings.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Images can be embedded using models such as <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://en.wikipedia.org/wiki/Convolutional_neural_network\" target=\"_blank\">convolutional neural networks (CNNs)</a>, Examples of CNNs include <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://arxiv.org/abs/1409.1556\" target=\"_blank\">VGG</a>, and <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://arxiv.org/abs/1409.4842\" target=\"_blank\">Inception</a>. Audio recordings can be transformed into vectors using image embedding transformations over the audio frequencies visual representation (e.g., using its <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://en.wikipedia.org/wiki/Spectrogram\" target=\"_blank\">Spectrogram</a>).</p><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"Example:-Image-Embedding-with-a-Convolutional-Neural-Network\">Example: Image Embedding with a Convolutional Neural Network</h2><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Consider the following example, in which raw images are represented as greyscale pixels. This is equivalent to a matrix (or table) of integer values in the range <code>0</code> to <code>255</code>. Wherein the value <code>0</code> corresponds to a black color and <code>255</code> to white color. The image below depicts a greyscale image and its corresponding matrix.</p><div class=\"flex flex-col items-center align-middle md:px-0 mt-7 pb-3\"><div class=\"w-full max-w-full\"><div class=\"flex flex-col items-center justify-center\"><div class=\"flex hover:cursor-zoom-in max-w-full items-center justify-center\" style=\"height:auto;cursor:zoom-in\"><img alt=\"Grayscale image representation\" class=\"max-w-full h-auto hover:cursor-zoom-in hover:opacity-80 transition-opacity duration-300\" data-nimg=\"1\" decoding=\"async\" height=\"290\" loading=\"lazy\" src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F31a1950de10f80473b43530795d034917a43f366-704x290.png&amp;w=1920&amp;q=75\" srcset=\"/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F31a1950de10f80473b43530795d034917a43f366-704x290.png&amp;w=750&amp;q=75 1x, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F31a1950de10f80473b43530795d034917a43f366-704x290.png&amp;w=1920&amp;q=75 2x\" style=\"color:transparent\" width=\"704\"/></div></div></div><div class=\"w-full text-center text-sm mt-4\">Source:<!-- --> <a class=\"text-brand-blue hover:text-brand-blue/80 transition-colors duration-300\" href=\"https://ai.stanford.edu/~syyeung/\" target=\"_blank\">Serena Young</a> </div></div><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">The left sub-image depicts the grayscale pixels, the middle sub-image contains the pixel grayscale values, and the rightmost sub-image defines the matrix. Notice the matrix values define a vector embedding in which its first coordinate is the matrix upper-left cell, then going left-to-right until the last coordinate which corresponds to the lower-right matrix cell.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Such embeddings are great at maintaining the semantic information of a pixel’s neighborhood in an image. However, they are very sensitive to transformations like shifts, scaling, cropping and other image manipulation operations. Therefore they are often used as raw inputs to learn more robust embeddings.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Convolutional Neural Network (CNN or ConvNet) is a class of deep learning architectures that are usually applied to visual data transforming images into embeddings.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">CNNs are processing the input via hierarchical small local sub-inputs which are termed receptive fields. Each neuron in each network layer processes a specific receptive field from the former layer. Each layer either applies a <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://en.wikipedia.org/wiki/Convolution\" target=\"_blank\">convolution</a> on the receptive field or reduces the input size, which is called subsampling.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">The image below depicts a typical CNN structure. Notice the receptive fields, depicted as sub-squares in each layer, service as an input to a single neuron within the preceding layer. Notice also the subsampling operations reduce the layer size, while the convolution operations extend the layer size. The resulting vector embedding is received via a fully connected layer.</p><div class=\"flex flex-col items-center align-middle md:px-0 mt-7 pb-3\"><div class=\"w-full max-w-full\"><div class=\"flex flex-col items-center justify-center\"><div class=\"flex hover:cursor-zoom-in max-w-full items-center justify-center\" style=\"height:auto;cursor:zoom-in\"><img alt=\"Typical CNN architecture\" class=\"max-w-full h-auto hover:cursor-zoom-in hover:opacity-80 transition-opacity duration-300\" data-nimg=\"1\" decoding=\"async\" height=\"320\" loading=\"lazy\" src=\"https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fca4f41cd19b9e78e35b355ee015d57757cdd0e4d-1040x320.png&amp;w=3840&amp;q=75\" srcset=\"/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fca4f41cd19b9e78e35b355ee015d57757cdd0e4d-1040x320.png&amp;w=1080&amp;q=75 1x, /_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2Fca4f41cd19b9e78e35b355ee015d57757cdd0e4d-1040x320.png&amp;w=3840&amp;q=75 2x\" style=\"color:transparent\" width=\"1040\"/></div></div></div><div class=\"w-full text-center text-sm mt-4\">Source:<!-- --> <a class=\"text-brand-blue hover:text-brand-blue/80 transition-colors duration-300\" href=\"https://commons.wikimedia.org/wiki/File:Typical_cnn.png\" target=\"_blank\">Aphex34, CC BY-SA 4.0</a> </div></div><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Learning the network weights (i.e., the embedding model) requires a large set of labeled images. The weights are being optimized in a way that images with the same labels are embedded closer compared to images with different labels. Once we learn the CNN embedding model we can transform the images into vectors and store them with a K-Nearest-Neighbor index. Now, given a new unseen image, we can transform it with the CNN model, retrieve its k-most similar vectors, and thus the corresponding similar images.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Although we used images and CNNs as examples, vector embeddings can be created for any kind of data and there are multiple models/methods that we can use to create them.</p><h2 class=\"text-h2-mobile lg:text-h2 text-text-primary my-[40px]\" id=\"Using-Vector-Embeddings\">Using Vector Embeddings</h2><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">The fact that embeddings can represent an object as a dense vector that contains its semantic information makes them very useful for a wide range of ML applications.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\"><a class=\"cursor-pointer underline-offset-4 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://www.pinecone.io/learn/what-is-similarity-search/\" target=\"_blank\">Similarity search</a> is one of the most popular uses of vector embeddings. Search algorithms like KNN and ANN require us to calculate distance between vectors to determine similarity. Vector embeddings can be used to calculate these distances. Nearest neighbor search in turn can be used for tasks like de-duplication, recommendations, anomaly detection, reverse image search, etc.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Even if we don’t use embeddings directly for an application, many popular ML models and methods internally rely on them. For example in <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346\" target=\"_blank\">encoder-decoder architectures</a>, embeddings produced by encoder contain the necessary information for the decoder to produce a result. This architecture is widely used in applications, such as machine translation and caption generation.</p><p class=\"mt-4 text-body leading-6 [&amp;_b]:font-semibold [&amp;_strong]:font-semibold text-text-primary\">Check out <a class=\"cursor-pointer underline-offset-2 transition-all duration-300 hover:opacity-50 text-text-primary underline\" href=\"https://www.pinecone.io/docs/examples/\" target=\"_blank\">some applications</a> you can build with vector embeddings and Pinecone.</p></div><div class=\"mt-10 block md:hidden\"><div class=\"text-text-secondary flex flex-col items-start gap-3 text-sm/[1.2]\">Share:<!-- --> <div class=\"flex items-start\"><a aria-label=\"Share to Twitter\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent\" href=\"https://twitter.com/intent/tweet?url=https://www.pinecone.io/learn/vector-embeddings\" target=\"_blank\"><svg fill=\"none\" height=\"12\" viewbox=\"0 0 14 12\" width=\"14\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M10.6367 0.5625H12.5508L8.33984 5.40234L13.3164 11.9375H9.43359L6.37109 7.97266L2.89844 11.9375H0.957031L5.46875 6.79688L0.710938 0.5625H4.70312L7.4375 4.19922L10.6367 0.5625ZM9.95312 10.7891H11.0195L4.12891 1.65625H2.98047L9.95312 10.7891Z\"></path></svg></a><a aria-label=\"Share to LinkedIn\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group border-r-transparent\" href=\"https://www.linkedin.com/sharing/share-offsite/?url=https://www.pinecone.io/learn/vector-embeddings\" target=\"_blank\"><svg fill=\"none\" height=\"13\" viewbox=\"0 0 13 13\" width=\"13\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M11.875 0.125C12.3398 0.125 12.75 0.535156 12.75 1.02734V11.5C12.75 11.9922 12.3398 12.375 11.875 12.375H1.34766C0.882812 12.375 0.5 11.9922 0.5 11.5V1.02734C0.5 0.535156 0.882812 0.125 1.34766 0.125H11.875ZM4.19141 10.625V4.80078H2.38672V10.625H4.19141ZM3.28906 3.98047C3.86328 3.98047 4.32812 3.51562 4.32812 2.94141C4.32812 2.36719 3.86328 1.875 3.28906 1.875C2.6875 1.875 2.22266 2.36719 2.22266 2.94141C2.22266 3.51562 2.6875 3.98047 3.28906 3.98047ZM11 10.625V7.42578C11 5.86719 10.6445 4.63672 8.8125 4.63672C7.9375 4.63672 7.33594 5.12891 7.08984 5.59375H7.0625V4.80078H5.33984V10.625H7.14453V7.75391C7.14453 6.98828 7.28125 6.25 8.23828 6.25C9.16797 6.25 9.16797 7.125 9.16797 7.78125V10.625H11Z\"></path></svg></a><a aria-label=\"Share to Hacker News\" class=\"flex h-8 w-8 shrink-0 items-center justify-center border transition-colors hover:border-text-primary group undefined\" href=\"https://news.ycombinator.com/submitlink?u=https://www.pinecone.io/learn/vector-embeddings\" target=\"_blank\"><svg fill=\"none\" height=\"13\" viewbox=\"0 0 13 13\" width=\"13\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"fill-text-secondary group-hover:fill-text-primary transition-colors\" d=\"M12.75 0.125V12.375H0.5V0.125H12.75ZM6.95312 7.125L9.05859 3.13281H8.15625L6.92578 5.62109C6.78906 5.89453 6.67969 6.14062 6.57031 6.35938L6.24219 5.62109L4.98438 3.13281H4.02734L6.13281 7.07031V9.66797H6.95312V7.125Z\"></path></svg></a></div></div></div><div class=\"mt-10\"><div class=\"inline-block border px-4 py-3\"><p class=\"text-text-secondary text-sm\">Was this article helpful?</p><div class=\"mt-2 flex gap-4\"><button class=\"text-text-secondary hover:text-text-primary flex cursor-pointer items-center gap-2 text-sm transition-colors\"><svg class=\"lucide lucide-thumbs-up h-4 w-4\" fill=\"none\" height=\"24\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewbox=\"0 0 24 24\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M7 10v12\"></path><path d=\"M15 5.88 14 10h5.83a2 2 0 0 1 1.92 2.56l-2.33 8A2 2 0 0 1 17.5 22H4a2 2 0 0 1-2-2v-8a2 2 0 0 1 2-2h2.76a2 2 0 0 0 1.79-1.11L12 2h0a3.13 3.13 0 0 1 3 3.88Z\"></path></svg>Yes</button><button class=\"text-text-secondary hover:text-text-primary flex cursor-pointer items-center gap-2 text-sm transition-colors\"><svg class=\"lucide lucide-thumbs-down h-4 w-4\" fill=\"none\" height=\"24\" stroke=\"currentColor\" stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" viewbox=\"0 0 24 24\" width=\"24\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M17 14V2\"></path><path d=\"M9 18.12 10 14H4.17a2 2 0 0 1-1.92-2.56l2.33-8A2 2 0 0 1 6.5 2H20a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2h-2.76a2 2 0 0 0-1.79 1.11L12 22h0a3.13 3.13 0 0 1-3-3.88Z\"></path></svg>No</button></div></div></div></div></div><div aria-hidden=\"true\" class=\"pointer-events-none absolute left-0 top-0 w-full\" role=\"presentation\"><span class=\"z-1 absolute border-l top-0 left-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent left-0 top-1 h-[10rem]\"></span><span class=\"z-1 absolute border-l top-0 right-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent right-0 top-1 h-[10rem]\"></span></div><div aria-hidden=\"true\" class=\"pointer-events-none absolute bottom-0 left-0 w-full\" role=\"presentation\"><span class=\"z-1 absolute border-l bottom-0 left-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-b from-background z-2 absolute w-[1px] from-10% to-transparent left-0 bottom-1 h-[10rem]\"></span><span class=\"z-1 absolute border-l bottom-0 right-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-b from-background z-2 absolute w-[1px] from-10% to-transparent right-0 bottom-1 h-[10rem]\"></span></div></div></div><div class=\"container\"><div class=\"relative px-4 sm:px-8 border-border border-t !px-0\"><div class=\"w-full pt-24 pb-20 border-border flex-col justify-start items-start gap-8 inline-flex\"><div class=\"self-stretch min-h-[54px] px-8 flex-col justify-start items-start gap-2 flex\"><div class=\"self-stretch text-brand-blue text-xs font-medium font-['GT Planar VCTR'] uppercase leading-[18px] tracking-widest\">Recommended for you</div><div class=\"self-stretch text-text-primary text-[32px] font-bold font-['GT Planar VCTR'] leading-[38.40px]\">Further Reading</div></div><div class=\"w-full grid grid-cols-1 md:grid-cols-3 divide-y md:divide-y-0 md:divide-x divide-border border border-border\"><a class=\"group gap-4 p-8 bg-background hover:bg-background-light flex flex-col justify-between transition-colors duration-300\" href=\"https://www.pinecone.io/learn/slab-architecture/\" target=\"_blank\"><div class=\"flex flex-col gap-4\"><div class=\"flex items-start justify-between\"><div class=\"text-text-secondary text-sm leading-[21px]\"></div><div class=\"text-text-secondary text-sm leading-[21px]\">Nov 4, 2025</div></div><div class=\"text-text-primary text-xl font-medium leading-relaxed min-h-[150px]\">Inside Pinecone: Slab Architecture</div></div><div class=\"text-text-secondary text-sm leading-[16.8px]\">11<!-- --> min read</div></a><a class=\"group gap-4 p-8 bg-background hover:bg-background-light flex flex-col justify-between transition-colors duration-300\" href=\"https://www.pinecone.io/learn/context-engineering/\" target=\"_blank\"><div class=\"flex flex-col gap-4\"><div class=\"flex items-start justify-between\"><div class=\"text-text-secondary text-sm leading-[21px]\"></div><div class=\"text-text-secondary text-sm leading-[21px]\">Jul 15, 2025</div></div><div class=\"text-text-primary text-xl font-medium leading-relaxed min-h-[150px]\">What is Context Engineering?</div></div><div class=\"text-text-secondary text-sm leading-[16.8px]\">8<!-- --> min read</div></a><a class=\"group gap-4 p-8 bg-background hover:bg-background-light flex flex-col justify-between transition-colors duration-300\" href=\"https://www.pinecone.io/learn/chunking-strategies/\" target=\"_blank\"><div class=\"flex flex-col gap-4\"><div class=\"flex items-start justify-between\"><div class=\"text-text-secondary text-sm leading-[21px]\"></div><div class=\"text-text-secondary text-sm leading-[21px]\">Jun 28, 2025</div></div><div class=\"text-text-primary text-xl font-medium leading-relaxed min-h-[150px]\">Chunking Strategies for LLM Applications</div></div><div class=\"text-text-secondary text-sm leading-[16.8px]\">16<!-- --> min read</div></a></div></div><div aria-hidden=\"true\" class=\"pointer-events-none absolute left-0 top-0 w-full\" role=\"presentation\"><span class=\"z-1 absolute border-l top-0 left-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent left-0 top-1 h-[10rem]\"></span><span class=\"z-1 absolute border-l top-0 right-0 h-[7.125rem]\"></span><span class=\"bg-linear-to-t from-background z-2 absolute w-[1px] from-10% to-transparent right-0 top-1 h-[10rem]\"></span></div></div></div></article>",
      "text": "←\nLearn\nWhat are Vector Embeddings\nRajat Tripathi\nJun 30, 2023\nCore Components\nShare:\nIntroduction\nVector embeddings are one of the most fascinating and useful concepts in machine learning. They are central to many NLP, recommendation, and search algorithms. If you’ve ever used things like recommendation engines, voice assistants, language translators, you’ve come across systems that rely on embeddings.\nML algorithms, like most software algorithms, need numbers to work with. Sometimes we have a dataset with columns of numeric values or values that can be translated into them (ordinal, categorical, etc). Other times we come across something more abstract like an entire document of text. We create vector embeddings, which are just lists of numbers, for data like this to perform various operations with them. A whole paragraph of text or any other object can be reduced to a vector. Even numerical data can be turned into vectors for easier operations.\nBut there is something special about vectors that makes them so useful. This representation makes it possible to translate\nsemantic similarity\nas perceived by humans to proximity in a\nvector space\n.\nIn other words, when we represent real-world objects and concepts such as images, audio recordings, news articles, user profiles, weather patterns, and political views as vector embeddings, the semantic similarity of these objects and concepts can be quantified by how close they are to each other as points in vector spaces. Vector embedding representations are thus suitable for common machine learning tasks such as clustering, recommendation, and classification.\nSource:\nDeepAI\nFor example, in a clustering task, clustering algorithms assign similar points to the same cluster while keeping points from different clusters as dissimilar as possible. In a recommendation task, when making recommendations for an unseen object, the recommender system would look for objects that are most similar to the object in question, as measured by their similarity as vector embeddings. In a classification task, we classify the label of an unseen object by the major vote over labels of the most similar objects.\nStart using Pinecone for free\nPinecone is the developer-favorite\nvector database\nthat's fast and easy to use at any scale.\nCreating Vector Embeddings\nOne way of creating vector embeddings is to engineer the vector values using domain knowledge. This is known as feature engineering. For example, in medical imaging, we use medical expertise to quantify a set of features such as shape, color, and regions in an image that capture the semantics. However, engineering vector embeddings requires domain knowledge, and it is too expensive to scale.\nInstead of engineering vector embeddings, we often train models to translate objects to vectors. A deep neural network is a common tool for training such models. The resulting embeddings are usually high dimensional (up to two thousand dimensions) and dense (all values are non-zero). For text data, models such as\nWord2Vec\n,\nGLoVE\n, and\nBERT\ntransform words, sentences, or paragraphs into vector embeddings.\nImages can be embedded using models such as\nconvolutional neural networks (CNNs)\n, Examples of CNNs include\nVGG\n, and\nInception\n. Audio recordings can be transformed into vectors using image embedding transformations over the audio frequencies visual representation (e.g., using its\nSpectrogram\n).\nExample: Image Embedding with a Convolutional Neural Network\nConsider the following example, in which raw images are represented as greyscale pixels. This is equivalent to a matrix (or table) of integer values in the range\n0\nto\n255\n. Wherein the value\n0\ncorresponds to a black color and\n255\nto white color. The image below depicts a greyscale image and its corresponding matrix.\nSource:\nSerena Young\nThe left sub-image depicts the grayscale pixels, the middle sub-image contains the pixel grayscale values, and the rightmost sub-image defines the matrix. Notice the matrix values define a vector embedding in which its first coordinate is the matrix upper-left cell, then going left-to-right until the last coordinate which corresponds to the lower-right matrix cell.\nSuch embeddings are great at maintaining the semantic information of a pixel’s neighborhood in an image. However, they are very sensitive to transformations like shifts, scaling, cropping and other image manipulation operations. Therefore they are often used as raw inputs to learn more robust embeddings.\nConvolutional Neural Network (CNN or ConvNet) is a class of deep learning architectures that are usually applied to visual data transforming images into embeddings.\nCNNs are processing the input via hierarchical small local sub-inputs which are termed receptive fields. Each neuron in each network layer processes a specific receptive field from the former layer. Each layer either applies a\nconvolution\non the receptive field or reduces the input size, which is called subsampling.\nThe image below depicts a typical CNN structure. Notice the receptive fields, depicted as sub-squares in each layer, service as an input to a single neuron within the preceding layer. Notice also the subsampling operations reduce the layer size, while the convolution operations extend the layer size. The resulting vector embedding is received via a fully connected layer.\nSource:\nAphex34, CC BY-SA 4.0\nLearning the network weights (i.e., the embedding model) requires a large set of labeled images. The weights are being optimized in a way that images with the same labels are embedded closer compared to images with different labels. Once we learn the CNN embedding model we can transform the images into vectors and store them with a K-Nearest-Neighbor index. Now, given a new unseen image, we can transform it with the CNN model, retrieve its k-most similar vectors, and thus the corresponding similar images.\nAlthough we used images and CNNs as examples, vector embeddings can be created for any kind of data and there are multiple models/methods that we can use to create them.\nUsing Vector Embeddings\nThe fact that embeddings can represent an object as a dense vector that contains its semantic information makes them very useful for a wide range of ML applications.\nSimilarity search\nis one of the most popular uses of vector embeddings. Search algorithms like KNN and ANN require us to calculate distance between vectors to determine similarity. Vector embeddings can be used to calculate these distances. Nearest neighbor search in turn can be used for tasks like de-duplication, recommendations, anomaly detection, reverse image search, etc.\nEven if we don’t use embeddings directly for an application, many popular ML models and methods internally rely on them. For example in\nencoder-decoder architectures\n, embeddings produced by encoder contain the necessary information for the decoder to produce a result. This architecture is widely used in applications, such as machine translation and caption generation.\nCheck out\nsome applications\nyou can build with vector embeddings and Pinecone.\nShare:\nWas this article helpful?\nYes\nNo\nRecommended for you\nFurther Reading\nNov 4, 2025\nInside Pinecone: Slab Architecture\n11\nmin read\nJul 15, 2025\nWhat is Context Engineering?\n8\nmin read\nJun 28, 2025\nChunking Strategies for LLM Applications\n16\nmin read",
      "code_blocks": [],
      "headings": [
        {
          "level": 1,
          "text": "What are Vector Embeddings"
        },
        {
          "level": 2,
          "text": "Introduction"
        },
        {
          "level": 2,
          "text": "Creating Vector Embeddings"
        },
        {
          "level": 2,
          "text": "Example: Image Embedding with a Convolutional Neural Network"
        },
        {
          "level": 2,
          "text": "Using Vector Embeddings"
        }
      ]
    }
  ]
}